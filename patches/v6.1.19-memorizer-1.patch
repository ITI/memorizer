diff --git a/.gitattributes b/.gitattributes
index 4b32eaa9571e..aea11557f131 100644
--- a/.gitattributes
+++ b/.gitattributes
@@ -2,3 +2,4 @@
 *.h   diff=cpp
 *.dtsi diff=dts
 *.dts  diff=dts
+*.raw binary
diff --git a/Documentation/memorizer.txt b/Documentation/memorizer.txt
new file mode 100644
index 000000000000..02e4046bdde8
--- /dev/null
+++ b/Documentation/memorizer.txt
@@ -0,0 +1,110 @@
+             +--------------------------------------------------+
+             | Memorizer: Kernel Memory Access Patterns (KMAPs) |
+             +--------------------------------------------------+
+
+Introduction
+============
+
+Memorizer is a tool to record information about access to kernel objects:
+specifically, it counts memory accesses from distinct IP addresses in the
+kernel source and also the PID that accessed, thereby providing spatial and
+temporal dimensions.
+
+Interface via debugfs
+=====================
+
+The tool has a very simple interface at the moment. It can:
+
+- Print out some statistics about memory allocations and memory accesses
+- Control enable/disable of memory object allocation tracking and memory access
+  tracing
+- Print the KMAP using the debugfs file system
+
+Enable object allocation tracking:
+```bash
+echo 1 > /sys/kernel/debug/memorizer/memorizer_enabled
+```
+
+Enable object access tracking:
+```bash
+echo 1 > /sys/kernel/debug/memorizer/memorizer_log_access
+```
+
+Show allocation statistics:
+```bash
+cat /sys/kernel/debug/memorizer/show_stats
+```
+
+Clear free'd objects:
+```bash
+echo 1 > /sys/kernel/debug/memorizer/clear_object_list
+```
+Enable function call tracking:
+```bash
+echo 1 > /sys/kernel/debug/memorizer/cfg_log_on
+```
+
+Enable function call and stack frame tracking:
+```bash
+echo 1 > /sys/kernel/debug/memorizer/stack_trace_on
+```
+
+Clear function call and stack frame tracking:
+```bash
+echo 1 > /sys/kernel/debug/memorizer/cfgmap
+```
+
+Using Memorizer to Collect KMAPs
+================================
+
+Memorizer lacks push style logging and clearing of the object lists, therefore
+it has the propensity of overflowing memory. The only way to manage the log and
+current set of objects is to manually clear and print the KMAPs.
+
+Therefore, a typical run using memorizer to create KMAPs includes:
+
+```bash
+# mount the debugfs filesystem if it isn't already
+mount -t debugfs nodev /sys/kernel/debug
+# clear free objects: the current system traces from boot with a lot of
+# uninteresting data
+echo 1 > /sys/kernel/debug/clear_object_list
+# enable memorizer object access tracking, which by default is off
+echo 1 > /sys/kernel/debug/memorizer_log_access
+# Now run whatever test
+tar zcf something.tar.gz /somedir &
+ssh u@h:/somefile
+...
+# Disable access logging
+echo 0 > /sys/kernel/debug/memorizer/memorizer_log_access
+# Disable memorizer object tracking: isn't necessary but will reduce noise
+echo 0 > /sys/kernel/debug/memorizer/memorizer_enabled
+# Cat the results: make sure to pipe to something
+cat /sys/kernel/debug/memorizer/kmap > test.kmap
+```
+
+Output Format
+=============
+
+Memorizer outputs data as text, which may change if space is a problem. The
+format of the kmap file is as follows:
+
+alloc_ip,pid,obj_va_ptr,size,alloc_jiffies,free_jiffies,free_ip,executable
+  access_ip,access_pid,write_count,read_count
+  access_ip,access_pid,write_count,read_count
+  access_ip,access_pid,write_count,read_count
+    ...
+    ...
+
+There are a few special error codes:
+
+    - Not all free_ip's could be obtained correctly and therefore some of these
+      will be 0.
+    - There is a bug where we insert into the live object map over another
+      allocation, this implies that we are missing a free. So for now we mark
+      the free_ip as 0xDEADBEEF.
+
+Note
+====
+cfg_log_on and stack_trace_on shares the same <caller, callee> mapping structure. Please
+choose either one to turn on and clean the cfgmap after finished.
diff --git a/Makefile b/Makefile
index ea18c4c20738..3cc482f6ca9a 100644
--- a/Makefile
+++ b/Makefile
@@ -2,7 +2,7 @@
 VERSION = 6
 PATCHLEVEL = 1
 SUBLEVEL = 19
-EXTRAVERSION =
+EXTRAVERSION = -memorizer
 NAME = Hurr durr I'ma ninja sloth
 
 # *DOCUMENTATION*
@@ -1090,6 +1090,7 @@ include-$(CONFIG_KASAN)		+= scripts/Makefile.kasan
 include-$(CONFIG_KCSAN)		+= scripts/Makefile.kcsan
 include-$(CONFIG_KMSAN)		+= scripts/Makefile.kmsan
 include-$(CONFIG_UBSAN)		+= scripts/Makefile.ubsan
+include-$(CONFIG_MEMORIZER)	+= scripts/Makefile.memorizer
 include-$(CONFIG_KCOV)		+= scripts/Makefile.kcov
 include-$(CONFIG_RANDSTRUCT)	+= scripts/Makefile.randstruct
 include-$(CONFIG_GCC_PLUGINS)	+= scripts/Makefile.gcc-plugins
diff --git a/README-iti.md b/README-iti.md
new file mode 100644
index 000000000000..cf2e188e2bae
--- /dev/null
+++ b/README-iti.md
@@ -0,0 +1,30 @@
+# How to build memorizer-v6 & run memorizer test in a VM
+
+This is how we build and execute a linux v6 memorizer kernel, as of 2023-03-22. 
+
+### Build
+
+#### build a kernel
+
+* `cd /data/<yourname>; export TOP=$PWD`
+* `git clone gitlab@code.iti.illinois.edu:ring0/memorizer.git`
+* `cd memorizer`
+* `make O=o defconfig`
+* `make O=o rob.config`
+* `make O=o memorizer.config`
+* `make O=o -j3` # But tune `-j3` to taste.
+
+#### build a rootfs
+
+* cd $TOP/VM
+* Adjust `mkosi.conf` to taste.
+* mkosi build
+
+### Run
+
+* Launch the QEMU virtual machine
+* `mkosi qemu`
+* Run some commands inside qemu's virtual machine. For more information on this step, see Documentation/memorizer.txt
+  * `uname -a` should indicate a kernel name like `6.1.19-memorizer`.  
+  * Run your test, capture your data.
+  * Copy your results to the outside world using `scp`. You can copy it anywhere you want. `10.0.2.2` is a QEMU alias for the machine on which QEMU is running, so my command looks like: `scp foo.bar me@10.0.2.2:/data/me/.`
diff --git a/VM/README.md b/VM/README.md
new file mode 100644
index 000000000000..7f1ffd89e819
--- /dev/null
+++ b/VM/README.md
@@ -0,0 +1,84 @@
+# Running memorizer tests
+
+To run a test using memorizer, you must:
+
+## If using QEMU
+
+* Build a kernel
+* Build a VM
+* Start the VM
+* Run the test
+* Copy out the results
+
+## If using hardware (e.g. LattePanda 3 Delta 864)
+
+* Build a kernel
+* Build a VM
+* Copy disk image to hardware
+* Boot the image on hardware
+* Run the test
+* Copy out the results
+
+## Build a kernel
+
+Build a kernel any way you please. You might use these commands:
+
+    cd ..
+    alias kb6='docker run -it --rm     \
+        -v "$(pwd):$(pwd)" -w "$(pwd)" \
+        -v /etc/passwd:/etc/passwd:ro -u "$(id -u):$(id -g)" \
+        pastorrob/kernel-build:6.0'
+    kb6 make O=o defconfig
+    kb6 make O=o rob.config
+    kb6 make O=o memorizer.config
+    kb6 make O=o -j3  # but tune '-j3' to taste
+
+## Build a VM
+
+These control files are for `mkosi`. First set up `mkosi`, following its directions.
+
+Then issue this command. In addition to setting up a working Ubuntu 20.04,
+this will copy the kernel from `../o` into the image.
+
+    sudo mkosi --force build
+
+The file `focal.img` now contains a memorizer kernel, a rootfs, and an EFI boot environment.
+It will work equally well in QEMU and on PC-standard hardware (e.g., LattePanda 3 Delta 864).
+
+## Start the VM
+
+If you are using QEMU, start the VM with this command:
+
+    mkosi qemu
+
+## Copy disk image to hardware
+
+If you are using a hardware device, copy the boot image with this command:
+
+    # below is an example. Select the correct device file for your flash drive
+    sudo dd if=focal.img of=/dev/sdq bs=10M
+
+## Boot the image on hardware
+
+If you are using a hardware device, boot the Memorizer kernel.
+The following works for at least some PC-compatible targets.
+
+Insert the flash into the target hardware and power the hardware on. You may need to enter
+its BIOS setup system to enable EFI booting from a USB device.
+
+During booting, the EFI boot menu should appear. Select "Linux 6.0 memorizer"
+
+## Run the test
+
+In either a VM or a hardware target, the file `/doit.sh` is automatically provided.
+
+Inside the shell, edit the file `/doit.sh` to suit your enviornment and test:
+
+    vi /doit.sh
+    /doit.sh
+
+## Copy out the results
+
+When run inside QEMU, the file `/doit.sh` automatically copies out the results to `/data/$USER/data/$UNAME/$DATE`.
+
+Copying out the data in a hardware test is left to the reader. :)
diff --git a/VM/mkosi.cache/.gitignore b/VM/mkosi.cache/.gitignore
new file mode 100644
index 000000000000..e69de29bb2d1
diff --git a/VM/mkosi.conf b/VM/mkosi.conf
new file mode 100644
index 000000000000..fccd94ca396f
--- /dev/null
+++ b/VM/mkosi.conf
@@ -0,0 +1,84 @@
+# The goal for this file (and other files name `mkosi.*`) is to simplify
+# the running of Memorizer. The goal is to be able to do:
+#  sudo mkosi build
+#  mkosi qemu -or- mkosi ssh
+
+
+# What distribution are we using? `Ubuntu 20.04.5 LTS (Focal Fossa)` is a good choice
+[Distribution]
+Distribution=ubuntu
+Release=focal
+
+[Output]
+# ext4 only because it is familiar
+Format=gpt_ext4
+
+# A bootable image can be copied to a flash drive or booted by `mkosi qemu`
+Bootable=yes
+
+# Console: console=ttyS0 (serial)
+#          console=tty0 (screen)
+# split_lock_detect=off is required for the UP board and the LattePanda board. 
+#                       more investigation is required.
+# no_hash_pointers: required for memorizer output to make sense.
+# nokaslr: required for memorizer output to make sense.
+# maxcpus=1: memorizer doesn't handle parallel execution well.
+# selinux, audit: I don't know. Maybe these aren't required.
+# loglevel=8: season to taste.
+# memalloc_size=2: measured in GB.
+# earlycon=efifb keep_bootcon : if your system hangs, try adding these to see more console details
+KernelCommandLine=maxcpus=1 split_lock_detect=off no_hash_pointers nokaslr selinux=0 audit=0 loglevel=8 rw memalloc_size=4 
+
+# Set everything up for Qemu
+Output=focal.raw
+QCow2=no
+
+# Season to taste
+Hostname=VM
+
+[Content]
+
+# Everything that might be required to run a test.
+# Add more & rebuild if you discover something missing
+BasePackages=yes
+Packages=
+    less
+    vim-tiny
+
+# A "slightly" larger set of packages:
+#Packages=
+#    apt
+#    apt-utils
+#    bind9-dnsutils
+#    build-essential
+#    curl
+#    gcc-multilib
+#    git
+#    g++-multilib
+#    iproute2
+#    iputils-ping
+#    isc-dhcp-client
+#    openssh-client
+#    vim
+#    wget
+
+# Easy-to-remember password
+Password=root
+Autologin=yes
+
+# Season to taste
+[Partitions]
+RootSize=2G
+
+[Host]
+# Add or delete whatever you need to get it to run.
+# QemuHeadless=yes
+QemuSmp=1
+QemuMem=8G
+QemuKvm=yes
+#QemuBoot=linux
+#QemuArgs=-kernel ../o/arch/x86/boot/bzImage -smp 1 -cpu max,pmu=off
+QemuArgs=-smp 1 -cpu max,pmu=off
+# Add "-s -S" to QemuArgs for gdb
+# Netdev=yes
+# Ssh=yes
diff --git a/VM/mkosi.finalize b/VM/mkosi.finalize
new file mode 100755
index 000000000000..10c0d58f22d7
--- /dev/null
+++ b/VM/mkosi.finalize
@@ -0,0 +1,8 @@
+#!/bin/sh -ex
+env
+/bin/pwd
+echo $SUDO_USER > $BUILDROOT/sudo_user
+ls -l $BUILDROOT/sudo_user
+
+/bin/cp ../o/arch/x86/boot/bzImage $BUILDROOT/efi/v6/linux
+/bin/cp $BUILDROOT/boot/initrd.img-* $BUILDROOT/efi/v6/initrd
diff --git a/VM/mkosi.postinst b/VM/mkosi.postinst
new file mode 100755
index 000000000000..703295a4b225
--- /dev/null
+++ b/VM/mkosi.postinst
@@ -0,0 +1,5 @@
+#!/bin/sh
+
+systemctl enable systemd-networkd.service
+
+
diff --git a/VM/mkosi.skeleton/doit.sh b/VM/mkosi.skeleton/doit.sh
new file mode 100755
index 000000000000..92a9a63db269
--- /dev/null
+++ b/VM/mkosi.skeleton/doit.sh
@@ -0,0 +1,30 @@
+#!/bin/bash -ex
+
+cd /sys/kernel/debug/memorizer
+
+#cfg_log_on
+#cfgmap
+#clear_dead_objs
+#clear_printed_list
+#global_table
+#kmap
+echo 0 > memorizer_enabled
+echo 0 > memorizer_log_access
+echo 1 > clear_dead_objs
+echo 0 > print_live_obj
+echo 1 > clear_printed_list
+[[ $(wc -l <kmap) == 0 ]]
+
+echo 1 > memorizer_enabled
+echo 1 > memorizer_log_access
+/bin/ls
+echo 0 > memorizer_enabled
+echo 0 > memorizer_log_access
+[[ $(wc -l <kmap) != 0 ]]
+wc -l kmap
+wc -l kmap
+cp kmap /tmp/kmap
+D="/data/robadams/data/$(uname -r)"
+U=robadams
+ssh $U@_gateway mkdir -p "$D"
+scp /tmp/kmap $U@_gateway:"$D/$(date -Iseconds)"
diff --git a/VM/mkosi.skeleton/efi/loader/entries/v6-debug.conf b/VM/mkosi.skeleton/efi/loader/entries/v6-debug.conf
new file mode 100644
index 000000000000..5ce1b101b90c
--- /dev/null
+++ b/VM/mkosi.skeleton/efi/loader/entries/v6-debug.conf
@@ -0,0 +1,6 @@
+title      Linux 6.0 memorizer
+version    Linux 6.0 Memorizer
+machine-id 19771c2f821b4e3591c241f5ad42d759
+options    split_lock_detect=off earlycon=efifb keep_bootcon selinux=0 audit=0 no_hash_pointers nokaslr loglevel=8 rw memalloc_size=2
+linux /v6/linux
+initrd /v6/initrd
diff --git a/VM/mkosi.skeleton/efi/loader/entries/v6.conf b/VM/mkosi.skeleton/efi/loader/entries/v6.conf
new file mode 100644
index 000000000000..bac9c3dc0189
--- /dev/null
+++ b/VM/mkosi.skeleton/efi/loader/entries/v6.conf
@@ -0,0 +1,6 @@
+title      Linux 6.0 memorizer
+version    Linux 6.0 Memorizer
+machine-id 19771c2f821b4e3591c241f5ad42d759
+options    split_lock_detect=off selinux=0 audit=0 no_hash_pointers nokaslr loglevel=4 rw memalloc_size=2
+linux /v6/linux
+initrd /v6/initrd
diff --git a/VM/mkosi.skeleton/efi/loader/loader.conf b/VM/mkosi.skeleton/efi/loader/loader.conf
new file mode 100644
index 000000000000..f4732f0a991c
--- /dev/null
+++ b/VM/mkosi.skeleton/efi/loader/loader.conf
@@ -0,0 +1,2 @@
+timeout 15
+console-mode keep
diff --git a/VM/mkosi.skeleton/efi/v6/README b/VM/mkosi.skeleton/efi/v6/README
new file mode 100644
index 000000000000..f802623b236a
--- /dev/null
+++ b/VM/mkosi.skeleton/efi/v6/README
@@ -0,0 +1,2 @@
+After building the kernel, before running `mkosi build`, copy the kernel and some initrd here
+as `linux` and `initrd`, respectively.
diff --git a/arch/x86/boot/Makefile b/arch/x86/boot/Makefile
index 9e38ffaadb5d..9c401d96efe3 100644
--- a/arch/x86/boot/Makefile
+++ b/arch/x86/boot/Makefile
@@ -22,6 +22,8 @@ OBJECT_FILES_NON_STANDARD	:= y
 # so boot code is not interesting anyway.
 KCOV_INSTRUMENT		:= n
 
+MEMORIZER_INSTRUMENT	:= n
+
 # If you want to preset the SVGA mode, uncomment the next line and
 # set SVGA_MODE to whatever number you want.
 # Set it to -DSVGA_MODE=NORMAL_VGA if you just want the EGA/VGA mode.
diff --git a/arch/x86/boot/compressed/Makefile b/arch/x86/boot/compressed/Makefile
index 3a261abb6d15..ea400030dc62 100644
--- a/arch/x86/boot/compressed/Makefile
+++ b/arch/x86/boot/compressed/Makefile
@@ -26,6 +26,8 @@ OBJECT_FILES_NON_STANDARD	:= y
 # Prevents link failures: __sanitizer_cov_trace_pc() is not linked in.
 KCOV_INSTRUMENT		:= n
 
+MEMORIZER_INSTRUMENT	:= n
+
 targets := vmlinux vmlinux.bin vmlinux.bin.gz vmlinux.bin.bz2 vmlinux.bin.lzma \
 	vmlinux.bin.xz vmlinux.bin.lzo vmlinux.bin.lz4 vmlinux.bin.zst
 
@@ -62,7 +64,9 @@ KBUILD_AFLAGS  := $(KBUILD_CFLAGS) -D__ASSEMBLY__
 GCOV_PROFILE := n
 UBSAN_SANITIZE :=n
 
-KBUILD_LDFLAGS := -m elf_$(UTS_MACHINE)
+# Memorizer: added allowing multiple definitions so that inlining libs works with
+# the compressed vmlinux too. There's probably a better way...
+KBUILD_LDFLAGS := -m elf_$(UTS_MACHINE) --allow-multiple-definition
 KBUILD_LDFLAGS += $(call ld-option,--no-ld-generated-unwind-info)
 # Compressed kernel should be built as PIE since it may be loaded at any
 # address by the bootloader.
diff --git a/arch/x86/boot/string.h b/arch/x86/boot/string.h
index e5d2c6b8c2f1..fbc86582a06d 100644
--- a/arch/x86/boot/string.h
+++ b/arch/x86/boot/string.h
@@ -3,7 +3,9 @@
 #define BOOT_STRING_H
 
 /* Undef any of these macros coming from string_32.h. */
+#ifdef CONFIG_INLINE_LIBS
 #undef memcpy
+#endif
 #undef memset
 #undef memcmp
 
diff --git a/arch/x86/configs/memorizer.config b/arch/x86/configs/memorizer.config
new file mode 100644
index 000000000000..d67a7c9598f4
--- /dev/null
+++ b/arch/x86/configs/memorizer.config
@@ -0,0 +1,4 @@
+CONFIG_KASAN=y
+CONFIG_MEMORIZER=y
+CONFIG_MEMORIZER_STATS=y
+CONFIG_X86_KERNEL_IBT=y
diff --git a/arch/x86/configs/rob.config b/arch/x86/configs/rob.config
new file mode 100644
index 000000000000..3a602b795ae9
--- /dev/null
+++ b/arch/x86/configs/rob.config
@@ -0,0 +1,44 @@
+CONFIG_X86_X2APIC=y
+CONFIG_BOOT_VESA_SUPPORT=y
+CONFIG_X86_KERNEL_IBT=y
+CONFIG_SYSFB=y
+CONFIG_SYSFB_SIMPLEFB is not set
+CONFIG_VT_HW_CONSOLE_BINDING=y
+CONFIG_DRM_FBDEV_EMULATION=y
+CONFIG_DRM_FBDEV_OVERALLOC=100
+CONFIG_DRM_SIMPLEDRM=y
+CONFIG_FB_NOTIFY=y
+CONFIG_FB=y
+CONFIG_FIRMWARE_EDID=y
+CONFIG_FB_CFB_FILLRECT=y
+CONFIG_FB_CFB_COPYAREA=y
+CONFIG_FB_CFB_IMAGEBLIT=y
+CONFIG_FB_SYS_FILLRECT=y
+CONFIG_FB_SYS_COPYAREA=y
+CONFIG_FB_SYS_IMAGEBLIT=y
+CONFIG_FB_FOREIGN_ENDIAN is not set
+CONFIG_FB_SYS_FOPS=y
+CONFIG_FB_DEFERRED_IO=y
+CONFIG_FB_MODE_HELPERS=y
+CONFIG_FB_TILEBLITTING=y
+CONFIG_FB_VGA16=y
+CONFIG_FB_UVESA=y
+CONFIG_FB_VESA=y
+CONFIG_FB_EFI=y
+CONFIG_FB_OPENCORES=y
+CONFIG_FB_NVIDIA=y
+CONFIG_FB_VIRTUAL=y
+CONFIG_FB_SIMPLE=y
+CONFIG_FRAMEBUFFER_CONSOLE=y
+CONFIG_FRAMEBUFFER_CONSOLE_DETECT_PRIMARY=y
+CONFIG_EFIVAR_FS=y
+CONFIG_INLINE_LIBS=y
+CONFIG_IKCONFIG=y
+CONFIG_IKCONFIG_PROC=y
+CONFIG_DEBUG_INFO=y
+# CONFIG_DEBUG_INFO_NONE is not set
+CONFIG_DEBUG_INFO_DWARF_TOOLCHAIN_DEFAULT=y
+# CONFIG_DEBUG_INFO_REDUCED is not set
+# CONFIG_DEBUG_INFO_COMPRESSED is not set
+# CONFIG_DEBUG_INFO_SPLIT is not set
+CONFIG_GDB_SCRIPTS=y
diff --git a/arch/x86/entry/vdso/Makefile b/arch/x86/entry/vdso/Makefile
index 3e88b9df8c8f..9c50cfe8f988 100644
--- a/arch/x86/entry/vdso/Makefile
+++ b/arch/x86/entry/vdso/Makefile
@@ -21,6 +21,8 @@ OBJECT_FILES_NON_STANDARD	:= y
 # Prevents link failures: __sanitizer_cov_trace_pc() is not linked in.
 KCOV_INSTRUMENT		:= n
 
+MEMORIZER_INSTRUMENT		:= n
+
 VDSO64-$(CONFIG_X86_64)		:= y
 VDSOX32-$(CONFIG_X86_X32_ABI)	:= y
 VDSO32-$(CONFIG_X86_32)		:= y
@@ -100,10 +102,10 @@ $(vobjs): KBUILD_AFLAGS += -DBUILD_VDSO
 #
 # vDSO code runs in userspace and -pg doesn't help with profiling anyway.
 #
-CFLAGS_REMOVE_vclock_gettime.o = -pg
-CFLAGS_REMOVE_vdso32/vclock_gettime.o = -pg
-CFLAGS_REMOVE_vgetcpu.o = -pg
-CFLAGS_REMOVE_vsgx.o = -pg
+CFLAGS_REMOVE_vclock_gettime.o = -pg -finstrument-functions
+CFLAGS_REMOVE_vdso32/vclock_gettime.o = -pg -finstrument-functions
+CFLAGS_REMOVE_vgetcpu.o = -pg -finstrument-functions
+CFLAGS_REMOVE_vsgx.o = -pg -finstrument-functions
 
 #
 # X32 processes use x32 vDSO to access 64bit kernel data.
diff --git a/arch/x86/include/asm/uaccess.h b/arch/x86/include/asm/uaccess.h
index 1cc756eafa44..fa6bea50b697 100644
--- a/arch/x86/include/asm/uaccess.h
+++ b/arch/x86/include/asm/uaccess.h
@@ -670,3 +670,29 @@ do {									\
 
 #endif /* _ASM_X86_UACCESS_H */
 
+
+#if 0
+// TODO memorizer - rethink what needs to be done
+diff a/arch/x86/include/asm/uaccess.h b/arch/x86/include/asm/uaccess.h	(rejected hunks)
+@@ -677,8 +677,18 @@ extern struct movsl_mask {
+ 
+ unsigned long __must_check _copy_from_user(void *to, const void __user *from,
+ 					   unsigned n);
+-unsigned long __must_check _copy_to_user(void __user *to, const void *from,
+-					 unsigned n);
++
++#ifdef CONFIG_INLINE_LIBS
++__attribute__((always_inline)) static unsigned long __must_check _copy_to_user(void __user *to, const void *from, unsigned n);
++inline static unsigned long _copy_to_user(void __user *to, const void *from, unsigned n)
++{
++  if (access_ok(VERIFY_WRITE, to, n))
++    n = __copy_to_user(to, from, n);
++  return n;
++}
++#else
++unsigned long __must_check _copy_to_user(void __user *to, const void *from, unsigned n);
++#endif
+ 
+ extern void __compiletime_error("usercopy buffer size is too small")
+ __bad_copy_user(void);
+#endif
diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
index f901658d9f7c..45c4a9c80536 100644
--- a/arch/x86/kernel/Makefile
+++ b/arch/x86/kernel/Makefile
@@ -38,6 +38,8 @@ KMSAN_SANITIZE_nmi.o					:= n
 # non-deterministic coverage.
 KCOV_INSTRUMENT		:= n
 
+MEMORIZER_INSTRUMENT					:= n
+
 CFLAGS_irq.o := -I $(srctree)/$(src)/../include/asm/trace
 
 obj-y			+= head_$(BITS).o
diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c
index d1d92897ed6b..b6f9faa0d4ef 100644
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@ -952,7 +952,9 @@ void __init alternative_instructions(void)
 	 */
 	apply_alternatives(__alt_instructions, __alt_instructions_end);
 
+#ifdef CONFIG_X86_KERNEL_IBT
 	apply_ibt_endbr(__ibt_endbr_seal, __ibt_endbr_seal_end);
+#endif
 
 #ifdef CONFIG_SMP
 	/* Patch to UP if other cpus not imminent. */
diff --git a/arch/x86/kernel/setup.c b/arch/x86/kernel/setup.c
index 892609cde4a2..4efab87fbd94 100644
--- a/arch/x86/kernel/setup.c
+++ b/arch/x86/kernel/setup.c
@@ -25,6 +25,7 @@
 #include <linux/static_call.h>
 #include <linux/swiotlb.h>
 #include <linux/random.h>
+#include <linux/memorizer.h>
 
 #include <uapi/linux/mount.h>
 
@@ -1250,6 +1251,8 @@ void __init setup_arch(char **cmdline_p)
 	if (!early_xdbc_setup_hardware())
 		early_xdbc_register_console();
 
+	memorizer_init();
+
 	x86_init.paging.pagetable_init();
 
 	kasan_init();
diff --git a/arch/x86/purgatory/Makefile b/arch/x86/purgatory/Makefile
index 17f09dc26381..fd478b0ce1a5 100644
--- a/arch/x86/purgatory/Makefile
+++ b/arch/x86/purgatory/Makefile
@@ -56,6 +56,10 @@ ifdef CONFIG_RETPOLINE
 PURGATORY_CFLAGS_REMOVE		+= $(RETPOLINE_CFLAGS)
 endif
 
+ifdef CONFIG_MEMORIZER
+PURGATORY_CFLAGS_REMOVE		+= -finstrument-functions
+endif
+
 ifdef CONFIG_CFI_CLANG
 PURGATORY_CFLAGS_REMOVE		+= $(CC_FLAGS_CFI)
 endif
diff --git a/arch/x86/realmode/Makefile b/arch/x86/realmode/Makefile
index a0b491ae2de8..ba96026a90f9 100644
--- a/arch/x86/realmode/Makefile
+++ b/arch/x86/realmode/Makefile
@@ -10,6 +10,7 @@
 # Sanitizer runtimes are unavailable and cannot be linked here.
 KASAN_SANITIZE			:= n
 KCSAN_SANITIZE			:= n
+MEMORIZER_INSTRUMENT		:= n
 
 subdir- := rm
 
diff --git a/arch/x86/realmode/rm/Makefile b/arch/x86/realmode/rm/Makefile
index f614009d3e4e..8a3a3f7da58d 100644
--- a/arch/x86/realmode/rm/Makefile
+++ b/arch/x86/realmode/rm/Makefile
@@ -16,6 +16,8 @@ OBJECT_FILES_NON_STANDARD	:= y
 # Prevents link failures: __sanitizer_cov_trace_pc() is not linked in.
 KCOV_INSTRUMENT		:= n
 
+MEMORIZER_INSTRUMENT	:= n
+
 always-y := realmode.bin realmode.relocs
 
 wakeup-objs	:= wakeup_asm.o wakemain.o video-mode.o
diff --git a/drivers/firmware/efi/libstub/Makefile b/drivers/firmware/efi/libstub/Makefile
index ef5045a53ce0..e8d8a54bdd8f 100644
--- a/drivers/firmware/efi/libstub/Makefile
+++ b/drivers/firmware/efi/libstub/Makefile
@@ -64,6 +64,8 @@ OBJECT_FILES_NON_STANDARD	:= y
 # Prevents link failures: __sanitizer_cov_trace_pc() is not linked in.
 KCOV_INSTRUMENT			:= n
 
+MEMORIZER_INSTRUMENT		:= n
+
 lib-y				:= efi-stub-helper.o gop.o secureboot.o tpm.o \
 				   file.o mem.o random.o randomalloc.o pci.o \
 				   skip_spaces.o lib-cmdline.o lib-ctype.o \
diff --git a/include/asm-generic/uaccess.h b/include/asm-generic/uaccess.h
index a5be9e61a2a2..900bc306ad9d 100644
--- a/include/asm-generic/uaccess.h
+++ b/include/asm-generic/uaccess.h
@@ -233,3 +233,78 @@ __must_check long strncpy_from_user(char *dst, const char __user *src,
 __must_check long strnlen_user(const char __user *src, long n);
 
 #endif /* __ASM_GENERIC_UACCESS_H */
+
+#if 0
+// TODO memorizer
+diff a/include/asm-generic/uaccess.h b/include/asm-generic/uaccess.h	(rejected hunks)
+@@ -103,6 +103,35 @@ static inline __must_check long __copy_from_user(void *to,
+ #endif
+ 
+ #ifndef __copy_to_user
++#ifdef CONFIG_INLINE_LIBS
++__attribute__((always_inline)) static inline __must_check long __copy_to_user(void __user *to,
++		const void *from, unsigned long n)
++{
++	if (__builtin_constant_p(n)) {
++		switch(n) {
++		case 1:
++			*(u8 __force *)to = *(u8 *)from;
++			return 0;
++		case 2:
++			*(u16 __force *)to = *(u16 *)from;
++			return 0;
++		case 4:
++			*(u32 __force *)to = *(u32 *)from;
++			return 0;
++#ifdef CONFIG_64BIT
++		case 8:
++			*(u64 __force *)to = *(u64 *)from;
++			return 0;
++#endif
++		default:
++			break;
++		}
++	}
++
++	memcpy((void __force *)to, from, n);
++	return 0;
++}
++#else
+ static inline __must_check long __copy_to_user(void __user *to,
+ 		const void *from, unsigned long n)
+ {
+@@ -131,6 +160,7 @@ static inline __must_check long __copy_to_user(void __user *to,
+ 	return 0;
+ }
+ #endif
++#endif
+ 
+ /*
+  * These are the main single-value transfer routines.  They automatically
+@@ -267,6 +297,17 @@ static inline long copy_from_user(void *to,
+ 	return res;
+ }
+ 
++#ifdef CONFIG_INLINE_LIBS
++__attribute__((always_inline)) static inline long copy_to_user(void __user *to,
++		const void *from, unsigned long n)
++{
++	might_fault();
++	if (access_ok(VERIFY_WRITE, to, n))
++		return __copy_to_user(to, from, n);
++	else
++		return n;
++}
++#else
+ static inline long copy_to_user(void __user *to,
+ 		const void *from, unsigned long n)
+ {
+@@ -276,6 +317,7 @@ static inline long copy_to_user(void __user *to,
+ 	else
+ 		return n;
+ }
++#endif
+ 
+ /*
+  * Copy a null terminated string from userspace.
+#endif
diff --git a/include/linux/kasan-checks.h b/include/linux/kasan-checks.h
index 3d6d22a25bdc..ed872ecaa57f 100644
--- a/include/linux/kasan-checks.h
+++ b/include/linux/kasan-checks.h
@@ -16,9 +16,13 @@
  * to validate access to an address.   Never use these in header files!
  */
 #if defined(CONFIG_KASAN_GENERIC) || defined(CONFIG_KASAN_SW_TAGS)
+enum AllocType kasan_obj_type(const void *p, unsigned int size);
 bool __kasan_check_read(const volatile void *p, unsigned int size);
 bool __kasan_check_write(const volatile void *p, unsigned int size);
 #else
+static inline void kasan_obj_type(const void *p, unsigned int size)
+{
+}
 static inline bool __kasan_check_read(const volatile void *p, unsigned int size)
 {
 	return true;
diff --git a/include/linux/log2.h b/include/linux/log2.h
index 9f30d087a128..990443ae7fe7 100644
--- a/include/linux/log2.h
+++ b/include/linux/log2.h
@@ -256,3 +256,44 @@ int __bits_per(unsigned long n)
 	__bits_per(n)				\
 )
 #endif /* _LINUX_LOG2_H */
+
+#if 0
+// TODO memorizer -- do we need this?
+diff a/include/linux/log2.h b/include/linux/log2.h	(rejected hunks)
+@@ -15,12 +15,6 @@
+ #include <linux/types.h>
+ #include <linux/bitops.h>
+ 
+-/*
+- * deal with unrepresentable constant logarithms
+- */
+-extern __attribute__((const, noreturn))
+-int ____ilog2_NaN(void);
+-
+ /*
+  * non-constant log of base 2 calculators
+  * - the arch may override these in asm/bitops.h if they can be implemented
+@@ -85,7 +79,7 @@ unsigned long __rounddown_pow_of_two(unsigned long n)
+ #define ilog2(n)				\
+ (						\
+ 	__builtin_constant_p(n) ? (		\
+-		(n) < 1 ? ____ilog2_NaN() :	\
++		(n) < 2 ? 0 :			\
+ 		(n) & (1ULL << 63) ? 63 :	\
+ 		(n) & (1ULL << 62) ? 62 :	\
+ 		(n) & (1ULL << 61) ? 61 :	\
+@@ -148,10 +142,7 @@ unsigned long __rounddown_pow_of_two(unsigned long n)
+ 		(n) & (1ULL <<  4) ?  4 :	\
+ 		(n) & (1ULL <<  3) ?  3 :	\
+ 		(n) & (1ULL <<  2) ?  2 :	\
+-		(n) & (1ULL <<  1) ?  1 :	\
+-		(n) & (1ULL <<  0) ?  0 :	\
+-		____ilog2_NaN()			\
+-				   ) :		\
++		1 ) :				\
+ 	(sizeof(n) <= 4) ?			\
+ 	__ilog2_u32(n) :			\
+ 	__ilog2_u64(n)				\
+
+TODO memorizer */
+#endif
diff --git a/include/linux/memorizer.h b/include/linux/memorizer.h
new file mode 100644
index 000000000000..c57601c04661
--- /dev/null
+++ b/include/linux/memorizer.h
@@ -0,0 +1,182 @@
+/*===-- LICENSE ------------------------------------------------------------===
+ * Developed by:
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2015, Nathan Dautenhahn
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ *===------------------------------------------------------------------------===
+ *
+ *       Filename:  memorizer.h
+ *
+ *    Description:  Memorizer records data for kernel object lifetime analysis.
+ *
+ *===------------------------------------------------------------------------===
+ */
+
+#ifndef _LINUX_MEMORIZER_H
+#define _LINUX_MEMORIZER_H
+
+#include <linux/types.h>
+
+#if 1
+#define FILTER_KASAN 1
+#endif
+
+/**
+ * struct memorizer_kobj - metadata for kernel objects
+ */
+enum AllocType {
+    MEM_STACK=0,
+    MEM_STACK_FRAME,
+    MEM_STACK_ARGS,
+    MEM_STACK_PAGE,
+    MEM_HEAP,
+    MEM_UFO_HEAP,
+    MEM_GLOBAL,
+    MEM_KMALLOC,
+    MEM_KMALLOC_ND,
+    MEM_KMEM_CACHE,
+    MEM_KMEM_CACHE_ND,
+    MEM_VMALLOC,
+    MEM_ALLOC_PAGES,
+    MEM_ALLOC_PAGES_EXACT,
+    MEM_ALLOC_PAGES_GETFREEPAGES,
+    MEM_ALLOC_PAGES_FOLIO,
+    MEM_INDUCED,
+    MEM_BOOTMEM,
+    MEM_MEMBLOCK,
+    MEM_UFO_MEMBLOCK,
+    MEM_MEMORIZER,
+    MEM_MZ_USER,
+    MEM_BUG,
+    MEM_UFO_GLOBAL,
+    MEM_UFO_NONE,
+    /* TODO: Legacy type, fix in tracking code to not use */
+    MEM_NONE,
+    NumAllocTypes
+};
+
+/* Storage for global metadata table. Used for offline processing of globals */
+extern char * global_table_text;
+extern char * global_table_ptr;
+
+/* Special value to indicate the alloc_ip of preallocated objects */
+#define MEMORIZER_PREALLOCED 0xfeedbeef
+
+#ifdef CONFIG_MEMORIZER /*----------- !CONFIG_MEMORIZER -------------------- */
+
+/* Special codes */
+enum MEMORIZER_CODES {
+    /* Assume this is the compiler but don't know */
+    MEM_KASAN_N = 0x5, /* for KASAN with no ret ip */
+};
+
+/* Init and Misc */
+void __init memorizer_init(void);
+void memorizer_alloc_init(void);
+
+/* Memorize access */
+void memorizer_mem_access(uintptr_t addr, size_t size, bool write, uintptr_t ip);
+
+/* Allocation memorization */
+void memorizer_kmalloc(unsigned long call_site, const void *ptr, size_t
+              bytes_req, size_t bytes_alloc, gfp_t gfp_flags);
+void memorizer_kmalloc_node(unsigned long call_site, const void *ptr, size_t
+               bytes_req, size_t bytes_alloc, gfp_t gfp_flags, int
+               node);
+void memorizer_kfree(unsigned long call_site, const void *ptr);
+void memorizer_alloc_pages(unsigned long call_site, struct page *page, unsigned
+        int order, gfp_t gfp_flags);
+void memorizer_alloc_pages_exact(unsigned long call_site, void * ptr, unsigned int size, gfp_t gfp_flags);
+void memorizer_alloc_getfreepages(unsigned long call_site, struct page *page, unsigned
+        int order, gfp_t gfp_flags);
+void memorizer_alloc_folio(unsigned long call_site, struct page *page, unsigned
+        int order, gfp_t gfp_flags);
+
+void memorizer_start_getfreepages(void);
+void memorizer_end_getfreepages(void);
+
+void memorizer_free_pages(unsigned long call_site, struct page *page, unsigned
+              int order);
+
+void memorizer_kmem_cache_alloc(unsigned long call_site, const void *ptr,
+        struct kmem_cache *s, gfp_t gfp_flags);
+void memorizer_kmem_cache_alloc_node (unsigned long call_site, const void *ptr,
+        struct kmem_cache *s, gfp_t gfp_flags, int node);
+bool memorizer_kmem_cache_set_alloc(unsigned long call_site, const void *ptr);
+
+void memorizer_kmem_cache_free(unsigned long call_site, const void *ptr);
+void memorizer_vmalloc_alloc(unsigned long call_site, const void *ptr, unsigned long size, gfp_t gfp_flags);
+void memorizer_vmalloc_free(unsigned long call_site, const void *ptr);
+void memorizer_register_global(const void *ptr, size_t size);
+void memorizer_stack_alloc(unsigned long call_site, const void *ptr, size_t
+        size);
+void memorizer_fork(struct task_struct *p, long nr);
+void memorizer_print_stats(void);
+void memorizer_stack_page_alloc(struct task_struct * task);
+void memorizer_alloc_bootmem(unsigned long call_site, void * v, uint64_t size);
+void memorizer_memblock_alloc(phys_addr_t base, phys_addr_t size);
+
+/* Temporary Debug and test code */
+int __memorizer_get_opsx(void);
+int __memorizer_get_allocs(void);
+void __memorizer_print_events(unsigned int num_events);
+
+#else /*----------- !CONFIG_MEMORIZER ------------------------- */
+
+static inline void __init memorizer_init(void) {}
+static inline void memorizer_alloc_init(void) {}
+static inline void memorizer_mem_access(uintptr_t addr, size_t size, bool write, uintptr_t ip) {}
+static inline void __memorizer_get_opsx(void) {}
+static inline void __memorizer_print_events(unsigned int num_events) {}
+static inline void memorizer_kmalloc(unsigned long call_site, const void *ptr, size_t bytes_req, size_t bytes_alloc, gfp_t gfp_flags) {}
+static inline void memorizer_kmalloc_node(unsigned long call_site, const void *ptr, size_t bytes_req, size_t bytes_alloc, gfp_t gfp_flags, int node) {}
+static inline void memorizer_kfree(unsigned long call_site, const void *ptr) {}
+static inline void memorizer_alloc_pages(unsigned long call_site, struct page *page, unsigned int order, gfp_t gfp_flags) {}
+static inline void memorizer_alloc_pages_exact(unsigned long call_site, void * ptr, unsigned int size, gfp_t gfp_flags){}
+static inline void memorizer_free_pages(unsigned long call_site, struct page *page, unsigned int order) {}
+static inline void memorizer_kmem_cache_alloc(unsigned long call_site, const void *ptr,
+        struct kmem_cache *s, gfp_t gfp_flags) {}
+static inline bool memorizer_kmem_cache_set_alloc(unsigned long call_site, const void *ptr){return true;}
+static inline void memorizer_kmem_cache_alloc_node (unsigned long call_site, const void *ptr,
+        struct kmem_cache *s, gfp_t gfp_flags, int node) {}
+static inline void memorizer_kmem_cache_free(unsigned long call_site, const void *ptr) {}
+static inline void memorizer_vmalloc_alloc(unsigned long call_site, const void *ptr, unsigned long size, gfp_t gfp_flags) {}
+static inline void memorizer_vmalloc_free(unsigned long call_site, const void *ptr) {}
+static inline void memorizer_register_global(const void *ptr, size_t size) {}
+static inline void memorizer_alloc(unsigned long call_site, const void *ptr,
+                   size_t size, enum AllocType AT){}
+static inline void memorizer_fork(struct task_struct *p, long nr) {}
+static inline void memorizer_print_stats(void) {}
+static inline void memorizer_stack_page_alloc(struct task_struct * task){}
+static inline void memorizer_stack_alloc(unsigned long call_site, const void *ptr, size_t size){}
+static inline void memorizer_alloc_bootmem(unsigned long call_site, void * v, uint64_t size){}
+static inline void memorizer_memblock_alloc(unsigned long base, unsigned long size){}
+static inline void memorizer_alloc_getfreepages(unsigned long call_site, struct page *page, unsigned
+                        int order, gfp_t gfp_flags){}
+
+static inline void memorizer_alloc_folio(
+		unsigned long call_site, struct page *page, unsigned
+		int order, gfp_t gfp_flags){}
+static inline void memorizer_start_getfreepages(void){}
+static inline void memorizer_end_getfreepages(void){}
+
+#endif /* CONFIG_MEMORIZER */
+
+#endif /* __MEMORIZER_H_ */
diff --git a/include/linux/sched.h b/include/linux/sched.h
index ffb6eb55cd13..4d9ed4a9e7e3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1359,6 +1359,9 @@ struct task_struct {
 #if defined(CONFIG_KASAN_GENERIC) || defined(CONFIG_KASAN_SW_TAGS)
 	unsigned int			kasan_depth;
 #endif
+#ifdef CONFIG_MEMORIZER
+	unsigned long memorizer_recursion;
+#endif
 
 #ifdef CONFIG_KCSAN
 	struct kcsan_ctx		kcsan_ctx;
diff --git a/include/linux/slab.h b/include/linux/slab.h
index 45efc6c553b8..e8c17641ca80 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -146,6 +146,7 @@
 				(unsigned long)ZERO_SIZE_PTR)
 
 #include <linux/kasan.h>
+#include <linux/memorizer.h>
 
 struct list_lru;
 struct mem_cgroup;
diff --git a/include/linux/string.h b/include/linux/string.h
index cf7607b32102..4282d4fbd2cd 100644
--- a/include/linux/string.h
+++ b/include/linux/string.h
@@ -23,11 +23,46 @@ extern void *memdup_user_nul(const void __user *, size_t);
 extern char * strcpy(char *,const char *);
 #endif
 #ifndef __HAVE_ARCH_STRNCPY
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) char * strncpy(char *,const char *, __kernel_size_t);
+extern inline char *strncpy(char *dest, const char *src, size_t count)
+{
+	char *tmp = dest;
+
+	while (count) {
+		if ((*tmp = *src) != 0)
+			src++;
+		tmp++;
+		count--;
+	}
+	return dest;
+}
+#else
 extern char * strncpy(char *,const char *, __kernel_size_t);
 #endif
+#endif
 #ifndef __HAVE_ARCH_STRLCPY
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) size_t strlcpy(char *, const char *, size_t);
+extern inline size_t strlcpy(char *dest, const char *src, size_t size)
+{
+	size_t ret = strlen(src);
+
+	if (size) {
+		size_t len = (ret >= size) ? size - 1 : ret;
+		//memcpy(dest, src, len);
+		int i;
+		for (i = 0; i < len; i++){
+		  dest[i] = src[i];
+		}
+		dest[len] = '\0';
+	}
+	return ret;
+}
+#else
 size_t strlcpy(char *, const char *, size_t);
 #endif
+#endif
 #ifndef __HAVE_ARCH_STRSCPY
 ssize_t strscpy(char *, const char *, size_t);
 #endif
@@ -44,9 +79,31 @@ extern char * strncat(char *, const char *, __kernel_size_t);
 #ifndef __HAVE_ARCH_STRLCAT
 extern size_t strlcat(char *, const char *, __kernel_size_t);
 #endif
+
+// strcmp()
 #ifndef __HAVE_ARCH_STRCMP
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) int strcmp(const char *,const char *);
+extern inline int strcmp(const char *cs, const char *ct)
+{
+	unsigned char c1, c2;
+
+	while (1) {
+		c1 = *cs++;
+		c2 = *ct++;
+		if (c1 != c2)
+			return c1 < c2 ? -1 : 1;
+		if (!c1)
+			break;
+	}
+	return 0;
+}
+#else
 extern int strcmp(const char *,const char *);
 #endif
+#endif
+
+
 #ifndef __HAVE_ARCH_STRNCMP
 extern int strncmp(const char *,const char *,__kernel_size_t);
 #endif
@@ -56,9 +113,24 @@ extern int strcasecmp(const char *s1, const char *s2);
 #ifndef __HAVE_ARCH_STRNCASECMP
 extern int strncasecmp(const char *s1, const char *s2, size_t n);
 #endif
+
+// strchr()
 #ifndef __HAVE_ARCH_STRCHR
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) char * strchr(const char *,int);
+extern inline char *strchr(const char *s, int c)
+{
+  for (; *s != (char)c; ++s)
+    if (*s == '\0')
+      return NULL;
+  return (char *)s;
+}
+#else
 extern char * strchr(const char *,int);
 #endif
+#endif
+
+
 #ifndef __HAVE_ARCH_STRCHRNUL
 extern char * strchrnul(const char *,int);
 #endif
@@ -78,15 +150,53 @@ static inline __must_check char *strstrip(char *str)
 	return strim(str);
 }
 
+
+// strstr()
 #ifndef __HAVE_ARCH_STRSTR
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) char * strstr(const char *, const char *);
+extern inline char *strstr(const char *s1, const char *s2)
+{
+	size_t l1, l2;
+
+	l2 = strlen(s2);
+	if (!l2)
+		return (char *)s1;
+	l1 = strlen(s1);
+	while (l1 >= l2) {
+		l1--;
+		if (!memcmp(s1, s2, l2))
+			return (char *)s1;
+		s1++;
+	}
+	return NULL;
+}
+#else
 extern char * strstr(const char *, const char *);
 #endif
+#endif
+
+
 #ifndef __HAVE_ARCH_STRNSTR
 extern char * strnstr(const char *, const char *, size_t);
 #endif
+
+// strlen()
 #ifndef __HAVE_ARCH_STRLEN
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) __kernel_size_t strlen(const char *);
+extern inline size_t strlen(const char *s)
+{
+  const char *sc;
+  for (sc = s; *sc != '\0'; ++sc){}
+  return sc - s;
+}
+#else
 extern __kernel_size_t strlen(const char *);
 #endif
+#endif
+
+
 #ifndef __HAVE_ARCH_STRNLEN
 extern __kernel_size_t strnlen(const char *,__kernel_size_t);
 #endif
@@ -153,8 +263,22 @@ extern void * memmove(void *,const void *,__kernel_size_t);
 extern void * memscan(void *,int,__kernel_size_t);
 #endif
 #ifndef __HAVE_ARCH_MEMCMP
+#ifdef CONFIG_INLINE_LIBS
+__attribute__((always_inline)) int memcmp(const void *,const void *,__kernel_size_t);
+extern inline int memcmp(const void *cs, const void *ct, size_t count)
+{
+	const unsigned char *su1, *su2;
+	int res = 0;
+
+	for (su1 = cs, su2 = ct; 0 < count; ++su1, ++su2, count--)
+		if ((res = *su1 - *su2) != 0)
+			break;
+	return res;
+}
+#else
 extern int memcmp(const void *,const void *,__kernel_size_t);
 #endif
+#endif
 #ifndef __HAVE_ARCH_BCMP
 extern int bcmp(const void *,const void *,__kernel_size_t);
 #endif
diff --git a/init/main.c b/init/main.c
index aa21add5f7c5..0599755ff90b 100644
--- a/init/main.c
+++ b/init/main.c
@@ -68,6 +68,7 @@
 #include <linux/lockdep.h>
 #include <linux/kmemleak.h>
 #include <linux/padata.h>
+#include <linux/memorizer.h>
 #include <linux/pid_namespace.h>
 #include <linux/device/driver.h>
 #include <linux/kthread.h>
diff --git a/kernel/fork.c b/kernel/fork.c
index 844dfdc8c639..4afe5d1c1c82 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -90,6 +90,7 @@
 #include <linux/compiler.h>
 #include <linux/sysctl.h>
 #include <linux/kcov.h>
+#include <linux/memorizer.h>
 #include <linux/livepatch.h>
 #include <linux/thread_info.h>
 #include <linux/stackleak.h>
@@ -987,6 +988,8 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	if (err)
 		goto free_stack;
 
+    memorizer_stack_page_alloc(tsk);
+
 #ifdef CONFIG_SECCOMP
 	/*
 	 * We must handle setting up seccomp filters once we're under
diff --git a/kernel/locking/Makefile b/kernel/locking/Makefile
index ea925731fa40..c486e98c49fa 100644
--- a/kernel/locking/Makefile
+++ b/kernel/locking/Makefile
@@ -3,6 +3,8 @@
 # and is generally not a function of system call inputs.
 KCOV_INSTRUMENT		:= n
 
+MEMORIZER_INSTRUMENT := n
+
 obj-y += mutex.o semaphore.o rwsem.o percpu-rwsem.o
 
 # Avoid recursion lockdep -> sanitizer -> ... -> lockdep.
diff --git a/kernel/trace/Makefile b/kernel/trace/Makefile
index c6651e16b557..1a8d5fe79f59 100644
--- a/kernel/trace/Makefile
+++ b/kernel/trace/Makefile
@@ -4,6 +4,8 @@
 
 ccflags-remove-$(CONFIG_FUNCTION_TRACER) += $(CC_FLAGS_FTRACE)
 
+KASAN_SANITIZE := n
+
 ifdef CONFIG_FUNCTION_TRACER
 
 # Avoid recursion due to instrumentation.
diff --git a/lib/Kconfig b/lib/Kconfig
index 9bbf8a4b2108..6298efac181d 100644
--- a/lib/Kconfig
+++ b/lib/Kconfig
@@ -710,6 +710,14 @@ config REF_TRACKER
 config SBITMAP
 	bool
 
+config INLINE_LIBS
+	bool "Force inlining of library functions (strlen, memcmp, etc)"
+	default y
+	help
+	  Forces gcc to inline calls to some library functions. This was
+	  added to Memorizer Linux so that these stateless lib functions do
+	  not need to exclusively live in one compartment.
+
 config PARMAN
 	tristate "parman" if COMPILE_TEST
 
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 12dfe6691dd5..98608462b025 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -985,6 +985,24 @@ source "lib/Kconfig.kasan"
 source "lib/Kconfig.kfence"
 source "lib/Kconfig.kmsan"
 
+config MEMORIZER
+	bool "Memorizer: kernel object lifetime access tracing"
+	depends on KASAN
+        help
+          Enables memory allocation and tracing tool that combines compiler
+          instrumentation on all loads/stores with embedded hooks for
+          allocations to track lifetime access patterns for kernel memory
+          objects.
+
+config MEMORIZER_STATS
+	bool "Memorizer stats: Enable the statistic summary for Memorizer"
+	depends on MEMORIZER
+		help
+		  Enables the statistic summary including the number of accesses and
+		  number of shadow object allocated for Memorizer which will slow down the
+		  performance.
+
+
 endmenu # "Memory Debugging"
 
 config DEBUG_SHIRQ
diff --git a/mm/Makefile b/mm/Makefile
index 8e105e5b3e29..25b3fb32afce 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -88,6 +88,7 @@ obj-$(CONFIG_PAGE_POISONING) += page_poison.o
 obj-$(CONFIG_SLAB) += slab.o
 obj-$(CONFIG_SLUB) += slub.o
 obj-$(CONFIG_KASAN)	+= kasan/
+obj-$(CONFIG_MEMORIZER)	+= memorizer/
 obj-$(CONFIG_KFENCE) += kfence/
 obj-$(CONFIG_KMSAN)	+= kmsan/
 obj-$(CONFIG_FAILSLAB) += failslab.o
diff --git a/mm/folio-compat.c b/mm/folio-compat.c
index e1e23b4947d7..5852f397282e 100644
--- a/mm/folio-compat.c
+++ b/mm/folio-compat.c
@@ -111,6 +111,20 @@ struct page *pagecache_get_page(struct address_space *mapping, pgoff_t index,
 	if ((fgp_flags & FGP_HEAD) || !folio || xa_is_value(folio))
 		return &folio->page;
 	return folio_file_page(folio, index);
+#if 0
+// TODO
+diff a/mm/filemap.c b/mm/filemap.c	(rejected hunks)
+@@ -1341,6 +1341,9 @@ struct page *pagecache_get_page(struct address_space *mapping, pgoff_t offset,
+ 				goto repeat;
+ 		}
+ 	}
++	// This function allocates a single page. We can reuse the below
++	// Memorizer hook with order 0 to track 1 page.
++	memorizer_alloc_pages(_RET_IP_, page, 0, gfp_mask);
+ 
+ 	return page;
+ }
+#endif
 }
 EXPORT_SYMBOL(pagecache_get_page);
 
diff --git a/mm/kasan/Makefile b/mm/kasan/Makefile
index d4837bff3b60..4b13b96acc13 100644
--- a/mm/kasan/Makefile
+++ b/mm/kasan/Makefile
@@ -2,6 +2,7 @@
 KASAN_SANITIZE := n
 UBSAN_SANITIZE := n
 KCOV_INSTRUMENT := n
+MEMORIZER_INSTRUMENT := n
 
 # Disable ftrace to avoid recursion.
 CFLAGS_REMOVE_common.o = $(CC_FLAGS_FTRACE)
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 21e66d7f261d..3386ae2752a9 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -226,6 +226,10 @@ static inline bool ____kasan_slab_free(struct kmem_cache *cache, void *object,
 		return true;
 	}
 
+#ifdef FILTER_KASAN
+	return false;
+#endif
+
 	kasan_poison(object, round_up(cache->object_size, KASAN_GRANULE_SIZE),
 			KASAN_SLAB_FREE, init);
 
@@ -366,6 +370,9 @@ static inline void *____kasan_kmalloc(struct kmem_cache *cache,
 	kasan_poison((void *)redzone_start, redzone_end - redzone_start,
 			   KASAN_SLAB_REDZONE, false);
 
+#ifdef FILTER_KASAN
+	return (void *)object;
+#endif
 	/*
 	 * Save alloc info (if possible) for kmalloc() allocations.
 	 * This also rewrites the alloc info when called from kasan_krealloc().
diff --git a/mm/kasan/generic.c b/mm/kasan/generic.c
index 4967988fb3c6..c5b735a19f97 100644
--- a/mm/kasan/generic.c
+++ b/mm/kasan/generic.c
@@ -169,6 +169,8 @@ static __always_inline bool check_region_inline(unsigned long addr,
 	if (unlikely(size == 0))
 		return true;
 
+	memorizer_mem_access(addr, size, write, ret_ip);
+
 	if (unlikely(addr + size < addr))
 		return !kasan_report(addr, size, write, ret_ip);
 
@@ -214,6 +216,9 @@ void kasan_cache_shutdown(struct kmem_cache *cache)
 
 static void register_global(struct kasan_global *global)
 {
+#ifdef CONFIG_MEMORIZER
+	int written;
+#endif
 	size_t aligned_size = round_up(global->size, KASAN_GRANULE_SIZE);
 
 	kasan_unpoison(global->beg, global->size, false);
@@ -221,6 +226,13 @@ static void register_global(struct kasan_global *global)
 	kasan_poison(global->beg + aligned_size,
 		     global->size_with_redzone - aligned_size,
 		     KASAN_GLOBAL_REDZONE, false);
+
+#ifdef CONFIG_MEMORIZER
+	memorizer_register_global(global->beg, global->size);
+	written = sprintf(global_table_ptr, "%p %d %s %s\n", global -> beg,
+			      (int)(global -> size), (char *)(global -> name), (char *)(global -> module_name));
+	global_table_ptr += written;
+#endif
 }
 
 void __asan_register_globals(struct kasan_global *globals, size_t size)
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index abbcc1b0eec5..b64af66913a6 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -125,6 +125,10 @@ static inline bool kasan_requires_meta(void)
 #define KASAN_SLAB_FREETRACK	0xFA  /* freed slab object with free track */
 #define KASAN_GLOBAL_REDZONE	0xF9  /* redzone for global variable */
 
+/* Memorizer-added exports from KASAN */
+bool in_kernel_space(void * p);
+u8 detect_access_kind(void * p);
+
 /* Stack redzone shadow values. Compiler ABI, do not change. */
 #define KASAN_STACK_LEFT	0xF1
 #define KASAN_STACK_MID		0xF2
diff --git a/mm/kasan/kasan_test.c b/mm/kasan/kasan_test.c
index 0d59098f0876..d491e886627f 100644
--- a/mm/kasan/kasan_test.c
+++ b/mm/kasan/kasan_test.c
@@ -12,6 +12,7 @@
 #include <linux/mm.h>
 #include <linux/mman.h>
 #include <linux/module.h>
+#include <linux/kasan.h>
 #include <linux/printk.h>
 #include <linux/random.h>
 #include <linux/slab.h>
diff --git a/mm/kasan/shadow.c b/mm/kasan/shadow.c
index ecb7acb3897c..c1a5c0bf6fba 100644
--- a/mm/kasan/shadow.c
+++ b/mm/kasan/shadow.c
@@ -26,6 +26,128 @@
 
 #include "kasan.h"
 
+
+/* Memorizer-introduced function to classify an access based on the
+   metadata in shadow space made available by KASAN. It returns the
+   shadow value type that it finds. See kasan.h for the possible
+   values. With the current design, it will return 0x00 if the obj
+   is larger than a page. This might make it unsuitable for heap
+   objects, but for stacks and globals it should be very accurate.
+   Now deprecated, see new implementation below.*/
+u8 detect_access_kind(void * p){
+
+    /* get shadow info for access address */
+    u8 shadow_val = *(u8 *)kasan_mem_to_shadow(p);
+    const void *first_poisoned_addr = p;
+
+    /* We now search for a shadow value. We search both forwards and
+       backwards without leaving the current page so we don't trigger
+       any invalid accesses. This may fail if there really is an obj
+       larger than a page, but for now we will accept these as losses.
+       That should be very rare for stacks/globals. A possible
+       extension is searching beyond 1 page, but first checking to see
+       if that will be valid.  */
+    // Calculate the page-aligned address we are on
+    void * p_aligned = (void *)( (long) p & (~((1 << PAGE_SHIFT) - 1)));
+    // Calculate the max forwards search distance
+    long search_size = (long) (p_aligned + PAGE_SIZE - p);
+    // Search forwards
+    while (shadow_val < KASAN_GRANULE_SIZE && first_poisoned_addr < p + search_size) {
+        first_poisoned_addr += KASAN_GRANULE_SIZE;
+        shadow_val = *(u8 *)kasan_mem_to_shadow(first_poisoned_addr);
+    }
+
+    // If no hit, search backwards too. Stay higher than p_aligned
+    first_poisoned_addr = p;
+    while (shadow_val < KASAN_GRANULE_SIZE && first_poisoned_addr > (p_aligned + KASAN_GRANULE_SIZE)) {
+        first_poisoned_addr -= KASAN_GRANULE_SIZE;
+        shadow_val = *(u8 *)kasan_mem_to_shadow(first_poisoned_addr);
+    }
+
+    return shadow_val;
+}
+#if 0
+
+// Another variant of this logic. Still debugging.
+u8 detect_access_kind_alt(void * p){
+
+  // Calculate page-aligned address
+  void * p_aligned = (void *) ((unsigned long) p & (~((1 << PAGE_SHIFT) - 1)));
+
+  // Initialize shadow pointer and current shadow value
+  u8* shadow_ptr = kasan_mem_to_shadow(p_aligned);
+  u8 shadow_val = *shadow_ptr;
+
+  // Set maximum search distance
+  u8* search_max = kasan_mem_to_shadow(p_aligned + 1*PAGE_SIZE);
+  /* Search until we (1) find a valid shadow type identifier, (2)
+     exceed the max search distance, or (3) would go beyond end of
+     shadow space.
+     Note that shadow values that are nonzero but less than
+     KASAN_SHADOW_SCALE encode a partial red zone, and you need
+     to look at the next byte to get the kind. */
+  while (shadow_val < KASAN_SHADOW_SCALE_SIZE &&
+	 shadow_ptr < search_max &&
+	 shadow_ptr < (u8 *)KASAN_SHADOW_END){
+    shadow_ptr++;
+    shadow_val = *shadow_ptr;
+  }
+  return shadow_val;
+}
+#endif
+
+extern char __start_rodata[];
+extern char __bss_start[];
+enum AllocType kasan_obj_type(const void *p, unsigned int size)
+{
+    /* If we are below the Kernel address space */
+	if (p < kasan_shadow_to_mem((void *)KASAN_SHADOW_START)) {
+        /* our pointer is to page 0... null ptr */
+		if ((unsigned long)p < PAGE_SIZE)
+            return MEM_BUG;
+        /* our pointer is in 0 to User space end addr range  */
+		else if ((unsigned long)p < TASK_SIZE)
+            return MEM_MZ_USER;
+        /* crazy other stuff */
+		else
+            return MEM_BUG;
+    } else {
+        /* get shadow info for access address */
+        u8 shadow_val = detect_access_kind((void *)p);
+        switch(shadow_val)
+        {
+            case KASAN_PAGE_REDZONE:
+                return MEM_ALLOC_PAGES;
+#ifdef KASAN_KMALLOC_REDZONE
+            case KASAN_KMALLOC_REDZONE:
+                return MEM_HEAP;
+#endif
+            case KASAN_GLOBAL_REDZONE:
+                return MEM_GLOBAL;
+            case KASAN_STACK_LEFT:
+            case KASAN_STACK_MID:
+            case KASAN_STACK_RIGHT:
+            case KASAN_STACK_PARTIAL:
+                return MEM_STACK_PAGE;
+            default:
+	      /* There are some global objects that are not registered by KASAN.
+		 We can use the section that the address is in to classify it
+		 as an unknown global. We'll count anything in rodata, data or bss.
+		 Very strangely, only a few of the section starts and ends are defined
+		 constants. I wish they were all defined...
+		 For now, taking the beginning of rodata to the end of bss as unknown
+		 global. There are some other sections in there, but we shouldn't
+		 be getting data accesses to them. In the future we could split these
+		 down more finely if we want to.
+	      */
+	      if (p >= (const void *)__start_rodata && p <= (const void *)(__bss_start + 0x01fea000)){
+			return MEM_GLOBAL;
+	      }
+	      return MEM_NONE;
+        }
+    }
+}
+
 bool __kasan_check_read(const volatile void *p, unsigned int size)
 {
 	return kasan_check_range((unsigned long)p, size, false, _RET_IP_);
diff --git a/mm/memblock.c b/mm/memblock.c
index 511d4783dcf1..29c05155d43b 100644
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@ -556,6 +556,7 @@ static void __init_memblock memblock_insert_region(struct memblock_type *type,
 	memblock_set_region_node(rgn, nid);
 	type->cnt++;
 	type->total_size += size;
+	memorizer_memblock_alloc(base,size);
 }
 
 /**
diff --git a/mm/memorizer/FunctionHashTable.c b/mm/memorizer/FunctionHashTable.c
new file mode 100644
index 000000000000..bc5c62b7a143
--- /dev/null
+++ b/mm/memorizer/FunctionHashTable.c
@@ -0,0 +1,248 @@
+#ifndef FUNCTIONHASHTABLE_C
+#define FUNCTIONHASHTABLE_C
+
+#include <linux/printk.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include "FunctionHashTable.h"
+#include "memalloc.h"
+#include "kobj_metadata.h"
+#include "memorizer.h"
+
+
+#define NUMBUCKS 1000000
+
+DEFINE_RWLOCK(fht_rwlock);
+
+struct FunctionHashTable * create_function_hashtable() {
+
+	struct FunctionHashTable * h = memalloc(sizeof(struct FunctionHashTable));
+	h -> buckets = zmemalloc(NUM_BUCKETS * sizeof(struct EdgeBucket *));
+	h -> number_buckets = NUM_BUCKETS;
+
+	return h;
+}
+
+/* Check whether arguments have been pushed to the stack */
+bool push_args_to_stack(struct pt_regs *pt_regs, struct memorizer_kobj*
+last_edge_frame_kobj)
+{
+	/**
+	 * The caller_bp might be 0, for example, entry_SYSCALL_64_fastpath ->
+	 * sys_dup2 where entry_SYSCALL_64_fastpath the first entry function in
+	 * the kernel, then its bp could be 0.
+	 */
+	if (last_edge_frame_kobj != NULL) {
+		/*
+		 * Before stack_trace is turned on, the caller's alloc type would be
+		 * MEM_STACK_PAGE. In this case, we don't know the caller's stack
+		 * frame shadow kobject information.
+		 */
+		if (last_edge_frame_kobj->alloc_type == MEM_STACK_FRAME) {
+			/**
+			 * There are cases that caller rbp and callee rbp's difference is
+			 * larger than THREAD_SIZE, for exmaple, ret_from_intr -> do_IRQ,
+			 * we need to understand how interrupt handle their rbp.
+			 */
+			if(abs(last_edge_frame_kobj->va_ptr - pt_regs->bp + 0x10) < THREAD_SIZE) {
+				/**
+				 * If the caller's sp is not equal to callee's bp, we should
+				 * allocate an argument kobj.
+				 */
+				if(last_edge_frame_kobj->va_ptr != pt_regs->bp + 0x10) {
+					return true;
+				}
+			}
+		}
+	}
+	return false;
+}
+
+/* Update shadow stack frame and argument kobj's meta data and the lookup table. */
+void update_stack_kobj(struct EdgeBucket *new_bucket, struct pt_regs *pt_regs)
+{
+	/** Interrupt confuses the kobj call stack, so we stop tracing interrupt for now */
+#if defined(__x86_64__)
+	uintptr_t caller_bp = *(uintptr_t *)pt_regs->bp;
+	struct memorizer_kobj *last_edge_frame_kobj = lt_get_kobj(caller_bp);
+	/**
+	 * If we find a <caller, callee> pair exists, then update the function's kobj
+	 * and argument kobj metadata type and lookup table.
+	 */
+	lt_insert_kobj(new_bucket->kobj);
+	/**
+	 * Update the function's argument kobj metadata and lookup table in case of
+	 * variable length arguments
+	 */
+	if (push_args_to_stack(pt_regs, last_edge_frame_kobj)) {
+		new_bucket->kobj->args_kobj->size = last_edge_frame_kobj->va_ptr - pt_regs->bp + 0x10;
+		lt_insert_kobj(new_bucket->kobj->args_kobj);
+	}
+#else
+	pr_info("Memorizer stack frame tracing only support x86_64 arch.");
+#endif
+}
+
+/* Create shadow stack frame and argument kobj's meta data and update the lookup table. */
+void create_stack_kobj(uintptr_t to, struct EdgeBucket *new_bucket, struct pt_regs *pt_regs)
+{
+#if defined(__x86_64__)
+	/* Allocate callee's stack frame */
+	uintptr_t caller_bp;
+	struct memorizer_kobj *last_edge_frame_kobj;
+	new_bucket->kobj = create_kobj(to, pt_regs->sp,
+			0x10 + pt_regs->bp - pt_regs->sp, MEM_STACK_FRAME);
+
+	/* Allocate arg_kobj and its size is the difference of the caller's sp and callee's bp*/
+	caller_bp = *(uintptr_t *)pt_regs->bp;
+	last_edge_frame_kobj = lt_get_kobj(caller_bp);
+	if (push_args_to_stack(pt_regs, last_edge_frame_kobj)) {
+		new_bucket->kobj->args_kobj = create_kobj(to, pt_regs->bp + 0x10,
+				last_edge_frame_kobj->va_ptr - pt_regs->bp + 0x10, MEM_STACK_ARGS);
+	} else {
+		/* If no arguments are pushed to the stack, create an argument kobj with size 0 */
+		new_bucket->kobj->args_kobj = create_kobj(to, pt_regs->bp + 0x10,
+				0, MEM_STACK_ARGS);
+	}
+#else
+	pr_info("Memorizer stack frame tracing only support x86_64 arch.");
+#endif
+}
+
+/**
+ * This function puts the <from, to, stack frame kobj, function argument kobj>
+ * tuple into the hash table. When stack_trace_on is disabled, stack frame kobj
+ * points to NULL value. If a stack frame kobj already exists, we use allocation
+ * promotion to overide the existing one.
+ * @ht: hash table pointer
+ * @from: caller's virtual address
+ * @to: callee's virtual address
+ * @pt_regs: a structure for base pointer and stack pointer, calculated at
+ * cyg_profile_function_enter.
+ * @stack_trace_on: if turned on, allocate the stack frame kobj and argument
+ * kobj.
+ */
+void cfg_update_counts(struct FunctionHashTable * ht, uintptr_t from, uintptr_t to,
+		struct pt_regs *pt_regs, bool stack_trace_on)
+{
+	int index;
+	struct EdgeBucket *search, *prev, *new_bucket;
+
+	/* TODO: index might be changed in multi-core env? */
+	write_lock(&fht_rwlock);
+	// pr_crit("Entering: %p -> %p", from,to);
+	// Compute index by xoring the from and to fields then masking away high bits
+	index = (from ^ to) & (ht -> number_buckets - 1);
+
+	// Search for edge. If found, increment count and return
+	search = ht -> buckets[index];
+	prev = ht->buckets[index];
+	new_bucket = NULL;
+
+	while (search != NULL) {
+		if (search -> from == from && search -> to == to) {
+			atomic_long_inc(&search -> count);
+			/**
+			 * Need to check if search-kobj is null or not. If a bucket is
+			 * created before we enable stack trace, then we will get a null for
+			 * kobj.
+			 */
+			if (stack_trace_on && search->kobj != NULL) {
+				update_stack_kobj(search, pt_regs);
+			} else if (stack_trace_on && search->kobj == NULL) {
+				/**
+				 * If a (caller, callee) pair exists before we enabled the stack trace,
+				 * then we need to create a new stack kobj for this frame.
+				 */
+				create_stack_kobj(to, search, pt_regs);
+			}
+			write_unlock(&fht_rwlock);
+			return;
+		} else {
+			// Collision, loop through the linked list
+			prev = search;
+			search = search -> next;
+		}
+	}
+
+	/**
+	 * If we can't find the match, there are two scenarios:
+	 * 1. The hash bucket does not have an entry yet.
+	 * 2. The hash bucket already have an entry (which is a colloision)
+	 * and prev points to that location. The new entry will be appended to
+	 * the end of the linked list.
+	 */
+	if (ht -> buckets[index] == NULL) {
+		// 1) Create new bucket if empty root
+		ht -> buckets[index] = memalloc(sizeof(struct EdgeBucket));
+		new_bucket = ht -> buckets[index];
+	} else if (prev -> next == NULL) {
+		// 2) Insert item onto end of existing chain
+		prev -> next = memalloc(sizeof(struct EdgeBucket));
+		new_bucket = prev -> next;
+	}
+
+	// Update bucket information
+	new_bucket -> from = from;
+	new_bucket -> to = to;
+	atomic_long_set(&ht -> buckets[index] -> count, 1);
+	new_bucket -> next = NULL;
+	new_bucket -> kobj = NULL;
+
+	// Create new stack frame kobj and arguments kobj for callee
+	if (stack_trace_on) {
+		create_stack_kobj(to, new_bucket, pt_regs);
+	}
+	write_unlock(&fht_rwlock);
+
+	return;
+}
+
+// Write hashtable contents (edge hits) to file
+void console_print(struct FunctionHashTable * ht)
+{
+	struct EdgeBucket * b;
+	int index;
+	for (index = 0; index < ht -> number_buckets; index++) {
+		b = ht -> buckets[index];
+		while (b != NULL) {
+			pr_crit("%lx %lx %ld\n", b -> from, b -> to, atomic_long_read(&b -> count));
+			b = b -> next;
+		}
+	}
+}
+
+// Clear the entries
+void cfgmap_clear(struct FunctionHashTable * ht)
+{
+	struct EdgeBucket * b;
+	int index;
+	for (index = 0; index < ht -> number_buckets; index++) {
+		b = ht -> buckets[index];
+		while (b != NULL) {
+			struct EdgeBucket * prev = b;
+			b = b -> next;
+			memset(prev, 0, sizeof(struct EdgeBucket));
+		}
+		ht -> buckets[index] = NULL;
+	}
+}
+
+// Release all allocated memory
+void destroy_function_hashtable(struct FunctionHashTable * ht)
+{
+	struct EdgeBucket * b;
+	int index;
+	for (index = 0; index < ht -> number_buckets; index++) {
+		b = ht -> buckets[index];
+		while (b != NULL) {
+			struct EdgeBucket * prev = b;
+			b = b -> next;
+			memset(prev, 0, sizeof(struct EdgeBucket));
+		}
+	}
+	kfree(ht->buckets);
+	kfree(ht);
+}
+
+#endif
diff --git a/mm/memorizer/FunctionHashTable.h b/mm/memorizer/FunctionHashTable.h
new file mode 100644
index 000000000000..0ed7522fa3cb
--- /dev/null
+++ b/mm/memorizer/FunctionHashTable.h
@@ -0,0 +1,43 @@
+// FunctionHashTable is a lightweight hashtable implementation for tracking
+// call/return edges in uSCOPE.
+
+#ifndef FUNCTIONHASHTABLE_H
+#define FUNCTIONHASHTABLE_H
+
+#include <linux/types.h>
+
+#define NUM_BUCKETS (_AC(1,UL) << 19)
+
+struct EdgeBucket {
+  uintptr_t from, to;
+  atomic_long_t count;
+  struct memorizer_kobj *kobj;
+  struct EdgeBucket * next;
+};
+
+struct FunctionHashTable {
+  struct EdgeBucket ** buckets;
+  int number_buckets;
+  int full_buckets;
+  int stored_items;
+};
+
+// Initialization for the table data structures
+void func_hash_tbl_init(void);
+
+// Create a new FunctionHashTable
+struct FunctionHashTable * create_function_hashtable(void);
+
+// Update the counts for an edge, adding to table if not already there
+void cfg_update_counts(struct FunctionHashTable * ht, uintptr_t from, uintptr_t to, struct pt_regs *pt_regs, bool stack_trace_on);
+
+// Clear entries and reset
+void cfgmap_clear(struct FunctionHashTable * ht);
+
+// Print directly to console TODO: this is just temp hack for check
+void console_print(struct FunctionHashTable * ht);
+
+// Release memory
+void destroy_function_hashtable(struct FunctionHashTable * ht);
+
+#endif
diff --git a/mm/memorizer/Makefile b/mm/memorizer/Makefile
new file mode 100644
index 000000000000..88d0cd8a1541
--- /dev/null
+++ b/mm/memorizer/Makefile
@@ -0,0 +1,12 @@
+KASAN_SANITIZE := n
+KCOV_INSTRUMENT := n
+MEMORIZER_INSTRUMENT := n
+
+CFLAGS_REMOVE_memorizer.o = -pg
+# Function splitter causes unnecessary splits in __asan_load1/__asan_store1
+# see: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=63533
+#
+# FIXME NDD: These flags were copied from kasan, I'm not sure if they are needed.
+CFLAGS_memorizer.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
+
+obj-y := memorizer.o kobj_metadata.o FunctionHashTable.o stats.o memalloc.o
diff --git a/mm/memorizer/README.md b/mm/memorizer/README.md
new file mode 100644
index 000000000000..04789b240b53
--- /dev/null
+++ b/mm/memorizer/README.md
@@ -0,0 +1,79 @@
+# Memorizer
+Memorizer is a tool to track all the allocs, accesses and frees for every object inside the kernel and output them as a CAPMAP, to be used for further analysis.
+
+# Registration
+The memorizer, if compiled into the kernel, is initialized by calling memorizer_init() from inside init/main.c
+The init function sets up the data structures to be used for keeping track of the events and kernel objects
+
+# Memorizer Hooks
+The memorizer uses hooks to track events within the kernel.
+Allocs and Frees are hooked by adding function hooks into the individual allocators, present of slub.c (We're only concerned about slub for now since that what most of the current systems use, although extending it to other allocators (SLAB and SLOB) should be trivial.
+Loads and Stores(Accesses) are tracked by using KASAN's instrumentation for Loads and Stores. It instruments __asan_load*(addr), and __asan_load*(addr) at the time of kernel compilation.
+
+The following table gives a summary of all the Hooks in Memorizer(Needs Revising):
+
+Hook | Type | Location | Recording Function | Description
+--- | --- | --- | --- | ---
+kmem_cache_alloc() | Function Call | slub.c | __memorizer_kmalloc() | Records kmem_cache_alloc()
+kmalloc() | Function Call | slub.c | __memorizer_kmalloc() | Records kmalloc()
+page_alloc() | Funtion Call | page_alloc.c | __memorizer_kmalloc() | Records page_alloc() (NEEDS FIXING)
+globals | Function Call | kasan.c (Check) | __memorizer_kmalloc() | Records globals (NEED to record Alloc Addr)
+loads | KASAN Instrumentation | kasan.c | memorizer_mem_access() | Records loads
+store | KASAN Instrumemtion | kasan.c | memorizer_mem_access() | Records Stores
+kmem_cache_free() | Function Call | slub.c | memorizer_free_kobj() | Records kmem_cache_free()
+kfree() | Function Call | slub.c | memorizer_free_kobj() | Records the kfree()
+
+# CAPMAP
+The memorizer records event data and outputs it in the form of a CAPMAP. A CAPMAP has two types of entries:
+
+## Alloc/Free Information
+These are denoted by non indented lines. Each line represents a kernel object and the information recorded is as follows:
+lloc IP
+* PID
+* Size
+* Alloc Jiffies
+* Free Jiffies
+* Free IP
+* Common name for the Process
+
+## Access Information
+These are denoted by indented lines. Each line represents a memory location that the current memory object has accesses. The information recorded is as follows:
+* Access IP
+* Access PID
+* Number of Writes
+* Number of Reads
+
+# DebugFS layout
+The memorizer uses the debugfs to communicate between the Kernel and User Space. The DebugFS interface is present in the /sys/kernel/debug/memorizer directory and provides controls for the memorizer. The following section details the use of each file in the DebugFS directory.
+
+## memorizer_enabled
+This turns the memorizer On or Off. When the memorizer is disabled, it doesn't track any information. When enabled, it only tracks the allocs and frees. It is enabled by default during bootup.
+
+Enabling the memorizer:
+```
+echo 1 > memorizer_enabled
+```
+Disabling the memorizer:
+```
+echo 0 > memorizer_enabled
+```
+
+## memorizer_log_access
+This enables/disables the tracking for accesses(loads and stores). It is disabled by default during bootup. To ensure complete tracking, both memorizer_enabled and memorizer_log_access should be enabled.
+
+Enabling access logging:
+```
+echo 1 > memorizer_log_access
+```
+Disabling access logging
+```
+echo 0 > memorizer_log_access
+```
+
+## kmap
+The CAPMAP generated can be printed out by reading the kmap file present in the directory. This is similar in design to the trace file in ftrace. A callback has been implemented in the kernel that prints out the kmap to the stdio. The CAPMAP can be saved to the file as follows:
+```
+cat kmap > <path_to_file>
+```
+
+The rest of the features can be controlled the same way and their names are self explanatory. The commands are therefore omitted for brevity.
diff --git a/mm/memorizer/event_structs.h b/mm/memorizer/event_structs.h
new file mode 100644
index 000000000000..2634f13e71c0
--- /dev/null
+++ b/mm/memorizer/event_structs.h
@@ -0,0 +1,20 @@
+/* This file describes the structs to be used to describe the events happening inside the kernel:
+ * 1. ALLOCATIONS
+ * 2. DEALLOCATIONS
+ * 3. ACCESSES
+ * 4. FORKS
+ * These will be used to create stateless logs for Memorizer 2.0
+ * */
+
+#include <linux/sched.h>
+
+/* Event and Access type  enumerations */
+//enum EventType {Memorizer_Mem_Alloc = 0xaa, Memorizer_Mem_Free = 0xbb, Memorizer_Mem_Read = 0xcc, Memorizer_Mem_Write = 0xdd, Memorizer_Fork = 0xee};
+enum AccessType {
+    Memorizer_READ=0,
+    Memorizer_WRITE,
+    Memorizer_Mem_Alloc,
+    Memorizer_Mem_Free,
+    Memorizer_Fork,
+    Memorizer_NULL
+};
\ No newline at end of file
diff --git a/mm/memorizer/kobj_metadata.c b/mm/memorizer/kobj_metadata.c
new file mode 100644
index 000000000000..435360042a2b
--- /dev/null
+++ b/mm/memorizer/kobj_metadata.c
@@ -0,0 +1,500 @@
+/*===-- LICENSE ------------------------------------------------------------===
+ * Developed by:
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2015, Nathan Dautenhahn
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ *===-----------------------------------------------------------------------===
+ *
+ *       Filename:  kobj_metadata.c
+ *
+ *    Description:  Metadata tracking for all kobject allocations. Includes
+ *		    types for metadata as well as data structure
+ *		    implementations.
+ *
+ *===-----------------------------------------------------------------------===
+ */
+
+#include <linux/gfp.h>
+#include <linux/atomic.h>
+#include <linux/slab.h>
+#include <linux/jiffies.h>
+#include <linux/seq_file.h>
+#include <linux/memorizer.h>
+
+#include "kobj_metadata.h"
+#include "memorizer.h"
+#include "stats.h"
+#include "memalloc.h"
+
+#define ALLOC_CODE_SHIFT    59
+#define ALLOC_INDUCED_CODE	(_AC(MEM_INDUCED,UL) << ALLOC_CODE_SHIFT)
+
+/* atomic object counter */
+static atomic_long_t global_kobj_id = ATOMIC_INIT(0);
+
+/* RW Spinlock for access to table */
+DEFINE_RWLOCK(lookup_tbl_rw_lock);
+
+static struct lt_l3_tbl kobj_l3_tbl;
+static struct lt_pid_tbl pid_tbl;
+
+/* Emergency Pools for l1 + l2 pages */
+#define NUM_EMERGENCY_PAGES 200
+struct pages_pool {
+    uintptr_t base;  /* pointer to array of l1/l2 pages */
+    size_t next;        /* index of next available */
+    size_t entries;     /* number of entries to last page */
+    size_t pg_size;     /* size of object for indexing */
+};
+
+//int test_and_set_bit(unsigned long nr, volatile unsigned long *addr);
+volatile unsigned long inlt;
+
+/**
+ * __klt_enter() - increment recursion counter for entry into memorizer
+ *
+ * The primary goal of this is to stop recursive handling of events. Memorizer
+ * by design tracks two types of events: allocations and accesses. Effectively,
+ * while tracking either type we do not want to re-enter and track memorizer
+ * events that are sources from within memorizer. Yes this means we may not
+ * track legitimate access of some types, but these are caused by memorizer and
+ * we want to ignore them.
+ */
+static inline int __klt_enter(void)
+{
+    return test_and_set_bit_lock(0,&inlt);
+}
+
+static __always_inline void __klt_exit(void)
+{
+    return clear_bit_unlock (0,&inlt);
+}
+
+/**
+ * get_pg_from_pool() --- get the next page from the pool
+ *
+ * @pool: the pool to get the next value
+ *
+ * desc: this should not care about the type, so the type info is put into the
+ * pages_pool struct so that we can do pointer arithmetic to find the next
+ * available entry. The pointer is going to be the next index * the size of the
+ * object, which is set on initializing the pool.
+ *
+ */
+uintptr_t get_pg_from_pool(struct pages_pool *pool)
+{
+    pr_info("Getting page from pool (%p). i=%d e=%d\n",
+            (void *)pool->base, (int)pool->next, (int)pool->entries);
+    if (pool->entries == pool->next)
+        return 0;
+    /* next * pg_size is the offset in bytes from the base of the pool */
+    return (uintptr_t) (pool->base + (pool->next++ * pool->pg_size));
+}
+
+struct lt_l1_tbl l1_tbl_pool[NUM_EMERGENCY_PAGES];
+struct pages_pool l1_tbl_reserve =
+{
+    .base = (uintptr_t) &l1_tbl_pool,
+    .next = 0,
+    .entries = NUM_EMERGENCY_PAGES,
+    .pg_size = sizeof(struct lt_l1_tbl)
+};
+
+struct lt_l2_tbl l2_tbl_pool[NUM_EMERGENCY_PAGES];
+struct pages_pool l2_tbl_reserve =
+{
+    .base = (uintptr_t) &l2_tbl_pool,
+    .next = 0,
+    .entries = NUM_EMERGENCY_PAGES,
+    .pg_size = sizeof(struct lt_l2_tbl)
+};
+
+/**
+ * tbl_get_l1_entry() --- get the l1 entry
+ * @addr:	The address to lookup
+ *
+ * Typical table walk starting from top to bottom.
+ *
+ * Return: the return value is a pointer to the entry in the table, which means
+ * it is a double pointer to the object pointed to by the region. To simplify
+ * lookup and setting this returns a double pointer so access to both the entry
+ * and the object in the entry can easily be obtained.
+ */
+static struct memorizer_kobj **tbl_get_l1_entry(uint64_t addr)
+{
+	struct memorizer_kobj **l1e;
+	struct lt_l1_tbl **l2e;
+	struct lt_l2_tbl **l3e;
+
+	/* Do the lookup starting from the top */
+	l3e = lt_l3_entry(&kobj_l3_tbl, addr);
+	if (!*l3e)
+		return NULL;
+	l2e = lt_l2_entry(*l3e, addr);
+	if (!*l2e)
+		return NULL;
+	l1e = lt_l1_entry(*l2e, addr);
+	if (!*l1e)
+		return NULL;
+	return l1e;
+}
+
+/**
+ * l1_alloc() --- allocate an l1 table
+ */
+static struct lt_l1_tbl * l1_alloc(void)
+{
+    struct lt_l1_tbl *l1_tbl;
+    int i = 0;
+
+    l1_tbl = memalloc(sizeof(struct lt_l1_tbl));
+    if (!l1_tbl) {
+        l1_tbl = (struct lt_l1_tbl *) get_pg_from_pool(&l1_tbl_reserve);
+        if (!l1_tbl) {
+            /* while in dev we want to print error and panic */
+            print_stats((size_t)KERN_CRIT);
+            panic("Failed to allocate L1 table for memorizer kobj\n");
+        }
+    }
+
+    /* Zero out the memory */
+    for (i = 0; i < LT_L1_ENTRIES; ++i)
+        l1_tbl->kobj_ptrs[i] = 0;
+
+    /* increment stats counter */
+    track_l1_alloc();
+
+    return l1_tbl;
+}
+
+/**
+ * l2_alloc() - alloc level 2 table
+ */
+static struct lt_l2_tbl * l2_alloc(void)
+{
+    struct lt_l2_tbl *l2_tbl;
+    int i = 0;
+
+    l2_tbl = memalloc(sizeof(struct lt_l2_tbl));
+    if (!l2_tbl) {
+        l2_tbl = (struct lt_l2_tbl *) get_pg_from_pool(&l2_tbl_reserve);
+        if (!l2_tbl) {
+            print_stats((size_t)KERN_CRIT);
+            panic("Failed to allocate L2 table for memorizer kobj\n");
+        }
+    }
+
+    /* Zero out the memory */
+    for (i = 0; i < LT_L2_ENTRIES; ++i)
+        l2_tbl->l1_tbls[i] = 0;
+
+    /* increment stats counter */
+    track_l2_alloc();
+
+    return l2_tbl;
+}
+
+/**
+ * l2_entry_may_alloc() - get the l2 entry and alloc if needed
+ * @l2_tbl:	pointer to the l2 table to look into
+ * @addr:		Pointer of the addr to index into the table
+ *
+ * Check if the l1 table exists, if not allocate.
+ */
+static struct lt_l1_tbl **l2_entry_may_alloc(struct lt_l2_tbl *l2_tbl, uintptr_t
+					     addr)
+{
+	struct lt_l1_tbl **l2e;
+	l2e = lt_l2_entry(l2_tbl, addr);
+	if (unlikely(!*l2e))
+		*l2e = l1_alloc();
+	return l2e;
+}
+
+/**
+ * l3_entry_may_alloc() - get the l3 entry and alloc if needed
+ * @addr:		Pointer of the addr to index into the table
+ *
+ * Check if the l2 table exists, if not allocate.
+ */
+static struct lt_l2_tbl **l3_entry_may_alloc(uintptr_t addr)
+{
+	struct lt_l2_tbl **l3e;
+	l3e = lt_l3_entry(&kobj_l3_tbl, addr);
+	if (unlikely(!*l3e))
+		*l3e = l2_alloc();
+	return l3e;
+}
+
+/**
+ *
+ */
+static bool is_tracked_obj(uintptr_t l1entry)
+{
+	return ((uint64_t) l1entry >> ALLOC_CODE_SHIFT) != (uint64_t)
+		MEM_INDUCED;
+}
+
+/**
+ * is_induced_obj() -
+ *
+ * Args:
+ *   @addr: the virtual address to check
+ *
+ * Description:
+ *	Return the code that is stored in the upper 5 bits of the pointer value.
+ *	This is stored when we detect that we've had an induced allocation. A
+ *	normally tracked allocation will have the value 0 and thus evaluate to
+ *	false.
+ */
+bool is_induced_obj(uintptr_t addr)
+{
+    struct memorizer_kobj **l1e = tbl_get_l1_entry(addr);
+    if (!l1e)
+        return false;
+    return ((uint64_t) *l1e >> ALLOC_CODE_SHIFT) == (uint64_t) MEM_INDUCED;
+}
+
+/**
+ * lt_remove_kobj() --- remove object from the table
+ * @addr: pointer to the beginning of the object
+ *
+ * This code assumes that it will only ever get a remove from the beginning of
+ * the kobj. TODO: check the beginning of the kobj to make sure.
+ *
+ * Return: the kobject at the location that was removed.
+ */
+struct memorizer_kobj * lt_remove_kobj(uintptr_t addr)
+{
+        struct memorizer_kobj **l1e, *kobj;
+        uintptr_t nextobj = 0;
+
+    /*
+     * Get the l1 entry for the addr, if there is not entry then we not only
+     * haven't tracked the object, but we also haven't allocated a l1 page
+     * for the particular address
+     */
+    l1e = tbl_get_l1_entry(addr);
+    if (!l1e)
+        return NULL;
+
+    /* Setup the return: if it is an induced object then no kobj exists */
+    /* the code is in the most significant bits so shift and compare */
+    if (is_tracked_obj((uintptr_t)*l1e)) {
+            kobj = *l1e;
+    } else {
+            kobj = NULL;
+    }
+
+	if(kobj)
+		nextobj = kobj->va_ptr + kobj->size;
+
+    /* For each byte in the object set the l1 entry to NULL */
+    while(nextobj > (uintptr_t)*l1e)
+    {
+            /* *free* the byte by setting NULL */
+            *l1e = 0;
+
+            /* move l1e to the next entry */
+            l1e = tbl_get_l1_entry(++addr);
+
+            /*
+             * we might get an object that ends at the end of a table and
+             * therefore the next call will fail to get the l1 table.
+             */
+            if(!l1e)
+                    break;
+    }
+    return kobj;
+}
+
+inline struct memorizer_kobj * lt_get_kobj(uintptr_t addr)
+{
+    struct memorizer_kobj **l1e = tbl_get_l1_entry(addr);
+    if (l1e && is_tracked_obj((uintptr_t)*l1e))
+        return *l1e;
+    return NULL;
+}
+
+/*
+ * handle_overalpping_insert() -- hanlde the overlapping insert case
+ * @addr:		the virtual address that is currently not vacant
+ * @prev_addr:          `addr` from the immediately preceeding allocation
+ * @new_kobj:           The l1 entry that will be inserted
+ * @obj:                the current l1 entry 
+ *
+ * If a successful klt_insert is followed immediately by another klt_insert
+ * of the same address, assume we have nested allocators (e.g.
+ * `alloc_pages_exact` calls `__get_free_pages`.) Update the old and new
+ * li entries appropriately.
+ *
+ * Otherwise, there are some missing free's, and it isn't clear what is causing them;
+ * however, if we assume objects are allocated before use then the most recent
+ * allocation will be viable for any writes to these regions so we remove the
+ * previous entry and set up its free times with a special code denoting it was
+ * evicted from the table in an erroneous fasion.
+ */
+static void noinline handle_overlapping_insert(uintptr_t addr, uintptr_t prev_addr, struct memorizer_kobj* new_kobj)
+{
+    unsigned long flags;
+    struct memorizer_kobj *obj = lt_get_kobj(addr);
+
+    /*
+     * If there is current obj, or the current object is already
+     * marked free, there is no resolution required.
+     *
+     * If `free_jiffies` is already set, that probably means
+     * that this is the Nth address of an allocation, and we
+     * updated `obj` when we saw the first address of this
+     * allocation.
+     */
+    if (!obj)
+        return;
+    if (obj->free_jiffies)
+        return;
+
+    /*
+     * TODO robadams@illinois.edu
+    if(obj->alloc_type == new_kobj->alloc_type) {
+	    pr_info("memorizer: va_ptr re-allocated:\n  %p %p %p %s\n  %p %p %p %s\n",
+			    (void*)obj->va_ptr,
+			    (void*)obj->alloc_ip, (void*)obj->free_ip,
+			    alloc_type_str(obj->alloc_type),
+			    (void*)new_kobj->va_ptr,
+			    (void*)new_kobj->alloc_ip, (void*)new_kobj->free_ip,
+			    alloc_type_str(new_kobj->alloc_type));
+    }
+    */
+
+    /*
+     * Note we don't need to free because the object is in the free list and
+     * will get expunged later.
+     */
+    write_lock_irqsave(&obj->rwlock, flags);
+    obj->free_jiffies = new_kobj->alloc_jiffies;
+
+    /* TODO robadams@illinois.edu is there room in the l1 entry for one more byte?
+     * That way, we could get rid of magic numbers below.
+     * */
+    if(addr == prev_addr) {
+	    /* This is a nested allocation */
+	    obj->free_ip = new_kobj->alloc_type | 0xfeed00000000;
+    } else {
+	    /* The reason for this duplicate alloc is unknown */
+	    obj->free_ip = new_kobj->alloc_type | 0xdeadbeef00000000;
+    }
+
+    write_unlock_irqrestore(&obj->rwlock, flags);
+}
+
+/**
+ * lt_insert_kobj() - insert kobject into the lookup table
+ * @kobj:	pointer to the kobj to insert
+ *
+ * For each virtual address in the range of the kobj allocation set the l1 table
+ * entry mapping for the virtual address to the kobj pointer. The function
+ * starts by getting the l2 table from the global l3 table. If it doesn't exist
+ * then allocates the table. The same goes for looking up the l1 table for the
+ * given addr. Once the particular l1 table is obtained for the start addr of the
+ * object, iterate through the table setting each entry of the object to the
+ * given kobj pointer.
+ */
+static int __klt_insert(uintptr_t ptr, size_t size, uintptr_t metadata)
+{
+	struct lt_l1_tbl **l2e;
+	struct lt_l2_tbl **l3e;
+	uint64_t l1_i = 0;
+	uintptr_t addr = ptr;
+	uintptr_t kobjend = ptr + size;
+	static uintptr_t prev_addr = 0;
+
+	while (addr < kobjend) {
+		/* Pointer to the l3 entry for addr and alloc if needed */
+		l3e = l3_entry_may_alloc(addr);
+
+		/* Pointer to the l2 entry for addr and alloc if needed */
+		l2e = l2_entry_may_alloc(*l3e, addr);
+
+		/*
+                 * Get the index for this addr for boundary on this l1 table;
+                 * however, TODO, this might not be needed as our table indices
+                 * are page aligned and it might be unlikely allocations are
+                 * page aligned and will not traverse the boundary of an l1
+                 * table. Note that I have not tested this condition yet.
+		 */
+		l1_i = lt_l1_tbl_index(addr);
+
+		while (l1_i < LT_L1_ENTRIES && addr < kobjend) {
+			/* get the pointer to the l1_entry for this addr byte */
+			struct memorizer_kobj **l1e = lt_l1_entry(*l2e,addr);
+
+			/* If it is not null then we are double allocating */
+			if (*l1e)
+				handle_overlapping_insert(addr, prev_addr, (struct memorizer_kobj *)metadata);
+
+			/* insert object pointer in the table for byte addr */
+			*l1e = (struct memorizer_kobj *)metadata;
+
+			/* Track end of the table and the object tracking */
+			addr += 1;
+			++l1_i;
+		}
+	}
+	prev_addr = ptr;
+	return 0;
+}
+
+/**
+ * We create a unique label for each induced allocated object so that we can
+ * easily free. We insert a 5 bit code for the type with the MSB as 0 to make
+ * sure we don't have a false positive with a real address. We then make the 59
+ * least significatn bits a unique identifier for this obj. By inserting this
+ * way the free just finds all matching entries in the table.
+ */
+size_t d = 0;
+int lt_insert_induced(void * ptr, size_t size)
+{
+    uintptr_t label = ((uintptr_t) MEM_INDUCED << ALLOC_CODE_SHIFT) |
+        atomic_long_inc_return(&global_kobj_id);
+    __klt_insert((uintptr_t)ptr, size, label);
+    return 1;
+}
+
+int lt_insert_kobj(struct memorizer_kobj *kobj)
+{
+        return __klt_insert(kobj->va_ptr, kobj->size, (uintptr_t)kobj);
+}
+
+void plt_insert(struct pid_obj pobj)
+{
+	// Insert into the PID Table based on the Key of the Object
+	pid_tbl.pid_obj_list[pobj.key] = pobj;
+}
+
+
+void __init lt_init(void)
+{
+	/* Zero the page dir contents */
+	memset(&kobj_l3_tbl, 0, sizeof(kobj_l3_tbl));
+	// Zero Out the Contents of the PID Table
+	memset(&pid_tbl, 0, sizeof(pid_tbl));
+	/* track that we statically allocated an l3 */
+	track_l3_alloc();
+}
diff --git a/mm/memorizer/kobj_metadata.h b/mm/memorizer/kobj_metadata.h
new file mode 100644
index 000000000000..4734a5f8fd23
--- /dev/null
+++ b/mm/memorizer/kobj_metadata.h
@@ -0,0 +1,195 @@
+/*===-- LICENSE ------------------------------------------------------------===
+ * Developed by:
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2015, Nathan Dautenhahn
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ *===-----------------------------------------------------------------------===
+ *
+ *       Filename:  kobj_metadata.h
+ *
+ *    Description:  Header file for metadata tracking functionality.
+ *
+ *===-----------------------------------------------------------------------===
+ */
+
+#ifndef _KOBJ_METADATA_H_
+#define _KOBJ_METADATA_H_
+
+#include <linux/kallsyms.h>
+#include <linux/rbtree.h>
+#include <linux/rwlock.h>
+#include <linux/sched.h>
+
+/**
+ * struct memorizer_kobj - metadata for kernel objects
+ * @rb_node:		the red-black tree relations
+ * @alloc_ip:		instruction that allocated the object
+ * @va_ptr:		Virtual address of the beginning of the object
+ * @pa_ptr:		Physical address of the beginning of object
+ * @size:		Size of the object
+ * @jiffies:		Time stamp of creation
+ * @pid:		PID of the current task
+ * @comm:		Executable name
+ * @kobj_list:		List of all objects allocated
+ * @access_counts:	List of memory access count structures
+ * @arg_kobj:   Pointer points to a function argument kobj.
+ *
+ * This data structure captures the details of allocated objects
+ */
+struct memorizer_kobj {
+	struct rb_node	    rb_node;
+	enum AllocType      alloc_type;
+	rwlock_t	    rwlock;
+	long		    obj_id;
+	uintptr_t	    alloc_ip;
+	uintptr_t	    free_ip;
+	uintptr_t	    va_ptr;
+	uintptr_t	    pa_ptr;
+	size_t		    size;
+	unsigned long	    alloc_jiffies;
+	unsigned long	    free_jiffies;
+	pid_t		    pid;
+	char		    comm[TASK_COMM_LEN];
+	char		    funcstr[KSYM_NAME_LEN];
+	bool		    printed;
+	//char		    *modsymb[KSYM_NAME_LEN];
+	char		    *slabname;
+	struct list_head    object_list;
+	struct list_head    access_counts;
+	struct memorizer_kobj *args_kobj;
+};
+
+/**
+ * access_counts - track reads/writes from single source IP
+ */
+struct access_from_counts {
+	struct list_head list;
+	uintptr_t ip;
+	uintptr_t caller;
+	uint64_t pid;
+	uint64_t writes;
+	uint64_t reads;
+};
+
+
+struct pid_obj {
+	uint32_t key;
+	pid_t pid;
+	char comm[TASK_COMM_LEN];
+};
+
+/*
+ * Kernel virtual addresses start at ffff880000000000 - ffffc7ffffffffff (=64
+ * TB) direct mapping of all phys. memory --- see
+ * Documentation/x86/x86_64/mm.txt. This means bit 43 is always set, which means
+ * we can remove all bytes where it is unset: TODO Optimization.
+ *
+ *  63             47 46                   24 23        12 11         0
+ * +-----------------+--*--------------------+------------+------------+
+ * |      ---        |          L3           |     L2     |     L1     |
+ * +-----------------+-----------------------+------------+------------+
+ *
+ * The lookup table maps each byte of allocatable virtual address space to a
+ * pointer to kernel object metadata--> 8 byte pointer.
+ *
+ */
+#define LT_L1_SHIFT		    12
+#define LT_L1_ENTRIES		(_AC(1,UL) << LT_L1_SHIFT)
+#define LT_L1_ENTRY_SIZE	(sizeof(void *))
+#define LT_L1_SIZE		    (LT_L1_ENTRIES * LT_L1_ENTRY_SIZE)
+
+#define LT_L2_SHIFT		    27
+#define LT_L2_ENTRIES		(_AC(1,UL) << (LT_L2_SHIFT - LT_L1_SHIFT))
+#define LT_L2_ENTRY_SIZE	(sizeof(void *))
+#define LT_L2_SIZE		    (LT_L2_ENTRIES * LT_L2_ENTRY_SIZE)
+
+#define LT_L3_SHIFT		    47
+#define LT_L3_ENTRIES		(_AC(1,UL) << (LT_L3_SHIFT - LT_L2_SHIFT))
+#define LT_L3_ENTRY_SIZE	(sizeof(void *))
+#define LT_L3_SIZE		    (LT_L3_ENTRIES * LT_L3_ENTRY_SIZE)
+
+
+#define PID_ENTRIES		    (_AC(1,UL) << 5)
+//PLACEHOLDER VALUE
+//==-- Table data structures -----------------------------------------------==//
+
+/*
+ * Each structure contains an array of pointers to the next level of the lookup.
+ * So the lowest level L1 has an array of pointers to the kobjects, L2 has an
+ * array of pointers to structs of type l2_tbl.
+ */
+struct lt_l1_tbl {
+	struct memorizer_kobj *kobj_ptrs[LT_L1_ENTRIES];
+};
+
+struct lt_l2_tbl {
+	struct lt_l1_tbl *l1_tbls[LT_L2_ENTRIES];
+};
+
+struct lt_l3_tbl {
+	struct lt_l2_tbl *l2_tbls[LT_L3_ENTRIES];
+};
+
+struct lt_pid_tbl {
+	struct pid_obj pid_obj_list[PID_ENTRIES];
+};
+
+#define lt_l1_tbl_index(va)	(va & (LT_L1_ENTRIES - 1))
+#define lt_l2_tbl_index(va)	((va >> LT_L1_SHIFT) & (LT_L2_ENTRIES - 1))
+#define lt_l3_tbl_index(va)	((va >> LT_L2_SHIFT) & (LT_L3_ENTRIES - 1))
+
+/*
+ * lt_l*_entry() --- get the table entry associated with the virtual address
+ *
+ * This uses ** because the value returned is a pointer to the table entry, but
+ * also can be dereferenced to point to the next level down.
+ */
+static inline struct memorizer_kobj **lt_l1_entry(struct lt_l1_tbl *l1_tbl,
+		uintptr_t va)
+{
+	return &(l1_tbl->kobj_ptrs[lt_l1_tbl_index(va)]);
+}
+
+static inline struct lt_l1_tbl **lt_l2_entry(struct lt_l2_tbl *l2_tbl, uintptr_t
+		va)
+{
+	return &l2_tbl->l1_tbls[lt_l2_tbl_index(va)];
+}
+
+static inline struct lt_l2_tbl **lt_l3_entry(struct lt_l3_tbl *l3_tbl, uintptr_t
+		va)
+{
+	return &l3_tbl->l2_tbls[lt_l3_tbl_index(va)];
+}
+
+static inline struct pid_obj * lt_pid(struct lt_pid_tbl *pid_tbl,  uint32_t key)
+{
+	return &(pid_tbl->pid_obj_list[key]);
+}
+
+//==-- External Interface -------------------------------------------------==//
+void lt_init(void);
+int lt_insert_kobj(struct memorizer_kobj *kobj);
+struct memorizer_kobj * lt_remove_kobj(uintptr_t va);
+struct memorizer_kobj * lt_get_kobj(uintptr_t va);
+int lt_insert_induced(void * vaddr, size_t size);
+bool is_induced_obj(uintptr_t va);
+
+#endif /* __KOBJ_METADATA_H_ */
diff --git a/mm/memorizer/memalloc.c b/mm/memorizer/memalloc.c
new file mode 100644
index 000000000000..a1e9533cdab2
--- /dev/null
+++ b/mm/memorizer/memalloc.c
@@ -0,0 +1,101 @@
+/*===-- LICENSE -------------------------------------------------------------===
+ * Developed by:
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2015, Nathan Dautenhahn
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ *===------------------------------------------------------------------------===
+ *
+ * Filename: memalloc.c
+ *
+ * Description: Memorizer declares an isolated boot-time memory region and
+ * use a lock before accessing the memory pointers.
+ *
+ *===------------------------------------------------------------------------===
+ */
+
+// #include <linux/bootmem.h>
+#include <linux/memblock.h>
+#include <linux/memorizer.h>
+
+#include "memalloc.h"
+
+uintptr_t pool_base = 0;
+uintptr_t pool_end = 0;
+uintptr_t pool_next_avail_byte = 0;
+unsigned long memalloc_size = MEMORIZER_POOL_SIZE;
+
+DEFINE_RWLOCK(mem_rwlock);
+
+/* function to let the size be specified as a boot parameter */
+static int __init early_memalloc_size(char *arg)
+{
+	unsigned long sizeGB;
+	if (!arg || kstrtoul(arg, 0, &sizeGB))
+		return 0;
+	memalloc_size = sizeGB << 30;
+	return 1;
+}
+early_param("memalloc_size", early_memalloc_size);
+
+void __init memorizer_alloc_init(void)
+{
+	// pool_base = (uintptr_t)alloc_bootmem(memalloc_size);
+	pool_base = (uintptr_t)memblock_alloc(memalloc_size, SMP_CACHE_BYTES);
+	if (!pool_base)
+		panic("No memorizer pool");
+	pool_end = pool_base + memalloc_size;
+	pool_next_avail_byte = pool_base;
+}
+
+void * memalloc(unsigned long size)
+{
+	unsigned long flags;
+	void *va;
+	write_lock_irqsave(&mem_rwlock, flags);
+	va = (void *)pool_next_avail_byte;
+	if (!pool_next_avail_byte)
+		return 0;
+	if (pool_next_avail_byte + size > pool_end)
+		panic("Memorizer ran out of internal heap: add more with kernel boot flag (# is read as GB): memalloc_size=60");
+	pool_next_avail_byte += size;
+	write_unlock_irqrestore(&mem_rwlock, flags);
+	return va;
+}
+
+void * zmemalloc(unsigned long size)
+{
+	unsigned long i = 0;
+	void * va = memalloc(size);
+	char * vatmp = va;
+	for (i = 0; i < size; i++)
+		vatmp[i] = 0;
+	return va;
+}
+
+void print_pool_info(void)
+{
+	pr_info("Mempool begin: 0x%p, end: 0x%p, size:%llu GB\n", (void *)pool_base,
+		(void *)pool_end, (long long unsigned int)(pool_end-pool_base)>>30);
+}
+
+bool in_pool(unsigned long va)
+{
+	return pool_base < va && va < pool_end;
+}
diff --git a/mm/memorizer/memalloc.h b/mm/memorizer/memalloc.h
new file mode 100644
index 000000000000..67a7660b7b86
--- /dev/null
+++ b/mm/memorizer/memalloc.h
@@ -0,0 +1,43 @@
+/*===-- LICENSE
+ * -------------------------------------------------------------===
+ * Developed by:
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2015, Nathan Dautenhahn
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ *===------------------------------------------------------------------------===
+ *
+ * Filename: memalloc.h
+ *
+ * Description: Memorizer allocates an isolated boot-time memory region for
+ * storing the shadow object and lookup table.
+ *
+ *===------------------------------------------------------------------------===
+ */
+
+#ifndef _MEMALLOC_H_
+#define _MEMALLOC_H_
+
+/* Start with minimally 3GB region else lookup tables will fail */
+#define MEMORIZER_POOL_SIZE     (_AC(1,UL) << 33)
+void * memalloc(unsigned long size);
+void * zmemalloc(unsigned long size);
+void print_pool_info(void);
+bool in_pool(unsigned long va);
+#endif /* __memalloc.h_H_ */
diff --git a/mm/memorizer/memorizer.c b/mm/memorizer/memorizer.c
new file mode 100644
index 000000000000..a82f8aebd249
--- /dev/null
+++ b/mm/memorizer/memorizer.c
@@ -0,0 +1,1691 @@
+/*===-- LICENSE ------------------------------------------------------------===
+ * Developed by:
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2015, Nathan Dautenhahn
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ *===-----------------------------------------------------------------------===
+ *
+ *       Filename:  memorizer.c
+ *
+ *    Description:  Memorizer is a memory tracing tool. It hooks into KASAN
+ *		    events to record object allocation/frees and all
+ *		    loads/stores.
+ *
+ *===-----------------------------------------------------------------------===
+ *
+ * Locking:
+ *
+ *	Memorizer has global and a percpu data structure:
+ *
+ *		- global rbtree of active kernel objects - queue for holding
+ *		  free'd objects that haven't logged - A percpu event queue to
+ *		  track memory access events (Not used in current version, ignore)
+ *
+ * 		- Global objects: object_list, memorizer_kobj, pool_next_avail_byte,
+ * 		  function hash table, and lookup table.
+ *
+ *     Therefore, we have the following locks:
+ *		- object_list_spinlock:
+ *
+ *			Lock for the list of all objects. This list is added to
+ *			on each kobj free. On log this queue should collect any
+ *			queued writes in the local PerCPU access queues and then
+ *			remove it from the list.
+ *
+ *		- memorizer_kobj.rwlock:
+ *
+ *			RW spinlock for access to object internals.
+ *
+ *		- mem_rwlock:
+ *
+ * 			Lock for memory's next available byte pointer.
+ *
+ * 		- fht_rwlock:
+ *
+ * 			Lock for function hash table. This lock is to protect
+ * 			the function list when a new bucket is inserted. Note,
+ * 			we don't need a read or write lock for updating the function
+ * 			count because we use an atomic variable for the count.
+ *
+ * 		- lookup_tbl_rw_lock:
+ *
+ * 			TODO: Need investigate whether we need this lock.
+ *
+ *===-----------------------------------------------------------------------===
+
+ * Per-CPU data:
+ *  	- inmem:
+ *
+ * 			inmem makes sure we don't have re-entrance problem. We make this
+ * 			a per-cpu data so that each core can execute Memorizer in parallel.
+ *
+ *===-----------------------------------------------------------------------===
+ *
+ * Re-Entrance:
+ *
+ *	This system hooks all memory reads/writes and object allocation,
+ *	therefore any external function called will re-enter via ld/st
+ *	instrumentation as well as from allocations. So to avoid this we must be
+ *	very careful about any external functions called to ensure correct
+ *	behavior. This is particulary critical of the memorize access function.
+ *	The others can call external, but note that the memory ld/st as a
+ *	response to that call will be recorded.
+ *
+ *===-----------------------------------------------------------------------===
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/bug.h>
+#include <linux/gfp.h>
+#include <linux/cpumask.h>
+#include <linux/debugfs.h>
+#include <linux/err.h>
+#include <linux/export.h>
+#include <linux/jiffies.h>
+#include <linux/kallsyms.h>
+#include <linux/kernel.h>
+#include <linux/memorizer.h>
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/mm_types.h>
+#include <linux/preempt.h>
+#include <linux/printk.h>
+#include <asm/page_64.h>
+#include <linux/rbtree.h>
+#include <linux/rwlock.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/smp.h>
+#include <linux/workqueue.h>
+#include <asm/atomic.h>
+#include <asm/bitops.h>
+#include <asm/percpu.h>
+#include <linux/relay.h>
+#include <asm-generic/bug.h>
+#include <linux/cdev.h>
+#include <linux/vmalloc.h>
+#include <linux/fs.h>
+#include <linux/sched.h>
+#include <linux/sched/task_stack.h>
+// #include <linux/bootmem.h>
+#include <linux/kasan-checks.h>
+#include <linux/mempool.h>
+
+#include<asm/fixmap.h>
+
+#include "kobj_metadata.h"
+#include "event_structs.h"
+#include "FunctionHashTable.h"
+#include "memorizer.h"
+#include "stats.h"
+#include "util.h"
+#include "memalloc.h"
+#include "../slab.h"
+#include "../kasan/kasan.h"
+
+
+
+//==-- Debugging and print information ------------------------------------==//
+#define MEMORIZER_DEBUG		1
+#define FIXME			0
+
+#define INLINE_EVENT_PARSE	1
+#define WORKQUEUES		0
+
+#define CALL_SITE_STRING	1
+#define TASK_STRING		1
+
+//==-- Prototype Declarations ---------------------------------------------==//
+static void inline __memorizer_kmalloc(unsigned long call_site, const void *ptr,
+		uint64_t bytes_req, uint64_t bytes_alloc,
+		gfp_t gfp_flags, enum AllocType AT);
+static inline struct memorizer_kobj * __create_kobj(uintptr_t call_site, uintptr_t
+		ptr, uint64_t size, enum AllocType AT);
+static struct memorizer_kobj * add_heap_UFO(uintptr_t va);
+//==-- Data types and structs for building maps ---------------------------==//
+#define global_table_text_size 1024 * 1024 * 10
+char * global_table_text;
+char * global_table_ptr;
+
+//==-- PER CPU data structures and control flags --------------------------==//
+// memorizer atomic flag: when set it means we are operating in memorizer. The
+// point of the flag is so that if we use code outside of memorizer or an
+// interrupt occurs, it won't reenter and go down an infinite loop of
+// recursion.
+DEFINE_PER_CPU(int, recursive_depth = 0);
+
+/*
+ * Flags to keep track of whether or not to track writes
+ *
+ * Make this and the next open for early boot param manipulation via bootloader
+ * kernel args: root=/hda1 memorizer_enabled=[yes|no]
+ */
+static bool memorizer_enabled = false;
+static bool memorizer_enabled_boot = true;
+static int __init early_memorizer_enabled(char *arg)
+{
+	if (!arg)
+		return 0;
+	if (strcmp(arg,"yes") == 0) {
+		pr_info("Enabling boot alloc logging\n");
+		memorizer_enabled_boot = true;
+	}
+	if (strcmp(arg,"no") == 0) {
+		pr_info("Disable boot alloc logging\n");
+		memorizer_enabled_boot = false;
+	}
+	return 1;
+}
+early_param("memorizer_enabled_boot", early_memorizer_enabled);
+
+/* flag enable/disable memory access logging */
+static bool memorizer_log_access = false;
+static bool mem_log_boot = false;
+static int __init early_mem_log_boot(char *arg)
+{
+	if (!arg)
+		return 0;
+	if (strcmp(arg,"yes") == 0) {
+		pr_info("Enabling boot accessing logging\n");
+		mem_log_boot= true;
+	}
+	if (strcmp(arg,"no") == 0) {
+		pr_info("Disabling boot accessing logging\n");
+		mem_log_boot= false;
+	}
+	return 1;
+}
+early_param("mem_log_boot", early_mem_log_boot);
+
+/* flag enable/disable memory access logging */
+static bool cfg_log_on = false;
+static bool cfg_log_boot = false;
+static int __init early_cfg_log_boot(char *arg)
+{
+	if (!arg)
+		return 0;
+	if (strcmp(arg,"yes") == 0) {
+		pr_info("Enabling boot accessing logging\n");
+		cfg_log_boot= true;
+	}
+	if (strcmp(arg,"no") == 0) {
+		pr_info("Disabling boot accessing logging\n");
+		cfg_log_boot= false;
+	}
+	return 1;
+}
+early_param("cfg_log_boot", early_cfg_log_boot);
+
+static bool track_calling_context = false;
+static int __init track_cc(char *arg){
+    if(!arg)
+        return 0;
+    if(strcmp(arg,"yes") == 0) {
+        pr_info("Enabling boot accessing logging\n");
+        track_calling_context = true;
+    }
+	return 1;
+}
+early_param("mem_track_cc", track_cc);
+
+static bool stack_trace_on = false;
+static bool stack_trace_boot = false;
+static int __init early_stack_trace_boot(char *arg)
+{
+	if (!arg)
+		return 0;
+	if (strcmp(arg,"yes") == 0) {
+		pr_info("Enabling boot accessing logging\n");
+		stack_trace_boot = true;
+	}
+	if (strcmp(arg,"no") == 0) {
+		pr_info("Disabling boot accessing logging\n");
+		stack_trace_boot= false;
+	}
+	return 1;
+}
+early_param("stack_trace_boot", early_stack_trace_boot);
+
+/* flag enable/disable printing of live objects */
+static bool print_live_obj = true;
+
+/* Function has table */
+struct FunctionHashTable * cfgtbl;
+
+/* full list of freed kobjs */
+static LIST_HEAD(object_list);
+
+/* global object id reference counter */
+static atomic_long_t global_kobj_id_count = ATOMIC_INIT(0);
+
+/* General kobj for catchall object references */
+static struct memorizer_kobj * general_kobjs[NumAllocTypes];
+
+//==-- Locks --=//
+/* RW Spinlock for access to freed kobject list */
+DEFINE_RWLOCK(object_list_spinlock);
+
+/* Monitor variable to prevent Memorizer from entering itself */
+DEFINE_PER_CPU(unsigned long, inmem);
+
+volatile unsigned long in_getfreepages;
+
+uintptr_t cur_caller = 0;
+
+//--- MEMBLOCK Allocator Tracking ---//
+/* This is somewhat challenging because these blocks are allocated on physical
+ * addresses. So we need to transition them.
+ */
+typedef struct {
+	uintptr_t loc;
+	uint64_t size;
+} memblock_alloc_t;
+memblock_alloc_t memblock_events[100000];
+size_t memblock_events_top = 0;
+bool in_memblocks(uintptr_t va_ptr)
+{
+	int i;
+	uintptr_t pa = __pa(va_ptr);
+	for(i=0;i<memblock_events_top;i++)
+	{
+		uintptr_t base = memblock_events[i].loc;
+		uintptr_t end = memblock_events[i].loc + memblock_events[i].loc;
+		if(pa >= base && pa < end)
+			return true;
+	}
+	return false;
+}
+
+/* global timestamp counter */
+atomic_t timestamp = ATOMIC_INIT(0);
+long get_ts(void) { return atomic_fetch_add(1,&timestamp); }
+
+/**
+ * __memorizer_enter() - increment recursion counter for entry into memorizer
+ *
+ * The primary goal of this is to stop recursive handling of events. Memorizer
+ * by design tracks two types of events: allocations and accesses. Effectively,
+ * while tracking either type we do not want to re-enter and track memorizer
+ * events that are sources from within memorizer. Yes this means we may not
+ * track legitimate access of some types, but these are caused by memorizer and
+ * we want to ignore them.
+ */
+static inline int __memorizer_enter(void)
+{
+    return this_cpu_cmpxchg(inmem, 0, 1);
+}
+
+static __always_inline void __memorizer_exit(void)
+{
+    this_cpu_write(inmem, 0);
+}
+
+/**
+ * __print_memorizer_kobj() - print out the object for debuggin
+ *
+ * Grab reader lock if you want to  make sure things don't get modified while we
+ * are printing
+ */
+void __print_memorizer_kobj(struct memorizer_kobj * kobj, char * title)
+{
+	struct list_head * listptr;
+	struct access_from_counts *entry;
+
+	pr_info("%s: \n", title);
+	pr_info("\tkobj_id:	%ld\n", kobj->obj_id);
+	//pr_info("\talloc_mod:	%s\n", *kobj->modsymb);
+	pr_info("\talloc_func:	%s\n", kobj->funcstr);
+	pr_info("\talloc_ip:	0x%p\n", (void*) kobj->alloc_ip);
+	pr_info("\tfree_ip:	0x%p\n", (void*) kobj->free_ip);
+	pr_info("\tva:		0x%p\n", (void*) kobj->va_ptr);
+	pr_info("\tpa:		0x%p\n", (void*) kobj->pa_ptr);
+	pr_info("\tsize:	%lu\n", kobj->size);
+	pr_info("\talloc jiffies: %lu\n", kobj->alloc_jiffies);
+	pr_info("\tfree jiffies:  %lu\n", kobj->free_jiffies);
+	pr_info("\tpid: %d\n", kobj->pid);
+	pr_info("\texecutable: %s\n", kobj->comm);
+	list_for_each(listptr, &(kobj->access_counts)){
+		entry = list_entry(listptr, struct access_from_counts, list);
+		pr_info("\t  Access IP: %p, PID: %d, Writes: %llu, Reads: %llu\n",
+				//(void *) entry->ip, entry->pid,
+				(void *) entry->ip, 0,
+				(unsigned long long) entry->writes,
+				(unsigned long long) entry->reads);
+	}
+}
+EXPORT_SYMBOL(__print_memorizer_kobj);
+
+void memorizer_print_stats(void)
+{
+    print_stats((size_t)KERN_CRIT);
+}
+EXPORT_SYMBOL(memorizer_print_stats);
+
+//----
+//==-- Memorizer Access Processing ----------------------------------------==//
+//----
+
+static struct access_from_counts *
+__alloc_afc(void)
+{
+	struct access_from_counts * afc = NULL;
+	afc = (struct access_from_counts *)
+	memalloc(sizeof(struct access_from_counts));
+	return afc;
+}
+
+/**
+ * init_access_counts_object() - initialize data for the object
+ * @afc:	object to init
+ * @ip:		ip of access
+ */
+static inline void
+init_access_counts_object(struct access_from_counts *afc, uint64_t ip, pid_t
+		pid)
+{
+	INIT_LIST_HEAD(&(afc->list));
+	afc->ip = ip;
+	afc->writes = 0;
+	afc->reads = 0;
+	if (track_calling_context)
+		afc->caller = cur_caller;
+	else
+		afc->caller = 0;
+}
+
+/**
+ * alloc_new_and_init_access_counts() - allocate a new access count and init
+ * @ip:		the access from value
+ */
+static inline struct access_from_counts *
+alloc_and_init_access_counts(uint64_t ip, pid_t pid)
+{
+	struct access_from_counts * afc = NULL;
+	afc = __alloc_afc();
+	init_access_counts_object(afc, ip, pid);
+	track_access_counts_alloc();
+	return afc;
+}
+
+/**
+ * access_from_counts - search kobj's access_from for an entry from src_ip
+ * @src_ip:	the ip to search for
+ * @kobj:	the object to search within
+ *
+ * This function does not do any locking and therefore assumes the caller will
+ * already have at least a reader lock. This is a big aggregate function, but
+ * given that it will occur a lot we will be searching the list for a given
+ * object, therefore we can easily do insertion if we don't find it, keeping a
+ * linearly monotonic sorted list.
+ *
+ * Here we insert a new entry for each (ip,threadid) tuple.
+ */
+static inline struct access_from_counts *
+unlckd_insert_get_access_counts(uint64_t src_ip, pid_t pid, struct
+		memorizer_kobj *kobj)
+{
+	struct list_head * listptr;
+	struct access_from_counts *entry;
+	struct access_from_counts * afc = NULL;
+	list_for_each (listptr, &(kobj->access_counts)) {
+		entry = list_entry(listptr, struct access_from_counts, list);
+		if (src_ip == entry->ip) {
+			if (kobj->alloc_type == MEM_NONE) {
+				if (entry->caller == cur_caller)
+					return entry;
+				else if (cur_caller < entry->caller)
+					break;
+			} else {
+				return entry;
+			}
+		} else if (src_ip < entry->ip) {
+			break;
+		}
+	}
+	/* allocate the new one and initialize the count none in list */
+	afc = alloc_and_init_access_counts(src_ip, pid);
+	if (afc)
+		list_add_tail(&(afc->list), listptr);
+	return afc;
+}
+
+/**
+ * update_kobj_access() - find and update the object information
+ * @memorizer_mem_access:	The access to account for
+ *
+ * @src_va_ptr: PC for source of operation
+ * @va_ptr: the virtual address being written to
+ * @pid: pid of access
+ * @access_type: type of access (read/write)
+ *
+ * Find the object associated with this memory write, search for the src ip in
+ * the access structures, incr if found or alloc and add new if not.
+ *
+ * Executes from the context of memorizer_mem_access and therefore we are
+ * already operating with interrupts off and preemption disabled, and thus we
+ * cannot sleep.
+ */
+
+static int reports_shown = 0;
+
+static inline int find_and_update_kobj_access(uintptr_t src_va_ptr,
+		uintptr_t va_ptr, pid_t pid, size_t access_type, size_t size)
+{
+	struct memorizer_kobj *kobj = NULL;
+	struct access_from_counts *afc = NULL;
+
+	if (in_pool(va_ptr)) {
+		track_access(MEM_MEMORIZER,size);
+		return -1;
+	}
+
+	/* Get the kernel object associated with this VA */
+	kobj = lt_get_kobj(va_ptr);
+
+	if (!kobj) {
+		if (is_induced_obj(va_ptr)) {
+			kobj = general_kobjs[MEM_INDUCED];
+			track_access(MEM_INDUCED,size);
+		} else if (in_memblocks(va_ptr)) {
+			kobj = __create_kobj(MEM_UFO_MEMBLOCK, va_ptr, size,
+					MEM_UFO_MEMBLOCK);
+			if (!kobj) {
+				kobj = general_kobjs[MEM_MEMBLOCK];
+				track_untracked_access(MEM_MEMBLOCK,size);
+			} else {
+				track_access(MEM_MEMBLOCK,size);
+			}
+		} else {
+			enum AllocType AT = kasan_obj_type((void *)va_ptr,size);
+			kobj =  general_kobjs[AT];
+			switch(AT){
+				case MEM_STACK_PAGE:
+					kobj = __create_kobj(MEM_STACK_PAGE, va_ptr,
+							size, MEM_UFO_GLOBAL);
+					track_access(MEM_STACK_PAGE,size);
+					break;
+                case MEM_HEAP:
+#if 1
+                    // Debugging feature to print a KASAN report for missed heap accesses.
+                        // Only prints up to 5 reports.
+                    if (reports_shown < 5){
+                        kasan_report((unsigned long) va_ptr, size, 1, (unsigned long)&kasan_report);
+                        reports_shown++;
+                    }
+#endif
+                    kobj = add_heap_UFO(va_ptr);
+
+                    track_access(MEM_UFO_HEAP,size);
+                    break;
+                case MEM_GLOBAL:
+                    kobj = __create_kobj(MEM_UFO_GLOBAL, va_ptr,
+                                 size, MEM_UFO_GLOBAL);
+                    track_access(MEM_UFO_GLOBAL,size);
+                    break;
+                case MEM_NONE:
+                    kobj = __create_kobj(MEM_UFO_NONE, va_ptr,
+                                 size, MEM_UFO_NONE);
+                    track_access(MEM_UFO_NONE,size);
+                    break;
+                default:
+                    track_untracked_access(AT,size);
+			}
+		}
+	} else {
+		track_access(kobj->alloc_type, size);
+	}
+
+	/* Grab the object lock here */
+	write_lock(&kobj->rwlock);
+
+	/* Search access queue to the entry associated with src_ip */
+	afc = unlckd_insert_get_access_counts(src_va_ptr, pid, kobj);
+
+	/* increment the counter associated with the access type */
+	if (afc)
+		access_type ? ++(afc->writes) : ++(afc->reads);
+
+	write_unlock(&kobj->rwlock);
+	return afc ? 0 : -1;
+}
+
+//==-- Memorizer memory access tracking -----------------------------------==//
+
+/**
+ * memorizer_mem_access() - record associated data with the load or store
+ * @addr:	The virtual address being accessed
+ * @size:	The number of bits for the load/store
+ * @write:	True if the memory access is a write (store)
+ * @ip:		IP of the invocing instruction
+ *
+ * Memorize, ie. log, the particular data access.
+ */
+void __always_inline memorizer_mem_access(uintptr_t addr, size_t size, bool
+		write, uintptr_t ip)
+{
+	unsigned long flags;
+	if (unlikely(!memorizer_log_access) || unlikely(!memorizer_enabled)) {
+		track_disabled_access();
+		return;
+	}
+
+	if (current->kasan_depth > 0) {
+		track_induced_access();
+		return;
+	}
+
+	if (__memorizer_enter()) {
+		track_induced_access();
+		return;
+	}
+
+	local_irq_save(flags);
+	find_and_update_kobj_access(ip,addr,-1,write,size);
+	local_irq_restore(flags);
+
+	__memorizer_exit();
+}
+
+//==-- Memorizer kernel object tracking -----------------------------------==//
+
+/**
+ * Requires: Calculate the callee's stack frame size
+ * and callee's arg size if arg registers are full.
+ * @ip: is the callee's virtual address.
+ * @parent_ip: is the caller's virtual address.
+ */
+void __cyg_profile_func_enter(void *ip, void *parent_ip)
+{
+	unsigned long flags;
+	struct pt_regs pt_regs;
+
+	if (!cfg_log_on && !stack_trace_on)
+		return;
+	/* Prevent infinete loop */
+	if (__memorizer_enter())
+		return;
+
+	if (track_calling_context)
+		cur_caller = (uintptr_t)parent_ip;
+
+	/* Disable interrupt */
+
+	local_irq_save(flags);
+#if defined(__x86_64__)
+#if INLINE_EVENT_PARSE
+	/**
+	 * | caller sp |
+	 * | ret addr  |
+	 * | callee bp |
+	 * | ...       |
+	 * | callee sp |
+	 * | cyg bp    |
+	 *
+	 * In order to calculate func bp, we need to dereference
+	 * the callee bp and callee bp + 0x10 is the func sp.
+	 */
+
+	if (stack_trace_on) {
+		uintptr_t callee_bp = 0, callee_sp = 0;
+		register uintptr_t cyg_rbp asm("rbp");
+		callee_bp = *(uintptr_t *)cyg_rbp; // deference callee bp
+		callee_sp = cyg_rbp + 0x10; // Prologue pushes the return address (0x8) and RBP (0x8)
+		/* Store function bp and sp into pt_regs structure */
+		pt_regs.bp = callee_bp;
+		pt_regs.sp = callee_sp;
+	}
+
+	/* cfg_update_counts creates <from, to, callee kobj, args kobj> tuple */
+	cfg_update_counts(cfgtbl, (uintptr_t)parent_ip, (uintptr_t)ip, &pt_regs, stack_trace_on);
+#endif
+
+#else
+	pr_info("Memorizer stack frame tracing only support x86_64 arch.");
+#endif
+
+	local_irq_restore(flags);
+	__memorizer_exit();
+}
+EXPORT_SYMBOL(__cyg_profile_func_enter);
+
+/**
+ * Future work: The stack frame kobjs are never free and there are lots
+ * of these kobjs. In the future, we can free the kobjs here and restore
+ * the lookup table pointing to the MEM_STACK_PAGE kobj.
+ * @ip: is the callee's virtual address.
+ * @parent_ip: is the caller's virtual address.
+ */
+void __cyg_profile_func_exit(void *ip, void *parent_ip)
+{
+
+}
+EXPORT_SYMBOL(__cyg_profile_func_exit);
+
+static struct kmem_cache * get_slab_cache(const void * addr)
+{
+	if ((addr >= (void *)PAGE_OFFSET) && (addr < high_memory)) {
+		struct page *page = virt_to_head_page(addr);
+		if (PageSlab(page)) {
+			return page_slab(page)->slab_cache;
+		}
+		return NULL;
+	}
+	return NULL;
+}
+
+/*
+ * If we miss lookup the object from the cache.  Note that the init_kobj will
+ * preset a string for the slab name. So these UFOs are aggregated in an
+ * intelligent and still useful way. We've missed the alloc (and thereofre the
+ * alloc site) but we've at least grouped them by type. Assume we get a page
+ * because we are in this case.
+ */
+static struct memorizer_kobj * add_heap_UFO(uintptr_t va)
+{
+	struct memorizer_kobj *kobj = NULL;
+	if ((va >= (uintptr_t)PAGE_OFFSET) && (va < (uintptr_t)high_memory)) {
+		struct page *page = virt_to_head_page((void *)va);
+		if (PageSlab(page)) {
+			void *object;
+			struct kmem_cache *cache = page_slab(page)->slab_cache;
+			object = nearest_obj(cache, virt_to_slab((void *)va), (void *)va);
+			//pr_err("Object at %p, in cache %s size: %d\n", object,
+			//cache->name, cache->object_size);
+			kobj = __create_kobj(MEM_UFO_HEAP, (uintptr_t)object,
+					cache->object_size,
+					MEM_UFO_HEAP);
+		}
+	}
+	return kobj;
+}
+
+/**
+ * init_kobj() - Initalize the metadata to track the recent allocation
+ */
+static void init_kobj(struct memorizer_kobj * kobj, uintptr_t call_site,
+		uintptr_t ptr_to_kobj, size_t bytes_alloc,
+		enum AllocType AT)
+{
+	struct kmem_cache * cache;
+
+	rwlock_init(&kobj->rwlock);
+	if (atomic_long_inc_and_test(&global_kobj_id_count)) {
+		pr_warn("Global kernel object counter overlapped...");
+	}
+
+	/* Zero out the whole object including the comm */
+	memset(kobj, 0, sizeof(struct memorizer_kobj));
+	kobj->alloc_ip = call_site;
+	kobj->va_ptr = ptr_to_kobj;
+	kobj->pa_ptr = __pa(ptr_to_kobj);
+	kobj->size = bytes_alloc;
+	kobj->alloc_jiffies = get_ts();
+	kobj->free_jiffies = 0;
+	kobj->free_ip = 0;
+	kobj->obj_id = atomic_long_read(&global_kobj_id_count);
+	kobj->printed = false;
+	kobj->alloc_type = AT;
+	kobj->args_kobj = NULL;
+	INIT_LIST_HEAD(&kobj->access_counts);
+	INIT_LIST_HEAD(&kobj->object_list);
+
+	/* get the slab name */
+	cache = get_slab_cache((void *)(kobj->va_ptr));
+	if (cache) {
+		kobj->slabname = memalloc(strlen(cache->name)+1);
+		strncpy(kobj->slabname, cache->name, strlen(cache->name));
+		kobj->slabname[strlen(cache->name)]='\0';
+	} else {
+		kobj->slabname = "no-slab";
+	}
+
+#if CALL_SITE_STRING == 1
+	/* Some of the call sites are not tracked correctly so don't try */
+	if (call_site)
+		kallsyms_lookup((unsigned long) call_site, NULL, NULL,
+				//&(kobj->modsymb), kobj->funcstr);
+			NULL, kobj->funcstr);
+#endif
+#if TASK_STRING == 1
+	/* task information */
+	if (in_irq()) {
+		kobj->pid = 0;
+		strncpy(kobj->comm, "hardirq", sizeof(kobj->comm));
+	} else if (in_softirq()) {
+		kobj->pid = 0;
+		strncpy(kobj->comm, "softirq", sizeof(kobj->comm));
+	} else {
+		kobj->pid = current->pid;
+		/*
+		 * There is a small chance of a race with set_task_comm(),
+		 * however using get_task_comm() here may cause locking
+		 * dependency issues with current->alloc_lock. In the worst
+		 * case, the command line is not correct.
+		 */
+		strncpy(kobj->comm, current->comm, sizeof(kobj->comm));
+	}
+#endif
+
+#if MEMORIZER_DEBUG >= 5
+	__print_memorizer_kobj(kobj, "Allocated and initalized kobj");
+#endif
+}
+
+/**
+ * free_access_from_entry() --- free the entry from the kmem_cache
+ */
+static void free_access_from_entry(struct access_from_counts *afc)
+{
+	//TODO clean up all the kmem_cache_free stuff
+	//kmem_cache_free(access_from_counts_cache, afc);
+	//TODO Create Free function here with new memalloc allocator
+}
+
+/**
+ * free_access_from_list() --- for each element remove from list and free
+ */
+static void free_access_from_list(struct list_head *afc_lh)
+{
+	struct access_from_counts *afc;
+	struct list_head *p;
+	struct list_head *tmp;
+	list_for_each_safe(p, tmp, afc_lh) {
+		afc = list_entry(p, struct access_from_counts, list);
+		list_del(&afc->list);
+		free_access_from_entry(afc);
+	}
+}
+
+/**
+ * free_kobj() --- free the kobj from the kmem_cache
+ * @kobj:	The memorizer kernel object metadata
+ *
+ * FIXME: there might be a small race here between the write unlock and the
+ * kmem_cache_free. If another thread is trying to read the kobj and is waiting
+ * for the lock, then it could get it. I suppose the whole *free_kobj operation
+ * needs to be atomic, which might be proivded by locking the list in general.
+ */
+static void free_kobj(struct memorizer_kobj * kobj)
+{
+	write_lock(&kobj->rwlock);
+	free_access_from_list(&kobj->access_counts);
+	write_unlock(&kobj->rwlock);
+	//kmem_cache_free(kobj_cache, kobj);
+	//TODO add new free function here from memalloc allocator
+	track_kobj_free();
+}
+
+/**
+ * clear_free_list() --- remove entries from free list and free kobjs
+ */
+static void clear_dead_objs(void)
+{
+	struct memorizer_kobj *kobj;
+	struct list_head *p;
+	struct list_head *tmp;
+	unsigned long flags;
+	pr_info("Clearing the free'd kernel objects\n");
+	/* Avoid rentrance while freeing the list */
+	while(!__memorizer_enter())
+		yield();
+	write_lock_irqsave(&object_list_spinlock, flags);
+	list_for_each_safe(p, tmp, &object_list) {
+		kobj = list_entry(p, struct memorizer_kobj, object_list);
+		/* If free_jiffies is 0 then this object is live */
+		if (kobj->free_jiffies > 0) {
+			/* remove the kobj from the free-list */
+			list_del(&kobj->object_list);
+			/* Free the object data */
+			free_kobj(kobj);
+		}
+	}
+	write_unlock_irqrestore(&object_list_spinlock, flags);
+	__memorizer_exit();
+}
+
+/**
+ * clear_printed_objects() --- remove entries from free list and free kobjs
+ */
+static void clear_printed_objects(void)
+{
+	struct memorizer_kobj *kobj;
+	struct list_head *p;
+	struct list_head *tmp;
+	unsigned long flags;
+	pr_info("Clearing the free'd and printed kernel objects\n");
+	__memorizer_enter();
+	write_lock_irqsave(&object_list_spinlock, flags);
+	list_for_each_safe(p, tmp, &object_list) {
+		kobj = list_entry(p, struct memorizer_kobj, object_list);
+		/* If free_jiffies is 0 then this object is live */
+		if (kobj->free_jiffies > 0 && kobj->printed) {
+			/* remove the kobj from the free-list */
+			list_del(&kobj->object_list);
+			/* Free the object data */
+			free_kobj(kobj);
+		}
+	}
+	write_unlock_irqrestore(&object_list_spinlock, flags);
+	__memorizer_exit();
+}
+
+/**
+ * __memorizer_free_kobj - move the specified objec to free list
+ *
+ * @call_site:	Call site requesting the original free
+ * @ptr:	Address of the object to be freed
+ *
+ * Algorithm:
+ *	1) find the object in the rbtree
+ *	2) add the object to the memorizer process kobj queue
+ *	3) remove the object from the rbtree
+ *
+ * Maybe TODO: Do some processing here as opposed to later? This depends on when
+ * we want to add our filtering.
+ * 0xvv
+ */
+void static __memorizer_free_kobj(uintptr_t call_site, uintptr_t kobj_ptr)
+{
+
+	struct memorizer_kobj *kobj;
+	unsigned long flags;
+
+	/* find and remove the kobj from the lookup table and return the
+	 * kobj */
+	kobj = lt_remove_kobj(kobj_ptr);
+
+	/*
+	 *   * If this is null it means we are freeing something we did
+	 *   not insert
+	 *       * into our tree and we have a missed alloc track,
+	 *       otherwise we update
+	 *           * some of the metadata for free.
+	 *               */
+	if (kobj) {
+		/* Update the free_jiffies for the object */
+		write_lock_irqsave(&kobj->rwlock, flags);
+		kobj->free_jiffies = get_ts();
+		kobj->free_ip = call_site;
+		write_unlock_irqrestore(&kobj->rwlock, flags);
+		track_free();
+		//TODO add free function here
+	}
+	else
+		track_untracked_obj_free();
+}
+
+/**
+ * memorizer_free_kobj - move the specified objec to free list
+ * @call_site:	Call site requesting the original free
+ * @ptr:	Address of the object to be freed
+ *
+ * Algorithm:
+ *	1) find the object in the rbtree
+ *	2) add the object to the memorizer process kobj queue
+ *	3) remove the object from the rbtree
+ *
+ * Maybe TODO: Do some processing here as opposed to later? This depends on when
+ * we want to add our filtering.
+ * 0xvv
+ */
+void static memorizer_free_kobj(uintptr_t call_site, uintptr_t kobj_ptr)
+{
+	unsigned long flags;
+
+	if (__memorizer_enter()) {
+		track_induced_free();
+		return;
+	}
+
+	local_irq_save(flags);
+	__memorizer_free_kobj(call_site, kobj_ptr);
+
+	local_irq_restore(flags);
+	__memorizer_exit();
+}
+
+struct memorizer_kobj *create_kobj(uintptr_t call_site, uintptr_t ptr, uint64_t size, enum AllocType AT) {
+	return __create_kobj(call_site, ptr, size, AT);
+}
+
+/**
+ * __create_kobj() - allocate and init kobj assuming locking and rentrance
+ *	protections already enabled.
+ * @call_site:  Address of the call site to the alloc
+ * @ptr:	Pointer to location of data structure in memory
+ * @size:	Size of the allocation
+ * @AT:		Type of allocation
+ */
+static inline struct memorizer_kobj * __create_kobj(uintptr_t call_site,
+		uintptr_t ptr, uint64_t
+		size, enum AllocType AT)
+{
+	struct memorizer_kobj *kobj;
+
+	/* inline parsing */
+	kobj = memalloc(sizeof(struct memorizer_kobj));
+	if (!kobj) {
+		track_failed_kobj_alloc();
+		return NULL;
+	}
+
+	/* initialize all object metadata */
+	init_kobj(kobj, call_site, ptr, size, AT);
+
+	/* memorizer stats tracking */
+	track_alloc(AT);
+
+	/* mark object as live and link in lookup table */
+	lt_insert_kobj(kobj);
+
+	/* Grab the writer lock for the object_list and insert into object list */
+	write_lock(&(list_first_entry(&object_list, struct memorizer_kobj,
+				      object_list))->rwlock);
+	list_add_tail(&kobj->object_list, &object_list);
+	write_unlock(&(list_first_entry(&object_list, struct memorizer_kobj,
+					object_list))->rwlock);
+	return kobj;
+}
+
+/**
+ * memorizer_alloc() - record allocation event
+ * @object:	Pointer to the beginning of hte object
+ * @size:	Size of the object
+ *
+ * Track the allocation and add the object to the set of active object tree.
+ */
+static void inline __memorizer_kmalloc(unsigned long call_site, const void
+		*ptr, uint64_t bytes_req, uint64_t bytes_alloc, gfp_t gfp_flags, enum AllocType AT)
+{
+
+	unsigned long flags;
+
+	if (unlikely(ptr==NULL))
+		return;
+
+	if (unlikely(!memorizer_enabled)) {
+		track_disabled_alloc();
+		return;
+	}
+
+	if (__memorizer_enter()) {
+		/* link in lookup table with dummy event */
+		local_irq_save(flags);
+		lt_insert_induced((void *)ptr,bytes_alloc);
+		track_induced_alloc();
+		local_irq_restore(flags);
+		return;
+	}
+
+	local_irq_save(flags);
+	__create_kobj((uintptr_t) call_site, (uintptr_t) ptr, bytes_alloc, AT);
+	local_irq_restore(flags);
+	__memorizer_exit();
+}
+
+/*** HOOKS similar to the kmem points ***/
+void memorizer_kmalloc(unsigned long call_site, const void *ptr, size_t
+		bytes_req, size_t bytes_alloc, gfp_t gfp_flags)
+{
+	__memorizer_kmalloc(call_site, ptr, bytes_req, bytes_alloc, gfp_flags,
+			MEM_KMALLOC);
+}
+
+void memorizer_kmalloc_node(unsigned long call_site, const void *ptr, size_t
+		bytes_req, size_t bytes_alloc, gfp_t gfp_flags, int
+		node)
+{
+	__memorizer_kmalloc(call_site, ptr, bytes_req, bytes_alloc, gfp_flags,
+			MEM_KMALLOC_ND);
+}
+
+void memorizer_kfree(unsigned long call_site, const void *ptr)
+{
+	/*
+	 * Condition for ensuring free is from online cpu: see trace point
+	 * condition from include/trace/events/kmem.h for reason
+	 */
+	if (unlikely(!cpu_online(raw_smp_processor_id())) || !memorizer_enabled) {
+		return;
+	}
+
+	memorizer_free_kobj((uintptr_t) call_site, (uintptr_t) ptr);
+}
+
+void memorizer_memblock_alloc(phys_addr_t base, phys_addr_t size)
+{
+	memblock_alloc_t * evt = &memblock_events[memblock_events_top++];
+	evt->loc = base;
+	evt->size = size;
+	track_alloc(MEM_MEMBLOCK);
+}
+
+void memorizer_memblock_free(phys_addr_t base, phys_addr_t size)
+{
+}
+
+void memorizer_alloc_bootmem(unsigned long call_site, void * v, uint64_t size)
+{
+	track_alloc(MEM_BOOTMEM);
+	__memorizer_kmalloc(call_site, v, size, size, 0, MEM_BOOTMEM);
+	return;
+}
+
+const char * l1str = "lt_l1_tbl";
+const char * l2str = "lt_l2_tbl";
+const char * memorizer_kobjstr = "memorizer_kobj";
+const char * access_from_countsstr = "access_from_counts";
+bool is_memorizer_cache_alloc(char * cache_str)
+{
+	if (!memstrcmp(l1str,cache_str))
+		return true;
+	if (!memstrcmp(l2str,cache_str))
+		return true;
+	if (!memstrcmp(memorizer_kobjstr,cache_str))
+		return true;
+	if (!memstrcmp(access_from_countsstr,cache_str))
+		return true;
+	return false;
+}
+
+
+void memorizer_vmalloc_alloc(unsigned long call_site, const void *ptr,
+		unsigned long size, gfp_t gfp_flags)
+{
+	if (unlikely(ptr == NULL))
+		return;
+	__memorizer_kmalloc(call_site, ptr, size, size,
+			gfp_flags, MEM_VMALLOC);
+}
+
+void memorizer_vmalloc_free(unsigned long call_site, const void *ptr)
+{
+	memorizer_free_kobj((uintptr_t) call_site, (uintptr_t) ptr);
+}
+
+
+// Update the allocation site of a kmem_cache object, only if has current special
+// value of MEMORIZER_PREALLOCED.
+bool memorizer_kmem_cache_set_alloc(unsigned long call_site, const void * ptr){
+
+  struct memorizer_kobj * kobj = lt_get_kobj((uintptr_t)ptr);
+
+  if (kobj == NULL){
+    return false;
+  } else {
+    if (kobj -> alloc_ip == MEMORIZER_PREALLOCED){
+      kobj -> alloc_ip = call_site;
+    }
+    return true;
+  }
+}
+
+void memorizer_kmem_cache_alloc(unsigned long call_site, const void *ptr,
+		struct kmem_cache *s, gfp_t gfp_flags)
+{
+	if (unlikely(ptr == NULL))
+		return;
+	if (!is_memorizer_cache_alloc((char *)s->name))
+		__memorizer_kmalloc(call_site, ptr, s->object_size, s->size,
+				gfp_flags, MEM_KMEM_CACHE);
+}
+
+void memorizer_kmem_cache_alloc_node (unsigned long call_site, const void *ptr,
+		struct kmem_cache *s, gfp_t gfp_flags, int node)
+{
+	if (unlikely(ptr == NULL))
+		return;
+	if (!is_memorizer_cache_alloc((char *)s->name))
+		__memorizer_kmalloc(call_site, ptr, s->object_size, s->size,
+				gfp_flags, MEM_KMEM_CACHE_ND);
+}
+
+void memorizer_kmem_cache_free(unsigned long call_site, const void *ptr)
+{
+	/*
+	 * Condition for ensuring free is from online cpu: see trace point
+	 * condition from include/trace/events/kmem.h for reason
+	 */
+	if (unlikely(!cpu_online(raw_smp_processor_id())) || !memorizer_enabled) {
+		return;
+	}
+
+	memorizer_free_kobj((uintptr_t) call_site, (uintptr_t) ptr);
+}
+
+
+void memorizer_alloc_pages(unsigned long call_site, struct page *page, unsigned
+		int order, gfp_t gfp_flags)
+{
+
+  if (test_bit(0,&in_getfreepages)){
+    return;
+  }
+    __memorizer_kmalloc(call_site, page_address(page),
+            (uintptr_t) PAGE_SIZE * (1 << order),
+            (uintptr_t) PAGE_SIZE * (1 << order),
+            gfp_flags, MEM_ALLOC_PAGES);
+
+}
+
+/* This is a slight variation to memorizer_alloc_pages(). Alloc_pages() can only return
+ * a power-of-two number of pages, whereas alloc_pages_exact() can return
+ * any specific number of pages. We don't want Memorizer to track the gap
+ * between the two, so use this special memorizer hook for this case. */
+void memorizer_alloc_pages_exact(unsigned long call_site, void * ptr, unsigned
+			   int size, gfp_t gfp_flags)
+{
+
+  // Compute the actual number of bytes that will be allocated
+  unsigned long alloc_size = PAGE_ALIGN(size);
+
+  __memorizer_kmalloc(call_site, ptr,
+		      alloc_size, alloc_size,
+		      gfp_flags, MEM_ALLOC_PAGES_EXACT);
+
+}
+void memorizer_free_pages_exact (unsigned long call_site, struct page *page, unsigned
+		int order)
+{
+	/*
+	 * Condition for ensuring free is from online cpu: see trace point
+	 * condition from include/trace/events/kmem.h for reason
+	 */
+	if (unlikely(!cpu_online(raw_smp_processor_id())) || !memorizer_enabled) {
+		return;
+	}
+	memorizer_free_kobj((uintptr_t) call_site, (uintptr_t)
+			page_address(page));
+}
+
+
+void memorizer_start_getfreepages(){
+  test_and_set_bit_lock(0,&in_getfreepages);
+}
+
+void memorizer_alloc_getfreepages(unsigned long call_site, struct page *page, unsigned
+			   int order, gfp_t gfp_flags)
+{
+    //TODO: Conflict here where one version used 1 << order, other used 2 << order.
+    __memorizer_kmalloc(call_site, page_address(page),
+            (uintptr_t) PAGE_SIZE * (1 << order),
+            (uintptr_t) PAGE_SIZE * (1 << order),
+            gfp_flags, MEM_ALLOC_PAGES_GETFREEPAGES);
+
+    clear_bit_unlock(0,&in_getfreepages);
+}
+
+void memorizer_alloc_folio(unsigned long call_site, struct page *page, unsigned
+			   int order, gfp_t gfp_flags)
+{
+    //TODO: Conflict here where one version used 1 << order, other used 2 << order.
+    __memorizer_kmalloc(call_site, page_address(page),
+            (uintptr_t) PAGE_SIZE * (1 << order),
+            (uintptr_t) PAGE_SIZE * (1 << order),
+            gfp_flags, MEM_ALLOC_PAGES_FOLIO);
+
+    clear_bit_unlock(0,&in_getfreepages);
+}
+
+void memorizer_end_getfreepages() {
+	clear_bit_unlock(0, &in_getfreepages);
+}
+
+void memorizer_free_pages(unsigned long call_site, struct page *page, unsigned
+		int order)
+{
+	/*
+	 * Condition for ensuring free is from online cpu: see trace point
+	 * condition from include/trace/events/kmem.h for reason
+	 */
+	if (unlikely(!cpu_online(raw_smp_processor_id())) || !memorizer_enabled) {
+		return;
+	}
+	memorizer_free_kobj((uintptr_t) call_site, (uintptr_t)
+			page_address(page));
+}
+
+/**
+ *
+ * Thread should have allocated and this stack should be in the table
+ */
+void memorizer_stack_page_alloc(struct task_struct *task)
+{
+	/* get the object */
+	struct memorizer_kobj * stack_kobj = lt_get_kobj((uintptr_t)task->stack);
+	/* if there then just mark it, but it appears to be filtered out */
+	if (!stack_kobj) {
+		void *base = task_stack_page(task);
+		__memorizer_kmalloc(_RET_IP_, base, THREAD_SIZE, THREAD_SIZE,
+				0, MEM_STACK_PAGE);
+	} else {
+		/* change alloc type to stack page alloc */
+		stack_kobj->alloc_type = MEM_STACK_PAGE;
+	}
+}
+
+void memorizer_stack_alloc(unsigned long call_site, const void *ptr, size_t
+		size)
+{
+	__memorizer_kmalloc(call_site, ptr, size, size, 0, MEM_STACK);
+}
+
+void memorizer_register_global(const void *ptr, size_t size)
+{
+	__memorizer_kmalloc(0, ptr, size, size, 0, MEM_GLOBAL);
+}
+
+
+//==-- Memorizer Data Export ----------------------------------------------==//
+static unsigned long seq_flags;
+static bool sequence_done = false;
+extern struct list_head *seq_list_start(struct list_head *head, loff_t pos);
+extern struct list_head *seq_list_next(void *v, struct list_head *head, loff_t
+		*ppos);
+
+/*
+ * kmap_seq_start() --- get the head of the free'd kobj list
+ *
+ * Grab the lock here and give back on close. There is an interesting problem
+ * here in that when the data gets to the page size limit for printing, the
+ * sequence file closes the file and opens up again by coming to the start
+ * location having processed a subset of the list already. The problem with this
+ * is that without having __memorizer_enter() it will add objects to the list
+ * between the calls to show and next opening the potential for an infinite
+ * loop. It also adds elements in between start and stop operations.
+ *
+ * For some reason the start is called every time after a *stop*, which allows
+ * more entries to be added to the list thus requiring the extra sequence_done
+ * flag that I added to detect the end of the list. So we add this flag so that
+ * any entries added after won't make the sequence continue forever in an
+ * infinite loop.
+ */
+static void *kmap_seq_start(struct seq_file *seq, loff_t *pos)
+{
+	__memorizer_enter();
+	write_lock_irqsave(&object_list_spinlock, seq_flags);
+
+	if (list_empty(&object_list))
+		return NULL;
+
+	if (*pos == 0) {
+		sequence_done = false;
+		return object_list.next;
+	}
+
+	/*
+	 * Second call back even after return NULL to stop. This must occur
+	 * after the check to (*pos == 0) otherwise it won't continue after the
+	 * first time a read is executed in userspace. The specs didn't mention
+	 * this but my experiments showed its occurrence.
+	 */
+	if (sequence_done == true)
+		return NULL;
+
+	return seq_list_start(&object_list, *pos);
+}
+
+/*
+ * kmap_seq_next() --- move the head pointer in the list or return null
+ */
+static void *kmap_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	return seq_list_next(v, &object_list, pos);
+}
+
+/*
+ * kmap_seq_show() - print out the object including access info
+ */
+static int kmap_seq_show(struct seq_file *seq, void *v)
+{
+	struct access_from_counts *afc;
+	struct memorizer_kobj *kobj = list_entry(v, struct memorizer_kobj,
+			object_list);
+	char *new_alloc_type = 0;
+	uintptr_t free_ip = 0;
+
+	read_lock(&kobj->rwlock);
+	/* If free_jiffies is 0 then this object is live */
+	if (!print_live_obj && kobj->free_jiffies == 0) {
+		read_unlock(&kobj->rwlock);
+		return 0;
+	}
+	kobj->printed = true;
+
+	/* Print object allocation info */
+	if((kobj->free_ip >> 32) == 0xdeadbeef) {
+		/* This allocation was replaced by another
+		 * allocation with no interveing `free()`
+		 * for reasons unknown. The subsequent
+		 * allocator is in `free_ip`.
+		 */
+		new_alloc_type = alloc_type_str(kobj->free_ip & 0xffff);
+		/* Some post-processing scripts expect to
+		 * see "DEADBEEF" in this case.
+		 */
+		free_ip = 0xdeadbeef;
+	} else if ((kobj->free_ip >> 32) == 0xfeed) {
+		/* This allocation was replaced by another
+		 * allocation with no intervening `free()` due
+		 * to a nested allocation. The subsequent allocator
+		 * is in `free_ip`.
+		 * */
+		new_alloc_type = alloc_type_str(kobj->free_ip & 0xffff);
+		free_ip = 0xfedbeef;
+	} else {
+		/* Normal allocation */
+		new_alloc_type = "";
+		free_ip = kobj->free_ip;
+	}
+	seq_printf(seq,"%-p,%d,%p,%lu,%lu,%lu,%p,%s,%s,%s,%s\n",
+			(void*) kobj->alloc_ip, kobj->pid, (void*) kobj->va_ptr,
+			kobj->size, kobj->alloc_jiffies, kobj->free_jiffies, (void*)
+			free_ip, alloc_type_str(kobj->alloc_type), kobj->comm,
+			kobj->slabname,new_alloc_type);
+
+	/* print each access IP with counts and remove from list */
+	list_for_each_entry(afc, &kobj->access_counts, list) {
+		if (kobj->alloc_type == MEM_NONE && track_calling_context) {
+			seq_printf(seq, "  from:%p,caller:%p,%llu,%llu\n",
+					(void *) afc->ip, (void *)afc->caller,
+					(unsigned long long) afc->writes,
+					(unsigned long long) afc->reads);
+		} else
+			seq_printf(seq, "  %p,%llu,%llu\n",
+					(void *) afc->ip,
+					(unsigned long long) afc->writes,
+					(unsigned long long) afc->reads);
+	}
+
+	read_unlock(&kobj->rwlock);
+	return 0;
+}
+
+/*
+ * kmap_seq_stop() --- clean up on sequence file stopping
+ *
+ * Must release locks and ensure that we can re-enter. Also must set the
+ * sequence_done flag to avoid an infinit loop, which is required so that we
+ * guarantee completions without reentering due to extra allocations between
+ * this invocation of stop and the start that happens.
+ */
+static void kmap_seq_stop(struct seq_file *seq, void *v)
+{
+	if (!v)
+		sequence_done = true;
+	write_unlock_irqrestore(&object_list_spinlock, seq_flags);
+	__memorizer_exit();
+}
+
+static const struct seq_operations kmap_seq_ops = {
+	.start = kmap_seq_start,
+	.next  = kmap_seq_next,
+	.stop  = kmap_seq_stop,
+	.show  = kmap_seq_show,
+};
+
+static int kmap_open(struct inode *inode, struct file *file)
+{
+	return seq_open(file, &kmap_seq_ops);
+}
+
+static ssize_t kmap_write(struct file *file, const char __user *user_buf,
+		size_t size, loff_t *ppos)
+{
+	return 0;
+}
+
+static const struct file_operations kmap_fops = {
+	.owner		= THIS_MODULE,
+	.open		= kmap_open,
+	.write		= kmap_write,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= seq_release,
+};
+
+/*
+ * clear_free_list_write() - call the function to clear the free'd kobjs
+ */
+static ssize_t clear_dead_objs_write(struct file *file, const char __user
+		*user_buf, size_t size, loff_t *ppos)
+{
+	clear_dead_objs();
+	*ppos += size;
+	return size;
+}
+
+static const struct file_operations clear_dead_objs_fops = {
+	.owner		= THIS_MODULE,
+	.write		= clear_dead_objs_write,
+};
+
+/*
+ * clear_printed_free_list_write() - call the function to clear the printed free'd kobjs
+ */
+static ssize_t clear_printed_list_write(struct file *file, const char __user
+		*user_buf, size_t size, loff_t *ppos)
+{
+	clear_printed_objects();
+	*ppos += size;
+	return size;
+}
+
+static const struct file_operations clear_printed_list_fops = {
+	.owner		= THIS_MODULE,
+	.write		= clear_printed_list_write,
+};
+
+static ssize_t cfgmap_write(struct file *file, const char __user
+		*user_buf, size_t size, loff_t *ppos)
+{
+	unsigned long flags;
+	__memorizer_enter();
+	local_irq_save(flags);
+	cfgmap_clear(cfgtbl);
+	local_irq_restore(flags);
+	__memorizer_exit();
+	*ppos += size;
+	return size;
+}
+
+static int cfgmap_seq_show(struct seq_file *seq, void *v)
+{
+	struct EdgeBucket * b;
+	int index;
+	for (index = 0; index < cfgtbl -> number_buckets; index++) {
+		b = cfgtbl -> buckets[index];
+		while (b != NULL) {
+			seq_printf(seq,"%lx %lx %ld\n", b -> from, b -> to, atomic_long_read(&b -> count));
+			b = b -> next;
+		}
+	}
+	return 0;
+}
+
+static int cfgmap_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &cfgmap_seq_show, NULL);
+}
+
+static const struct file_operations cfgmap_fops = {
+	.owner		= THIS_MODULE,
+	.write		= cfgmap_write,
+	.open		= cfgmap_open,
+	.read		= seq_read,
+};
+
+static int stats_seq_show(struct seq_file *seq, void *v)
+{
+	return seq_print_stats(seq);
+}
+
+static int show_stats_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &stats_seq_show, NULL);
+}
+
+static const struct file_operations show_stats_fops = {
+	.owner		= THIS_MODULE,
+	.open		= show_stats_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+/* The debugging info generated by gcc doesn't quite include *everything*,
+ * even when using -g3 for most debugging info. As far as I can tell, the
+ * only things missing are some string constants, etc that are not very
+ * interesting. However, on the uSCOPE analysis side, we really want to map
+ * these back to files / folders for analysis. This interface lets you print
+ * the entire global table exactly as KASAN sees it, so that everything matches
+ * up and we get complete debug info for all globals. */
+static int globaltable_seq_show(struct seq_file *seq, void *v)
+{
+  seq_printf(seq, "%s\n", global_table_text);
+  return 0;
+}
+
+static int globaltable_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, &globaltable_seq_show, NULL);
+}
+
+static const struct file_operations globaltable_fops = {
+	.owner		= THIS_MODULE,
+	.open		= globaltable_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+//==-- Memorizer Initializtion --------------------------------------------==//
+/**
+ * memorizer_init() - initialize memorizer state
+ *
+ * Set enable flag to true which enables tracking for memory access and object
+ * allocation. Allocate the object cache as well.
+ */
+void __init memorizer_init(void)
+{
+	unsigned long flags;
+	int i = 0;
+
+	__memorizer_enter();
+#if INLINE_EVENT_PARSE == 0
+	init_mem_access_wls();
+#endif
+	/* allocate and initialize memorizer internal allocator */
+	memorizer_alloc_init();
+
+	/* initialize the lookup table */
+	lt_init();
+
+	/* initialize the table tracking CFG edges */
+	cfgtbl = create_function_hashtable();
+
+	/* Create default catch all objects for types of allocated memory */
+	for (i = 0; i < NumAllocTypes; i++) {
+		general_kobjs[i] = memalloc(sizeof(struct memorizer_kobj));
+		init_kobj(general_kobjs[i], 0, 0, 0, i);
+		write_lock(&object_list_spinlock);
+		list_add_tail(&general_kobjs[i]->object_list, &object_list);
+		write_unlock(&object_list_spinlock);
+	}
+
+	/* Allocate memory for the global metadata table.
+	 * Not used by Memorizer, but used in processing globals offline. */
+	global_table_text = memalloc(global_table_text_size);
+	global_table_ptr = global_table_text;
+
+	local_irq_save(flags);
+	if (memorizer_enabled_boot) {
+		memorizer_enabled = true;
+	} else {
+		memorizer_enabled = false;
+	}
+	if (mem_log_boot) {
+		memorizer_log_access = true;
+	} else {
+		memorizer_log_access = false;
+	}
+	if (cfg_log_boot) {
+		cfg_log_on = true;
+	} else {
+		cfg_log_on = false;
+	}
+	if (stack_trace_boot && !cfg_log_on) {
+		stack_trace_on = true;
+	} else {
+		stack_trace_on = false;
+	}
+	print_live_obj = true;
+
+	local_irq_restore(flags);
+	__memorizer_exit();
+}
+
+/*
+ * Late initialization function.
+ */
+static int memorizer_late_init(void)
+{
+	struct dentry *dentryMemDir;
+
+	dentryMemDir = debugfs_create_dir("memorizer", NULL);
+	if (!dentryMemDir)
+		pr_warn("Failed to create debugfs memorizer dir\n");
+
+	debugfs_create_file("kmap", S_IRUGO, dentryMemDir,
+			NULL, &kmap_fops);
+	/* stats interface */
+	debugfs_create_file("show_stats", S_IRUGO, dentryMemDir,
+			NULL, &show_stats_fops);
+	debugfs_create_file("clear_dead_objs", S_IWUGO, dentryMemDir,
+			NULL, &clear_dead_objs_fops);
+	debugfs_create_file("clear_printed_list", S_IWUGO, dentryMemDir,
+			NULL, &clear_printed_list_fops);
+	debugfs_create_file("cfgmap", S_IRUGO|S_IWUGO, dentryMemDir,
+			NULL, &cfgmap_fops);
+	debugfs_create_bool("memorizer_enabled", S_IRUGO|S_IWUGO,
+			dentryMemDir, &memorizer_enabled);
+	debugfs_create_bool("memorizer_log_access", S_IRUGO|S_IWUGO,
+			dentryMemDir, &memorizer_log_access);
+	debugfs_create_bool("cfg_log_on", S_IRUGO|S_IWUGO,
+			dentryMemDir, &cfg_log_on);
+	debugfs_create_bool("stack_trace_on", S_IRUGO|S_IWUGO,
+			dentryMemDir, &stack_trace_on);
+	debugfs_create_bool("print_live_obj", S_IRUGO | S_IWUGO,
+			dentryMemDir, &print_live_obj);
+	debugfs_create_file("global_table", S_IRUGO, dentryMemDir,
+				     NULL, &globaltable_fops);
+
+	pr_info("Memorizer initialized\n");
+	pr_info("Size of memorizer_kobj:%d\n",(int)(sizeof(struct memorizer_kobj)));
+	pr_info("FIXADDR_START: %p,  FIXADDR_SIZE %p", (void *)FIXADDR_START, (void *)FIXADDR_SIZE);
+	print_pool_info();
+	print_stats((size_t)KERN_INFO);
+
+	return 0;
+}
+late_initcall(memorizer_late_init);
diff --git a/mm/memorizer/memorizer.h b/mm/memorizer/memorizer.h
new file mode 100644
index 000000000000..f07c66b696ff
--- /dev/null
+++ b/mm/memorizer/memorizer.h
@@ -0,0 +1,43 @@
+/*===-- LICENSE -------------------------------------------------------------===
+ * Developed by:
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2015, Nathan Dautenhahn
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ *===------------------------------------------------------------------------===
+ *
+ *       Filename:  memorizer.h
+ *
+ *    Description:  General inclues and utilities for memorizer tracing
+ *                  framework.
+ *
+ *===------------------------------------------------------------------------===
+ */
+
+#ifndef __MEMORIZER_H_
+#define __MEMORIZER_H_
+
+#include <linux/gfp.h>
+
+/* mask to apply to memorizer allocations TODO: verify the list */
+#define gfp_memorizer_mask(gfp)	((GFP_ATOMIC | __GFP_NOTRACK | __GFP_NORETRY | GFP_NOWAIT))
+
+long get_ts(void);
+struct memorizer_kobj *create_kobj(uintptr_t call_site, uintptr_t ptr, uint64_t size, enum AllocType AT);
+#endif /* __MEMORIZER_H_ */
diff --git a/mm/memorizer/stats.c b/mm/memorizer/stats.c
new file mode 100644
index 000000000000..3eaea0581a9a
--- /dev/null
+++ b/mm/memorizer/stats.c
@@ -0,0 +1,456 @@
+/*===-- LICENSE -------------------------------------------------------------===
+ * Developed by:
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2015, Nathan Dautenhahn
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ *===------------------------------------------------------------------------===
+ *
+ *       Filename:  stats.c
+ *
+ *    Description:  Statistic summary of Memorizer data
+ *
+ *===------------------------------------------------------------------------===
+ */
+
+#include <linux/debugfs.h>
+#include <linux/printk.h>
+#include <linux/seq_file.h>
+
+#include <linux/memorizer.h>
+#include "stats.h"
+#include "kobj_metadata.h"
+
+#ifdef CONFIG_MEMORIZER_STATS
+
+//==-- Debug and Stats Output Code --==//
+
+/* syntactic sugar to reduce line length below */
+static __always_inline int64_t geta(atomic64_t * a) { return atomic64_read(a); }
+static __always_inline void inca(atomic64_t * a) { atomic64_inc(a); }
+static __always_inline void adda(uint64_t i, atomic64_t * a){atomic64_add(i,a);}
+
+/* stats data structure accounting for each type of alloc */
+static atomic64_t untracked_refs[NumAllocTypes];
+static atomic64_t tracked_refs[NumAllocTypes];
+static atomic64_t tracked_allocs[NumAllocTypes];
+static atomic64_t untracked_bytes_accessed[NumAllocTypes];
+static atomic64_t tracked_bytes_accessed[NumAllocTypes];
+static uint64_t bytes_accessed_overflows = 0;
+
+/* Lookup Table */
+static atomic64_t num_l3 = ATOMIC_INIT(0);
+static atomic64_t num_l2 = ATOMIC_INIT(0);
+static atomic64_t num_l1 = ATOMIC_INIT(0);
+static const uint64_t l3size = sizeof(struct lt_l3_tbl);
+static const uint64_t l2size = sizeof(struct lt_l2_tbl);
+static const uint64_t l1size = sizeof(struct lt_l1_tbl);
+
+void __always_inline track_l1_alloc(void){inca(&num_l1);};
+void __always_inline track_l2_alloc(void){inca(&num_l2);};
+void __always_inline track_l3_alloc(void){inca(&num_l3);};
+
+/* Memory Access */
+static atomic64_t tracked_kobj_accesses = ATOMIC_INIT(0);
+static atomic64_t num_induced_accesses = ATOMIC_INIT(0);
+static atomic64_t num_stack_accesses = ATOMIC_INIT(0);
+static atomic64_t num_accesses_while_disabled = ATOMIC_INIT(0);
+static atomic64_t num_untracked_obj_access = ATOMIC_INIT(0);
+
+char * alloc_type_str(enum AllocType AT)
+{
+	switch(AT)
+	{
+		case MEM_STACK:
+			return "STACK";
+		case MEM_STACK_FRAME:
+			return "STACK_FRAME";
+		case MEM_STACK_ARGS:
+			return "STACK_ARGS";
+		case MEM_STACK_PAGE:
+			return "STACK_PAGE";
+		case MEM_HEAP:
+			return "GEN_HEAP";
+		case MEM_UFO_HEAP:
+			return "UFO_HEAP";
+		case MEM_GLOBAL:
+			return "GLOBAL";
+		case MEM_KMALLOC:
+			return "KMALLOC";
+		case MEM_KMALLOC_ND:
+			return "KMALLOC_ND";
+		case MEM_KMEM_CACHE:
+			return "KMEM_CACHE";
+		case MEM_KMEM_CACHE_ND:
+			return "KMEM_CACHE_ND";
+		case MEM_ALLOC_PAGES:
+			return "ALLOC_PAGES";
+		case MEM_ALLOC_PAGES_EXACT:
+			return "ALLOC_PAGES_EXACT";
+		case MEM_ALLOC_PAGES_GETFREEPAGES:
+			return "ALLOC_PAGES_GETFREEPAGES";
+		case MEM_ALLOC_PAGES_FOLIO:
+			return "ALLOC_PAGES_FOLIO";
+		case MEM_VMALLOC:
+			return "VMALLOC";
+		case MEM_INDUCED:
+			return "INDUCED_ALLOC";
+		case MEM_BOOTMEM:
+			return "BOOTMEM";
+		case MEM_MEMBLOCK:
+			return "MEMBLOCK";
+		case MEM_UFO_MEMBLOCK:
+			return "UFO_MEMBLOCK";
+		case MEM_MEMORIZER:
+			return "MEMORIZER";
+		case MEM_MZ_USER:
+			return "USER";
+		case MEM_BUG:
+			return "BUG";
+		case MEM_UFO_GLOBAL:
+			return "UFO_GLOBAL";
+		case MEM_UFO_NONE:
+			return "UFO_NONE";
+		case MEM_NONE:
+			return "NONE";
+		default:
+			pr_info("Searching for unavailable alloc type");
+			return "ALLOC TYPE NOT FOUND";
+	}
+};
+
+void __always_inline
+track_access(enum AllocType AT, uint64_t size)
+{
+    inca(&tracked_kobj_accesses);
+    if (AT<NumAllocTypes) {
+        inca(&tracked_refs[AT]);
+        adda(size, &tracked_bytes_accessed[AT]);
+        if(geta(&tracked_bytes_accessed[AT]) < 0)
+            bytes_accessed_overflows = 0;
+    }
+}
+
+void __always_inline
+track_induced_access(void)
+{
+    inca(&num_induced_accesses);
+}
+
+void __always_inline
+track_stack_access(void)
+{
+    inca(&num_stack_accesses);
+}
+
+void __always_inline
+track_disabled_access(void)
+{
+    inca(&num_accesses_while_disabled);
+}
+
+void __always_inline
+track_untracked_access(enum AllocType AT, uint64_t size)
+{
+    inca(&num_untracked_obj_access);
+    if (AT<NumAllocTypes) {
+        inca(&untracked_refs[AT]);
+        adda(size, &untracked_bytes_accessed[AT]);
+        if (geta(&untracked_bytes_accessed[AT]) < 0)
+            bytes_accessed_overflows = 0;
+    }
+}
+
+/* General object info */
+static atomic64_t num_allocs_while_disabled = ATOMIC_INIT(0);
+static atomic64_t num_induced_allocs = ATOMIC_INIT(0);
+static atomic64_t stats_frees = ATOMIC_INIT(0);
+static atomic64_t num_induced_frees = ATOMIC_INIT(0);
+static atomic64_t stats_untracked_obj_frees = ATOMIC_INIT(0);
+static atomic64_t stats_kobj_frees = ATOMIC_INIT(0);
+static atomic64_t failed_kobj_allocs = ATOMIC_INIT(0);
+static atomic64_t num_access_counts = ATOMIC_INIT(0);
+
+void __always_inline track_disabled_alloc(void) { inca(&num_allocs_while_disabled); }
+void __always_inline track_induced_alloc(void) { inca(&num_induced_allocs); }
+void __always_inline track_free(void) { inca(&stats_frees); }
+void __always_inline track_untracked_obj_free(void) { inca(&stats_untracked_obj_frees); }
+void __always_inline track_induced_free(void) { inca(&num_induced_frees); }
+void __always_inline track_kobj_free(void) { inca(&stats_kobj_frees); }
+void __always_inline track_failed_kobj_alloc(void) { inca(&failed_kobj_allocs); }
+void __always_inline track_access_counts_alloc(void) { inca(&num_access_counts); }
+
+void __always_inline track_alloc(enum AllocType AT)
+{
+    if (AT > NumAllocTypes) {
+        pr_info("Bad allocation type for memorizer!");
+        return;
+    }
+    inca(&tracked_allocs[AT]);
+}
+
+void lt_pr_stats(size_t pr_level)
+{
+    int64_t l3s = geta(&num_l3);
+    int64_t l2s = geta(&num_l2);
+    int64_t l1s = geta(&num_l1);
+	printk(KERN_CRIT "------- Memorizer LT Stats -------\n");
+	printk(KERN_CRIT "  L3: %8lld tbls * %6llu KB = %6llu MB\n",
+            l3s, l3size>>10, (l3s*l3size)>>20);
+	printk(KERN_CRIT "  L2: %8lld tbls * %6llu KB = %6llu MB\n",
+            l2s, l2size>>10, (l2s*l2size)>>20);
+	printk(KERN_CRIT "  L1: %8lld tbls * %6llu KB = %6llu MB\n",
+            l1s, l1size>>10, (l1s*l1size)>>20);
+}
+
+void lt_pr_stats_seq(struct seq_file *seq)
+{
+    int64_t l3s = 1;
+    int64_t l2s = geta(&num_l2);
+    int64_t l1s = geta(&num_l1);
+	seq_printf(seq,"------- Memorizer LT Stats -------\n");
+	seq_printf(seq,"  L3: %8lld tbls * %6lld KB = %6lld MB\n",
+            l3s, l3size>>10, (l3s*l3size)>>20);
+	seq_printf(seq,"  L2: %8lld tbls * %6lld KB = %6lld MB\n",
+            l2s, l2size>>10, (l2s*l2size)>>20);
+	seq_printf(seq,"  L1: %8lld tbls * %6lld KB = %6lld MB\n",
+            l1s, l1size>>10, (l1s*l1size)>>20);
+}
+
+static int64_t _total_tracked_refs(void)
+{
+    int i;
+    int64_t total = 0;
+    for (i = 0; i < NumAllocTypes; i++)
+            total += geta(&tracked_refs[i]);
+    return total;
+}
+
+static int64_t _total_untracked_refs(void)
+{
+    int64_t i;
+    int64_t total = 0;
+    for (i = 0; i < NumAllocTypes; i++)
+            total += geta(&untracked_refs[i]);
+    return total;
+}
+
+static size_t _percent_refs_hit(void)
+{
+    return (_total_tracked_refs() || _total_untracked_refs()) ?
+            100*_total_tracked_refs() /
+            (_total_untracked_refs()+_total_tracked_refs()) : 0;
+}
+
+static int64_t _total_tracked_bytes(void)
+{
+    int i;
+    int64_t total = 0;
+    for (i = 0; i < NumAllocTypes; i++)
+            total += geta(&tracked_bytes_accessed[i]);
+    return total;
+}
+
+static int64_t _total_untracked_bytes(void)
+{
+    int64_t i;
+    int64_t total = 0;
+    for (i = 0; i < NumAllocTypes; i++)
+            total += geta(&untracked_bytes_accessed[i]);
+    return total;
+}
+
+static size_t _percent_bytes_hit(void)
+{
+    return (_total_tracked_bytes() || _total_untracked_bytes()) ?
+            100*_total_tracked_bytes() /
+            (_total_untracked_bytes()+_total_tracked_bytes()) : 0;
+}
+
+static int64_t _total_tracked(void)
+{
+    int64_t i;
+    int64_t total = 0;
+    for ( i = 0; i < NumAllocTypes; i++)
+            total += geta(&tracked_allocs[i]);
+    return total;
+}
+
+static uint64_t _live_objs(void)
+{
+    return _total_tracked() - geta(&stats_frees);
+}
+
+static int64_t _total_accesses(void)
+{
+    return geta(&tracked_kobj_accesses)
+        + geta(&num_induced_accesses)
+        + geta(&num_accesses_while_disabled)
+        + geta(&num_untracked_obj_access);
+}
+
+/**
+ * print_stats() - print global stats from memorizer
+ */
+void print_stats(size_t pr_level)
+{
+    int i;
+    printk(KERN_CRIT "------- Memory Accesses -------\n");
+    printk(KERN_CRIT "   Tracked:%16lld\n", geta(&tracked_kobj_accesses));
+    printk(KERN_CRIT "   Missing:%16lld\n", geta(&num_untracked_obj_access));
+    printk(KERN_CRIT "   Induced:%16lld\n", geta(&num_induced_accesses));
+    printk(KERN_CRIT "  Disabled:%16lld\n", geta(&num_accesses_while_disabled));
+    printk(KERN_CRIT "    ---------------------------\n");
+    printk(KERN_CRIT "  Total Obs:    %16lld\n", _total_accesses());
+
+/* Print out the access counts */
+    printk(KERN_CRIT "------- Per Object Access Count (hit/miss) -------\n");
+    for ( i = 0; i < NumAllocTypes; i++) {
+            printk(KERN_CRIT "   %-15s: %16lld, %16lld\n",
+                            alloc_type_str(i), geta(&tracked_refs[i]),
+			geta(&untracked_refs[i]));
+	}
+
+    printk(KERN_CRIT "    ---------------------------\n");
+    printk(KERN_CRIT "   %-15s: %16lld, %16lld --- %d%% hit rate\n", "Total",
+                    _total_tracked_refs(), _total_untracked_refs(),
+                    (int)_percent_refs_hit());
+
+	/* Print out the byte counts using simple total bytes accessed */
+	printk(KERN_CRIT "------- Per Object Bytes Accessed (hit/miss) -------\n");
+    for (i = 0; i < NumAllocTypes; i++) {
+        printk(KERN_CRIT "   %-15s: %16lld, %16lld\n",
+		alloc_type_str(i),
+		geta(&tracked_bytes_accessed[i]),
+		geta(&untracked_bytes_accessed[i]));
+	}
+
+    printk(KERN_CRIT "    ---------------------------\n");
+    printk(KERN_CRIT "   %-15s: %16lld, %16lld --- %d%% hit rate\n", "Total",
+                    _total_tracked_bytes(), _total_untracked_bytes(),
+                    (int)_percent_bytes_hit());
+	printk(KERN_CRIT "    ****** We had %lld overflows on byte counters.\n",
+	       bytes_accessed_overflows);
+
+	/* Print aggregate memory alloc stats for mem types */
+    printk(KERN_CRIT "------- Tracked Memory Allocations -------\n");
+    for (i = 0; i < NumAllocTypes; i++) {
+            printk(KERN_CRIT "   %-15s: %16lld\n",
+                            alloc_type_str(i), geta(&tracked_allocs[i]));
+    }
+    printk(KERN_CRIT "        ------\n");
+    printk(KERN_CRIT "  Total:        %16lld\n", _total_tracked());
+    printk(KERN_CRIT "  Frees:        %16lld\n", geta(&stats_frees));
+    printk(KERN_CRIT "  Live Now:     %16lld\n", _live_objs());
+
+	/* Print out info on missing allocations */
+	/* -- TODO: depracated and can probably remove */
+    printk(KERN_CRIT "------- Missing Allocs -------\n");
+    printk(KERN_CRIT "  Mem disabled: %16lld\n", geta(&num_allocs_while_disabled));
+    printk(KERN_CRIT "  Allocs(InMem):%16lld\n", geta(&num_induced_allocs));
+    printk(KERN_CRIT "  Frees(InMem): %16lld\n", geta(&num_induced_frees));
+    printk(KERN_CRIT "  Frees(NoObj): %16lld\n", geta(&stats_untracked_obj_frees));
+    printk(KERN_CRIT "  kobj fails:   %16lld\n", geta(&failed_kobj_allocs));
+
+    printk(KERN_CRIT "------- Internal Allocs -------\n");
+    /* TODO: right now if we don't drain inline then this is total tracked */
+    printk(KERN_CRIT "  Live KOBJs: %10lld * %lu B = %6llu MB\n",
+                    _total_tracked()-geta(&stats_kobj_frees), sizeof(struct
+                            memorizer_kobj),
+                    (_total_tracked()-geta(&stats_kobj_frees)) * sizeof(struct
+                            memorizer_kobj) >> 20 );
+
+    printk(KERN_CRIT "  Total Edgs: %10lld * %lu B = %6llu MB\n",
+                    geta(&num_access_counts), sizeof(struct access_from_counts),
+                    geta(&num_access_counts)*sizeof(struct access_from_counts)>>20);
+
+    lt_pr_stats(pr_level);
+}
+
+int seq_print_stats(struct seq_file *seq)
+{
+	int i;
+	seq_printf(seq,"------- Memory Accesses -------\n");
+	seq_printf(seq,"  Tracked:      %16lld\n", geta(&tracked_kobj_accesses));
+	seq_printf(seq,"  Missing:      %16lld\n", geta(&num_untracked_obj_access));
+	seq_printf(seq,"  Induced:      %16lld\n", geta(&num_induced_accesses));
+	seq_printf(seq,"  Disabled:     %16lld\n", geta(&num_accesses_while_disabled));
+	seq_printf(seq,"    ---------------------------\n");
+	seq_printf(seq,"  Total Obs:    %16lld\n", _total_accesses());
+
+	seq_printf(seq,"------- Per Object Access Count (hit/miss) -------\n");
+	for (i = 0; i < NumAllocTypes; i++) {
+		seq_printf(seq,"   %-15s: %16lld, %16lld\n",
+			   alloc_type_str(i), geta(&tracked_refs[i]),
+			   geta(&untracked_refs[i]));
+	}
+
+	seq_printf(seq,"    ---------------------------\n");
+	seq_printf(seq,"   %-15s: %16lld, %16lld --- %lu%% hit rate\n", "Total",
+		   _total_tracked_refs(), _total_untracked_refs(),
+		   _percent_refs_hit());
+
+	/* Print out the byte counts using simple total bytes accessed */
+	seq_printf(seq,"------- Per Object Bytes Accessed (hit/miss) -------\n");
+	for (i = 0; i < NumAllocTypes; i++) {
+		seq_printf(seq,"   %-15s: %16lld, %16lld\n",
+			   alloc_type_str(i),
+			   geta(&tracked_bytes_accessed[i]),
+			   geta(&untracked_bytes_accessed[i]));
+	}
+
+	seq_printf(seq,"    ---------------------------\n");
+	seq_printf(seq,"   %-15s: %16lld, %16lld --- %lu%% hit rate\n", "Total",
+		   _total_tracked_bytes(), _total_untracked_bytes(),
+		   _percent_bytes_hit());
+	seq_printf(seq,"    ****** We had %lld overflows on byte counters.\n",
+	       bytes_accessed_overflows);
+
+	seq_printf(seq,"------- Tracked Memory Allocations -------\n");
+	for (i = 0; i < NumAllocTypes; i++) {
+		seq_printf(seq,"   %-15s: %16lld\n",
+			   alloc_type_str(i), geta(&tracked_allocs[i]));
+	}
+	seq_printf(seq,"        ------\n");
+	seq_printf(seq,"  Total:        %16lld\n", _total_tracked());
+	seq_printf(seq,"  Frees:        %16lld\n", geta(&stats_frees));
+	seq_printf(seq,"  Live Now:     %16lld\n", _live_objs());
+
+	seq_printf(seq,"------- Missing Allocs -------\n");
+	seq_printf(seq,"  Mem disabled: %16lld\n", geta(&num_allocs_while_disabled));
+	seq_printf(seq,"  Allocs(InMem):%16lld\n", geta(&num_induced_allocs));
+	seq_printf(seq,"  Frees(InMem): %16lld\n", geta(&num_induced_frees));
+	seq_printf(seq,"  Frees(NoObj): %16lld\n", geta(&stats_untracked_obj_frees));
+	seq_printf(seq,"  kobj fails:   %16lld\n", geta(&failed_kobj_allocs));
+
+	seq_printf(seq,"------- Internal Allocs -------\n");
+	/* TODO: right now if we don't drain inline then this is total tracked */
+	seq_printf(seq,"  Live KOBJs: %10lld * %lu B = %6lld MB\n",
+		   _total_tracked()-geta(&stats_kobj_frees),
+		   sizeof(struct memorizer_kobj),
+		   (_total_tracked()-geta(&stats_kobj_frees)) * sizeof(struct memorizer_kobj) >> 20 );
+
+	seq_printf(seq,"  Total Edges: %10lld * %lu B = %6llu MB\n",
+		   geta(&num_access_counts), sizeof(struct access_from_counts),
+		   geta(&num_access_counts) * sizeof(struct access_from_counts)>>20);
+	lt_pr_stats_seq(seq);
+	return 0;
+}
+
+#endif /* CONFIG_MEMORIZER_STATS */
diff --git a/mm/memorizer/stats.h b/mm/memorizer/stats.h
new file mode 100644
index 000000000000..b788f75951b4
--- /dev/null
+++ b/mm/memorizer/stats.h
@@ -0,0 +1,88 @@
+/*===-- LICENSE
+ * -------------------------------------------------------------===
+ * Developed by:
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2015, Nathan Dautenhahn
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ *===------------------------------------------------------------------------===
+ *
+ *       Filename:  stats.h
+ *
+ *    Description:
+ *
+ *===------------------------------------------------------------------------===
+ */
+
+
+#ifndef _MEMSTATS_H_
+#define _MEMSTATS_H_
+
+#include <asm/atomic.h>
+#include <linux/memorizer.h>
+#include "kobj_metadata.h"
+
+//==-- External Interface --==//
+#ifdef CONFIG_MEMORIZER_STATS
+char * alloc_type_str(enum AllocType AT);
+void track_alloc(enum AllocType AT);
+void track_disabled_alloc(void);
+void track_induced_alloc(void);
+void track_failed_kobj_alloc(void);
+void track_free(void);
+void track_untracked_obj_free(void);
+void track_induced_free(void);
+void track_kobj_free(void);
+void track_access(enum AllocType AT, uint64_t size);
+void track_induced_access(void);
+void track_stack_access(void);
+void track_disabled_access(void);
+void track_untracked_access(enum AllocType AT, uint64_t size);
+void track_access_counts_alloc(void);
+void track_l1_alloc(void);
+void track_l2_alloc(void);
+void track_l3_alloc(void);
+void print_stats(size_t pr_level);
+int seq_print_stats(struct seq_file *seq);
+#else
+static inline char * alloc_type_str(enum AllocType AT){return "";}
+static inline void track_alloc(enum AllocType AT){}
+static inline void track_disabled_alloc(void){}
+static inline void track_induced_alloc(void){}
+static inline void track_failed_kobj_alloc(void){}
+static inline void track_free(void){}
+static inline void track_untracked_obj_free(void){}
+static inline void track_induced_free(void){}
+static inline void track_kobj_free(void){}
+static inline void track_access(enum AllocType AT, uint64_t size) {}
+static inline void track_induced_access(void){}
+static inline void track_stack_access(void){}
+static inline void track_disabled_access(void){}
+static inline void track_untracked_access(enum AllocType AT, uint64_t size){}
+static inline void track_access_counts_alloc(void){}
+static inline void track_l1_alloc(void){}
+static inline void track_l2_alloc(void){}
+static inline void track_l3_alloc(void){}
+static inline void print_stats(size_t pr_level){}
+static inline int seq_print_stats(struct seq_file *seq){return 0;}
+#endif
+
+//TODO: Add kernel config option so can be disabled or add boot flag
+
+#endif /* _MEMSTATS_H_ */
diff --git a/mm/memorizer/util.h b/mm/memorizer/util.h
new file mode 100644
index 000000000000..f8a8e31ab77c
--- /dev/null
+++ b/mm/memorizer/util.h
@@ -0,0 +1,50 @@
+/*===-- LICENSE
+ * -------------------------------------------------------------===
+ * Developed by:
+ *
+ *    Research Group of Professor Vikram Adve in the Department of Computer
+ *    Science The University of Illinois at Urbana-Champaign
+ *    http://web.engr.illinois.edu/~vadve/Home.html
+ *
+ * Copyright (c) 2015, Nathan Dautenhahn
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ *===------------------------------------------------------------------------===
+ *
+ *       Filename:  utilities.h
+ *
+ *    Description:  Utility functions for developing
+ *
+ *===------------------------------------------------------------------------===
+ */
+
+#ifndef _UTIL_H_
+#define _UTIL_H_
+
+int memstrcmp(const char *cs, const char *ct)
+{
+	unsigned char c1, c2;
+
+	while (1) {
+		c1 = *cs++;
+		c2 = *ct++;
+		if (c1 != c2)
+			return c1 < c2 ? -1 : 1;
+		if (!c1)
+			break;
+	}
+	return 0;
+}
+#endif /* __utilities.h_H_ */
diff --git a/mm/oom_kill.c b/mm/oom_kill.c
index 1276e49b31b0..e855824cf65a 100644
--- a/mm/oom_kill.c
+++ b/mm/oom_kill.c
@@ -43,6 +43,7 @@
 #include <linux/ratelimit.h>
 #include <linux/kthread.h>
 #include <linux/init.h>
+#include <linux/memorizer.h>
 #include <linux/mmu_notifier.h>
 
 #include <asm/tlb.h>
@@ -1107,6 +1108,8 @@ bool out_of_memory(struct oom_control *oc)
 {
 	unsigned long freed = 0;
 
+	memorizer_print_stats();
+
 	if (oom_killer_disabled)
 		return false;
 
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index b2877a84ed19..fd3c8150fecb 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -766,6 +766,9 @@ static inline bool pcp_allowed_order(unsigned int order)
 
 static inline void free_the_page(struct page *page, unsigned int order)
 {
+	/* TODO robadams@illinois.edu
+	 * Also in free_pages_prepare. Belt and suspenders? */
+	memorizer_free_pages(_RET_IP_, page, order);
 	if (pcp_allowed_order(order))		/* Via pcp? */
 		free_unref_page(page, order);
 	else
@@ -1409,6 +1412,10 @@ static __always_inline bool free_pages_prepare(struct page *page,
 	trace_mm_page_free(page, order);
 	kmsan_free_page(page, order);
 
+	/* TODO robadams@illinois.edu
+	 * Also in free_the_page. Belt and suspenders? */
+	memorizer_free_pages(_RET_IP_, page, order);
+
 	if (unlikely(PageHWPoison(page)) && !order) {
 		/*
 		 * Do not let hwpoison pages hit pcplists/buddy
@@ -1495,6 +1502,7 @@ static __always_inline bool free_pages_prepare(struct page *page,
 
 	debug_pagealloc_unmap_pages(page, 1 << order);
 
+
 	return true;
 }
 
@@ -5580,6 +5588,8 @@ struct page *__alloc_pages(gfp_t gfp, unsigned int order, int preferred_nid,
 	trace_mm_page_alloc(page, order, alloc_gfp, ac.migratetype);
 	kmsan_alloc_page(page, order, alloc_gfp);
 
+	memorizer_alloc_pages(_RET_IP_, page, order, alloc_gfp);
+
 	return page;
 }
 EXPORT_SYMBOL(__alloc_pages);
@@ -5592,6 +5602,10 @@ struct folio *__folio_alloc(gfp_t gfp, unsigned int order, int preferred_nid,
 
 	if (page && order > 1)
 		prep_transhuge_page(page);
+
+	if(page)
+		memorizer_alloc_folio(_RET_IP_, page, order, gfp | __GFP_COMP);
+
 	return (struct folio *)page;
 }
 EXPORT_SYMBOL(__folio_alloc);
@@ -5605,16 +5619,28 @@ unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
 {
 	struct page *page;
 
+	memorizer_start_getfreepages();
+
 	page = alloc_pages(gfp_mask & ~__GFP_HIGHMEM, order);
-	if (!page)
+	if (!page) {
+		memorizer_end_getfreepages();
 		return 0;
+	}
+
+	memorizer_alloc_getfreepages(_RET_IP_, page, order, gfp_mask);
+
 	return (unsigned long) page_address(page);
 }
 EXPORT_SYMBOL(__get_free_pages);
 
 unsigned long get_zeroed_page(gfp_t gfp_mask)
 {
-	return __get_free_pages(gfp_mask | __GFP_ZERO, 0);
+	unsigned long ret = __get_free_pages(gfp_mask | __GFP_ZERO, 0);
+
+	// Memorizer hook here to attribute alloc to this caller
+	memorizer_alloc_pages(_RET_IP_, (void *) ret, 0, gfp_mask);
+
+	return ret;
 }
 EXPORT_SYMBOL(get_zeroed_page);
 
@@ -5826,12 +5852,24 @@ void *alloc_pages_exact(size_t size, gfp_t gfp_mask)
 {
 	unsigned int order = get_order(size);
 	unsigned long addr;
+	void *ret;
 
 	if (WARN_ON_ONCE(gfp_mask & (__GFP_COMP | __GFP_HIGHMEM)))
 		gfp_mask &= ~(__GFP_COMP | __GFP_HIGHMEM);
 
 	addr = __get_free_pages(gfp_mask, order);
-	return make_alloc_exact(addr, order, size);
+
+	// TODO: memorizer: var decl
+	ret = make_alloc_exact(addr, order, size);
+
+	// Memorizer hook here to attribute alloc to this caller
+	// Special Memorizer hook for exact page allocation
+
+	// TODO robadams@illinois.edu
+	// But does this lead to a bunch of 0xDEADBEEF?
+	memorizer_alloc_pages_exact(_RET_IP_, ret, size, gfp_mask);
+
+	return ret;
 }
 EXPORT_SYMBOL(alloc_pages_exact);
 
diff --git a/mm/slab_common.c b/mm/slab_common.c
index 0042fb2730d1..f0f2e6847be5 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -1124,6 +1124,7 @@ void *kmalloc_large_node(size_t size, gfp_t flags, int node)
 
 	trace_kmalloc(_RET_IP_, ret, size, PAGE_SIZE << get_order(size),
 		      flags, node);
+	memorizer_kmalloc(_RET_IP_, ret, size, PAGE_SIZE << get_order(size), flags);
 	return ret;
 }
 EXPORT_SYMBOL(kmalloc_large_node);
diff --git a/mm/slub.c b/mm/slub.c
index 157527d7101b..8253d49655d6 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1768,8 +1768,16 @@ static inline bool slab_free_freelist_hook(struct kmem_cache *s,
 	return *head != NULL;
 }
 
+static gfp_t last_flags = 0;
 static void *setup_object(struct kmem_cache *s, void *object)
 {
+
+	// TODO memorizer : consider setup_object_debug
+	/* This function is called when Slub allocates new objects for a cache.
+	 * Memorizer preallocates objects here so any accesses from constructors
+	 * are captured correctly. */
+	memorizer_kmem_cache_alloc(MEMORIZER_PREALLOCED, object, s, last_flags);
+
 	setup_object_debug(s, object);
 	object = kasan_init_slab_obj(s, object);
 	if (unlikely(s->ctor)) {
@@ -1933,6 +1941,9 @@ static struct slab *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	 * so we fall-back to the minimum order allocation.
 	 */
 	alloc_gfp = (flags | __GFP_NOWARN | __GFP_NORETRY) & ~__GFP_NOFAIL;
+
+	last_flags = alloc_gfp;
+
 	if ((alloc_gfp & __GFP_DIRECT_RECLAIM) && oo_order(oo) > oo_order(s->min))
 		alloc_gfp = (alloc_gfp | __GFP_NOMEMALLOC) & ~__GFP_RECLAIM;
 
@@ -1964,7 +1975,9 @@ static struct slab *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 
 	setup_slab_debug(s, slab, start);
 
-	shuffle = shuffle_freelist(s, slab);
+	// For Memorizer, let's not shuffle slab. This way there's only one place where setup_object() is called.
+	// shuffle = shuffle_freelist(s, slab);
+	shuffle = false;
 
 	if (!shuffle) {
 		start = fixup_red_left(s, start);
@@ -3308,6 +3321,7 @@ static __always_inline void maybe_wipe_obj_freeptr(struct kmem_cache *s,
 static __always_inline void *slab_alloc_node(struct kmem_cache *s, struct list_lru *lru,
 		gfp_t gfpflags, int node, unsigned long addr, size_t orig_size)
 {
+	// TODO robadams@illinois.edu memorizer path?
 	void *object;
 	struct kmem_cache_cpu *c;
 	struct slab *slab;
@@ -3411,9 +3425,14 @@ void *__kmem_cache_alloc_lru(struct kmem_cache *s, struct list_lru *lru,
 			     gfp_t gfpflags)
 {
 	void *ret = slab_alloc(s, lru, gfpflags, _RET_IP_, s->object_size);
+	int update;
 
 	trace_kmem_cache_alloc(_RET_IP_, ret, s, gfpflags, NUMA_NO_NODE);
 
+	update = memorizer_kmem_cache_set_alloc(_RET_IP_, ret);
+	if (!update)
+		memorizer_kmem_cache_alloc(_RET_IP_, ret, s, gfpflags);
+
 	return ret;
 }
 
@@ -3434,6 +3453,7 @@ void *__kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags,
 			      int node, size_t orig_size,
 			      unsigned long caller)
 {
+	// TODO robadams@illinois.edu -- memorizer?
 	return slab_alloc_node(s, NULL, gfpflags, node,
 			       caller, orig_size);
 }
@@ -3441,9 +3461,14 @@ void *__kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags,
 void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags, int node)
 {
 	void *ret = slab_alloc_node(s, NULL, gfpflags, node, _RET_IP_, s->object_size);
+	int update;
 
 	trace_kmem_cache_alloc(_RET_IP_, ret, s, gfpflags, node);
 
+	update = memorizer_kmem_cache_set_alloc(_RET_IP_, ret);
+	if (!update)
+		memorizer_kmem_cache_alloc_node(_RET_IP_, ret, s, gfpflags, node);
+
 	return ret;
 }
 EXPORT_SYMBOL(kmem_cache_alloc_node);
@@ -3681,6 +3706,7 @@ void kmem_cache_free(struct kmem_cache *s, void *x)
 		return;
 	trace_kmem_cache_free(_RET_IP_, x, s);
 	slab_free(s, virt_to_slab(x), x, NULL, &x, 1, _RET_IP_);
+	memorizer_kmem_cache_free(_RET_IP_, x);
 }
 EXPORT_SYMBOL(kmem_cache_free);
 
diff --git a/mm/vmalloc.c b/mm/vmalloc.c
index ccaa461998f3..23568ab481e1 100644
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@ -2780,6 +2780,12 @@ static void __vfree(const void *addr)
  */
 void vfree(const void *addr)
 {
+
+	// Memorizer hook free.
+	// TODO robadams@illinois.edu - do we call memorizer*free twice
+	// on the same address? Once here, and once in __vfree()?
+	memorizer_vmalloc_free(_RET_IP_,  addr);
+
 	BUG_ON(in_nmi());
 
 	kmemleak_free(addr);
@@ -3214,6 +3220,11 @@ void *__vmalloc_node_range(unsigned long size, unsigned long align,
 	if (!ret)
 		goto fail;
 
+	// Memorizer hooking here
+	memorizer_vmalloc_alloc((unsigned long) caller, area->addr, size, gfp_mask);
+	// TODO memorizer : do we need other calls?
+
+
 	/*
 	 * Mark the pages as accessible, now that they are mapped.
 	 * The condition for setting KASAN_VMALLOC_INIT should complement the
diff --git a/scripts/Makefile.lib b/scripts/Makefile.lib
index 3aa384cec76b..abdaf11a6fe9 100644
--- a/scripts/Makefile.lib
+++ b/scripts/Makefile.lib
@@ -185,6 +185,12 @@ _c_flags += $(if $(patsubst n%,, \
 	$(CFLAGS_KCOV))
 endif
 
+ifeq ($(CONFIG_MEMORIZER),y)
+_c_flags += $(if $(patsubst n%,, \
+		$(MEMORIZER_INSTRUMENT_$(basetarget).o)$(MEMORIZER_INSTRUMENT)y), \
+		$(CFLAGS_MEMORIZER))
+endif
+
 #
 # Enable KCSAN flags except some files or directories we don't want to check
 # (depends on variables KCSAN_SANITIZE_obj.o, KCSAN_SANITIZE)
diff --git a/scripts/Makefile.memorizer b/scripts/Makefile.memorizer
new file mode 100644
index 000000000000..8f5e7e294c2c
--- /dev/null
+++ b/scripts/Makefile.memorizer
@@ -0,0 +1,5 @@
+# SPDX-License-Identifier: GPL-2.0
+
+CFLAGS_MEMORIZER := -finstrument-functions
+
+export CFLAGS_MEMORIZER 
diff --git a/scripts/asn1_compiler.c b/scripts/asn1_compiler.c
index 71d4a7c87900..8a061995f811 100644
--- a/scripts/asn1_compiler.c
+++ b/scripts/asn1_compiler.c
@@ -427,7 +427,7 @@ static void tokenise(char *buffer, char *end)
 				}
 				memcpy(tokens[tix].content, start, tokens[tix].size);
 				tokens[tix].content[tokens[tix].size] = 0;
-				
+
 				/* If it begins with a lowercase letter then
 				 * it's an element name
 				 */
diff --git a/scripts/cp_test.sh b/scripts/cp_test.sh
new file mode 100755
index 000000000000..34494f0d76bd
--- /dev/null
+++ b/scripts/cp_test.sh
@@ -0,0 +1,18 @@
+#!/bin/sh
+
+cd /sys/kernel/debug/memorizer
+echo 1 > clear_object_list
+echo 1 > clear_printed_list
+echo 1 > memorizer_enabled
+echo 1 > memorizer_log_access
+
+cd /root
+
+
+cp -R /mnt/host/src/repos/linuxkit /root
+echo "Done Copying"
+cd /sys/kernel/debug/memorizer
+echo 0 > memorizer_log_access
+echo 0 > memorizer_enabled
+
+cd /root
diff --git a/scripts/event_structs.h b/scripts/event_structs.h
new file mode 100644
index 000000000000..d6f0325d2c2a
--- /dev/null
+++ b/scripts/event_structs.h
@@ -0,0 +1,62 @@
+/*
+ * 1. ALLOCATIONS
+ * 2. DEALLOCATIONS
+ * 3. ACCESSES
+ * These will be used to create stateless logs for Memorizer 2.0
+ * */
+
+#include <linux/sched.h>
+
+/* Event and Access type  enumerations */
+enum EventType {Memorizer_Mem_Alloc = 0xaa, Memorizer_Mem_Free = 0xbb, Memorizer_Mem_Read = 0xcc, Memorizer_Mem_Write = 0xdd, Memorizer_Fork = 0xee};
+enum AccessType {Memorizer_READ=0,Memorizer_WRITE};
+
+struct memorizer_kernel_event {
+	enum EventType event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	uintptr_t	src_pa_ptr;
+	size_t		event_size;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+	enum		AccessType access_type;
+	char		comm[16];
+	char		funcstr[128];
+
+
+};
+
+struct memorizer_kernel_alloc {
+	char		event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	uintptr_t	src_pa_ptr;
+	size_t		event_size;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+	char		comm[16];
+	char		funcstr[128];
+};
+
+struct memorizer_kernel_free {
+	char		event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+};
+
+struct memorizer_kernel_access {
+	char		event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	size_t		event_size;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+};
+
+struct memorizer_kernel_fork {
+	char		event_type;
+	long		pid;
+	char		comm[16];
+};
diff --git a/scripts/memorizer/README.txt b/scripts/memorizer/README.txt
new file mode 100644
index 000000000000..6aae8ff33ab9
--- /dev/null
+++ b/scripts/memorizer/README.txt
@@ -0,0 +1,50 @@
+Files: memorizer.py, test_memorizer.py
+
+Dependencies:
+In order to run the test_memorizer w/ linux test suite, you must
+wget the latest version from the ltp github repo and set it up.
+Ex:
+wget https://github.com/linux-test-project/ltp/releases/download/20170116/ltp-full-20170116.tar.bz2
+tar xvfj ltp-full-20170116.tar.bz2
+# cd into the untarred dir
+./configure
+make
+sudo make install
+
+Good documentation / examples: http://ltp.sourceforge.net/documentation/how-to/ltp.php
+
+memorizer.py: accepts processes to run in quotes.
+Ex: python memorizer.py "ls" "mkdir dir"
+In order to run the script, you must have your user be in the
+memorizer group, which you should setup if not.
+How-to: sudo groupadd memorizer; sudo usermod -aG memorizer <user>
+You will be queried to enter your pw in order to set group
+permissions on the /sys/kernel/debug dirs which include ftrace
+and memorizer.
+
+test_memorizer.py: accepts either -e, -m, or -h flags.
+Ex: python test_memorizer.py -e
+*All modes will run the setup/cleanup checks to ensure all virtual nodes
+are being set correctly.
+-e: Runs ls, wget, and tar sequentially.
+-m: Runs the linux test suite and saves a human-readable log to
+/opt/ltp/results/ltp.log
+-h: runs both -e and -m
+As with the memorizer.py, you will need your user to be in the
+memorizer group.  Additionally, you will be queried to enter your
+pw in order to set group permissions on the /opt/ltp dirs.
+
+
+
+===============
+ MEMORIZER 2.0
+===============
+In order to test the new Memorizer with the busybox userland initramfs image, since the system is completely barebones, the environment needs to be set up.
+
+
+./setup_env.sh : Sets up the environment
+./userApp c: Prints the number of remaining bytes in the buffer
+./userApp p: Prints the buffer. Currently, it only prints the first 100 entries in the buffer
+./cp_test: Performs a test and copies the linuxkit directory to the root directory. Initializes the memorizer before the test and disables it afterwards.
+./enable_memorizer: Enables the memorizer and access logging
+./disable_memorizer: Disables the memorier and access logging
diff --git a/scripts/memorizer/cp_test.sh b/scripts/memorizer/cp_test.sh
new file mode 100755
index 000000000000..ef0f6324c4f4
--- /dev/null
+++ b/scripts/memorizer/cp_test.sh
@@ -0,0 +1,19 @@
+#!/bin/sh
+
+cd /sys/kernel/debug/memorizer
+echo 1 > clear_object_list
+echo 1 > clear_printed_list
+echo 1 > memorizer_enabled
+echo 1 > memorizer_log_access
+
+cd /root
+
+
+cp -R /mnt/host/src/repos/linuxkit /root
+echo "Done Copying"
+cd /sys/kernel/debug/memorizer
+echo 0 > memorizer_log_access
+echo 0 > memorizer_enabled
+
+cd /root
+./userApp p > outputMap
diff --git a/scripts/memorizer/disable_memorizer.sh b/scripts/memorizer/disable_memorizer.sh
new file mode 100755
index 000000000000..ff13d76de21a
--- /dev/null
+++ b/scripts/memorizer/disable_memorizer.sh
@@ -0,0 +1,5 @@
+!#/bin/sh
+
+cd /sys/kernel/debug/memorizer
+echo 0 > memorizer_log_accesses
+echo 0 > memorizer_enabled
diff --git a/scripts/memorizer/enable_memorizer.sh b/scripts/memorizer/enable_memorizer.sh
new file mode 100755
index 000000000000..cf6001cf655b
--- /dev/null
+++ b/scripts/memorizer/enable_memorizer.sh
@@ -0,0 +1,6 @@
+#!/bin/sh
+cd /sys/kernel/debug/memorizer
+echo 1 > clear_object_list
+echo 1 > clear_printed_list
+echo 1 > memorizer_enabled
+echo 1 > memorizer_log_access
diff --git a/scripts/memorizer/event_structs.h b/scripts/memorizer/event_structs.h
new file mode 100644
index 000000000000..68dea736743a
--- /dev/null
+++ b/scripts/memorizer/event_structs.h
@@ -0,0 +1,62 @@
+/* This file describes the structs to be used to describe the events happening inside the kernel:
+ * 1. ALLOCATIONS
+ * 2. DEALLOCATIONS
+ * 3. ACCESSES
+ * These will be used to create stateless logs for Memorizer 2.0
+ * */
+
+#include <linux/sched.h>
+
+/* Event and Access type  enumerations */
+enum EventType {Memorizer_Mem_Alloc = 0xaa, Memorizer_Mem_Free = 0xbb, Memorizer_Mem_Access = 0xcc};
+enum AccessType {Memorizer_READ=0,Memorizer_WRITE};
+
+struct memorizer_kernel_event {
+	enum EventType event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	uintptr_t	src_pa_ptr;
+	size_t		event_size;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+	enum		AccessType access_type;
+	char		comm[16];
+	char		funcstr[128];
+
+
+};
+
+struct memorizer_kernel_alloc {
+	char		event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	uintptr_t	src_pa_ptr;
+	size_t		event_size;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+	char		comm[16];
+	char		funcstr[128];
+};
+
+struct memorizer_kernel_free {
+	char		event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+};
+
+struct memorizer_kernel_access {
+	char		event_type;
+	uintptr_t	event_ip;
+	uintptr_t	src_va_ptr;
+	size_t		event_size;
+	unsigned long	event_jiffies;
+	pid_t		pid;
+};
+
+struct memorizer_kernel_fork {
+	char		event_type;
+	long		pid;
+	char		comm[16];
+};
diff --git a/scripts/memorizer/memorizer.py b/scripts/memorizer/memorizer.py
new file mode 100755
index 000000000000..2b56cdb85dd8
--- /dev/null
+++ b/scripts/memorizer/memorizer.py
@@ -0,0 +1,152 @@
+import sys,threading,os,subprocess,operator,time
+
+mem_path = "/sys/kernel/debug/memorizer/"
+directory = ""
+completed = False
+
+def worker(cmd):
+  ret = os.system(cmd)
+  if(ret != 0):
+    print "Failed attempt on: " + cmd
+    exit(1)
+
+def basic_cleanup():
+  print "Basic tests completed. Now cleaning up."
+  ret = os.system("rm UPennlogo2.jpg")
+
+def memManager():
+  while(not completed):
+    stats = subprocess.check_output(["free"])
+    stats_list = stats.split()
+    total_mem = float(stats_list[7])
+    used_mem = float(stats_list[8])
+    memory_usage = used_mem / total_mem
+    if(memory_usage > 0.8):
+      ret = os.system("cat " + mem_path + "kmap >> " + directory + "test.kmap")
+      if ret != 0:
+        print "Failed to append kmap to temp file"
+        exit(1)
+      ret = os.system("echo 1 > " + mem_path + "clear_printed_list")
+      if ret != 0:
+        print "Failed to clear printed list"
+        exit(1)
+    time.sleep(2)
+
+def startup():
+  ret = os.system("sudo chgrp -R memorizer /opt/")
+  if ret != 0:
+    print "Failed to change group permissions of /opt/"
+    exit(1)
+  os.system("sudo chmod -R g+wrx /opt/")
+  if ret != 0:
+    print "Failed to grant wrx permissions to /opt/"
+    exit(1)
+  # Setup group permissions to ftrace & memorizer directories
+  ret = os.system("sudo chgrp -R memorizer /sys/kernel/debug/")
+  if ret != 0:
+    print "Failed to change memorizer group permissions to /sys/kernel/debug/"
+    exit(1)
+  ret = os.system("sudo chmod -R g+wrx /sys/kernel/debug/")
+  if ret != 0:
+    print "Failed to grant wrx persmissions to /sys/kernel/debug/"
+    exit(1)
+  # Memorizer Startup
+  ret = os.system("echo 1 > " + mem_path + "clear_object_list")
+  if ret != 0:
+    print "Failed to clear object list"
+    exit(1)
+  ret = os.system("echo 0 > " + mem_path + "print_live_obj")
+  if ret != 0:
+    print "Failed to disable live object dumping"
+    exit(1)
+  ret = os.system("echo 1 > " + mem_path + "memorizer_enabled")
+  if ret != 0:
+    print "Failed to enable memorizer object allocation tracking"
+    exit(1)
+  ret = os.system("echo 1 > " + mem_path + "memorizer_log_access")
+  if ret != 0:
+    print "Failed to enable memorizer object access tracking"
+    exit(1)
+
+def cleanup():
+  # Memorizer cleanup
+  ret = os.system("echo 0 > " + mem_path + "memorizer_log_access")
+  if ret != 0:
+    print "Failed to disable memorizer object access tracking"
+    exit(1)
+  ret = os.system("echo 0 > " + mem_path + "memorizer_enabled")
+  if ret != 0:
+    print "Failed to disable memorizer object allocation tracking"
+    exit(1)
+  # Print stats
+  ret = os.system("cat " + mem_path + "show_stats")
+  if ret != 0:
+    print "Failed to display memorizer stats"
+    exit(1)
+  ret = os.system("echo 1 > " + mem_path + "print_live_obj")
+  if ret != 0:
+    print "Failed to enable live object dumping"
+    exit(1)
+  # Make local copies of outputs
+  ret = os.system("cat " + mem_path + "kmap >> " +directory+ "test.kmap")
+  if ret != 0:
+    print "Failed to copy live and freed objs to kmap"
+    exit(1)
+  ret = os.system("echo 1 > " + mem_path + "clear_object_list")
+  if ret != 0:
+    print "Failed to clear all freed objects in obj list"
+    exit(1)
+
+def main(argv):
+  global completed
+  global directory
+  if len(sys.argv) == 1:
+    print "Invalid/missing arg. Please enter -e for basic tests, -m for ltp tests, and/or specify a full process to run in quotes. Specify path using the -p <path> otherwise default to ."
+    return
+  startup()
+  processes = []
+  easy_processes = False
+  next_arg = False
+  for arg in argv:
+    if next_arg:
+      next_arg = False
+      directory = str(arg) + "/"
+    elif arg == '-p':
+      next_arg = True
+    #User wants to run ltp
+    elif arg == '-m':
+      print "Performing ltp tests"
+      processes.append("/opt/ltp/runltp -p -l ltp.log")
+      print "See /opt/ltp/results/ltp.log for ltp results"
+    #User wants to run wget,ls,etc.
+    elif arg == '-e':
+      easy_processes = True
+      print "Performing basic ls test"
+      processes.append("ls")
+      print "Performing wget test"
+      processes.append("wget http://www.sas.upenn.edu/~egme/UPennlogo2.jpg")
+  print "Attempting to remove any existing kmaps in the specified path"
+  os.system("rm " + directory + "test.kmap")
+  print "Startup completed. Generating threads."
+  manager = threading.Thread(target=memManager, args=())
+  manager.start()
+  threads = []
+  for process in processes:
+    try:
+      t = threading.Thread(target=worker, args=(process,))
+      threads.append(t)
+      t.start()
+    except:
+      print "Error: unable to start thread"
+  for thr in threads:
+    thr.join()
+  completed = True
+  manager.join()
+  print "Threads ran to completion. Cleaning up."
+  basic_cleanup()
+  cleanup()
+  print "Cleanup successful."
+  return 0
+
+if __name__ == "__main__":
+  main(sys.argv)
diff --git a/scripts/memorizer/setup_env.sh b/scripts/memorizer/setup_env.sh
new file mode 100755
index 000000000000..aac7a410632d
--- /dev/null
+++ b/scripts/memorizer/setup_env.sh
@@ -0,0 +1,9 @@
+#!/bin/sh
+
+mount -t debugfs nodev /sys/kernel/debug
+mknod /dev/null c 1 3
+cp cp_test.sh /root
+cp userApp /root
+cp enable_memorizer.sh /root
+cp disable_memorizer.sh /root
+cd /root && mknod node c 252 0
diff --git a/scripts/memorizer/test_cp_memorizer.sh b/scripts/memorizer/test_cp_memorizer.sh
new file mode 100755
index 000000000000..ca292175d19b
--- /dev/null
+++ b/scripts/memorizer/test_cp_memorizer.sh
@@ -0,0 +1,18 @@
+#!/bin/sh
+
+KDIR=/sys/kernel/debug/memorizer
+UDIR=/mnt/host/src/repos/linux-slice/scripts
+cd $WDIR
+echo 1 > clear_object_list
+echo 1 > clear_printed_list
+echo 1 > memorizer_enabled
+echo 1 > memorizer_log_access
+
+
+cp -R /mnt/host/src/repos/linuxkit /root
+cd /root
+./userApp
+
+cd $WDIR
+echo 0 > memorizer_enabled
+echo 0 > memorizer_log_access
diff --git a/scripts/memorizer/userApp b/scripts/memorizer/userApp
new file mode 100755
index 000000000000..8465a1ef7e09
Binary files /dev/null and b/scripts/memorizer/userApp differ
diff --git a/scripts/memorizer/userApp.c b/scripts/memorizer/userApp.c
new file mode 100644
index 000000000000..79cf2de56506
--- /dev/null
+++ b/scripts/memorizer/userApp.c
@@ -0,0 +1,236 @@
+#include <stdio.h>
+#include <sys/syscall.h>
+#include <unistd.h>
+#include <stdio.h>
+#include <unistd.h>
+#include <sys/mman.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <stdlib.h>
+#include <stdint.h>
+#include <inttypes.h>
+#include "event_structs.h"
+
+
+#define ML 400000  // The size of profiler buffer (Unit: memory page)
+
+#define BUFF_MUTEX_LOCK { \
+		while(*buff_mutex); \
+		*buff_mutex = *buff_mutex + 1;\
+	}
+
+#define BUFF_MUTEX_UNLOCK {*buff_mutex = *buff_mutex - 1;}
+
+#define BUFF_FILL_RESET {*buff_fill = 0;}
+
+
+
+static int buf_fd = -1;
+static int buf_len;
+struct stat s ;
+char *buf;
+char *buff_end;
+char *buff_fill;
+struct memorizer_kernel_event *mke_ptr;
+unsigned int *buff_free_size;
+
+
+
+// This function opens a character device (which is pointed by a file named as fname) and performs the mmap() operation. If the operations are successful, the base address of memory mapped buffer is returned. Otherwise, a NULL pointer is returned.
+void *buf_init(char *fname)
+{
+	unsigned int *kadr;
+
+	if(buf_fd == -1){
+	buf_len = ML * getpagesize();
+	if ((buf_fd=open(fname, O_RDWR|O_SYNC))<0){
+	          printf("File open error. %s\n", fname);
+	          return NULL;
+		}
+	}
+	kadr = mmap(0, buf_len, PROT_READ|PROT_WRITE, MAP_SHARED, buf_fd, 0);
+	if (kadr == MAP_FAILED){
+		printf("Buf file open error.\n");
+		return NULL;
+		}
+	return kadr;
+}
+
+// This function closes the opened character device file
+void buf_exit()
+{
+	if(buf_fd!=-1){
+		close(buf_fd);
+		buf_fd = -1;
+	}
+}
+
+void printAllocHex()
+{
+	struct memorizer_kernel_alloc *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_alloc *)buf;
+	printf("aa, ");
+	printf("%llx, ",(unsigned long long)mke_ptr->event_ip);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_pa_ptr);
+	printf("%x, ",mke_ptr->event_size);
+	printf("%lx, ",mke_ptr->event_jiffies);
+	printf("%x, ",mke_ptr->pid);
+	printf("%s, ",mke_ptr->comm);
+	printf("%s\n",mke_ptr->funcstr);
+	buf = buf + sizeof(struct memorizer_kernel_alloc);
+}
+
+void printAlloc()
+{
+	struct memorizer_kernel_alloc *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_alloc *)buf;
+	printf("Alloc: ");
+	printf("%llx, ",(unsigned long long)mke_ptr->event_ip);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_pa_ptr);
+	printf("%u, ",mke_ptr->event_size);
+	printf("%lu, ",mke_ptr->event_jiffies);
+	printf("%u, ",mke_ptr->pid);
+	printf("%s, ",mke_ptr->comm);
+	printf("%s\n",mke_ptr->funcstr);
+	buf = buf + sizeof(struct memorizer_kernel_alloc);
+}
+
+
+void printFreeHex()
+{
+	struct memorizer_kernel_free *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_free *)buf;
+	printf("0xbb, ");
+	printf("%llx, ",(unsigned long long)mke_ptr->event_ip);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	printf("%lx, ",mke_ptr->event_jiffies);
+	printf("%x\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_free);
+}
+
+void printFree()
+{
+	struct memorizer_kernel_free *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_free *)buf;
+	printf("Free: ");
+	printf("%llx, ",(unsigned long long)mke_ptr->event_ip);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	printf("%lu, ",mke_ptr->event_jiffies);
+	printf("%u\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_free);
+}
+
+void printAccessHex(char type)
+{
+	struct memorizer_kernel_access *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_access *)buf;
+	if(type=='r')
+		printf("0xcc, ");
+	else
+		printf("0xdd, ");
+	printf("%llx, ",(unsigned long long)mke_ptr->event_ip);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	printf("%x, ",mke_ptr->event_size);
+	printf("%lx, ",mke_ptr->event_jiffies);
+	printf("%x\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_access);
+}
+
+
+void printAccess(char type)
+{
+	struct memorizer_kernel_access *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_access *)buf;
+	if(type=='r')
+		printf("Read: ");
+	else
+		printf("Write: ");
+	printf("%llx, ",(unsigned long long)mke_ptr->event_ip);
+	printf("%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	printf("%u, ",mke_ptr->event_size);
+	printf("%lu, ",mke_ptr->event_jiffies);
+	printf("%u\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_access);
+}
+
+void printFork()
+{
+	struct memorizer_kernel_fork *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_fork *)buf;
+	printf("Fork: ");
+	printf("%ld, ",mke_ptr->pid);
+	printf("%s\n",mke_ptr->comm);
+	buf = buf + sizeof(struct memorizer_kernel_fork);
+
+}
+
+int main (int argc, char *argv[])
+{
+	if(argc != 2)
+	{
+		printf("Incorrect number of Command Line Arguments!\n");
+		return 0;
+	}
+
+	// Open the Character Device and MMap
+	buf = buf_init("node");
+	if(!buf)
+		return -1;
+
+	//Read and count the MMaped data entries
+	buff_end = (buf + ML*getpagesize()) - 1;
+	buff_fill = buf;
+	buf++;
+	buff_free_size = (unsigned int *)buf;
+	buf = buf + sizeof(unsigned int);
+
+	mke_ptr = (struct memorizer_kernel_event *)buf;
+	if(*argv[1]=='c')
+	{
+		printf("Remaining Bytes: ");
+		printf("%u",*buff_free_size);
+	}
+	else if(*argv[1]=='p')
+	{
+
+		//TODO: Call different functions for different events
+		while(*buf!=0)
+		{
+			if(*buf == 0xffffffaa)
+				printAlloc();
+			else if (*buf == 0xffffffbb)
+				printFree();
+			else if(*buf == 0xffffffcc)
+				printAccess('r');
+			else if(*buf == 0xffffffdd)
+				printAccess('w');
+			else if(*buf == 0xffffffee)
+				printFork();
+
+		}
+
+	}
+	else if(*argv[1]=='h')
+	{
+
+		//TODO: Call different functions for different events
+		while(*buf!=0)
+		{
+			if(*buf == 0xffffffaa)
+				printAllocHex();
+			else if (*buf == 0xffffffbb)
+				printFreeHex();
+			else if(*buf == 0xffffffcc)
+				printAccessHex('r');
+			else if(*buf == 0xffffffdd)
+				printAccessHex('w');
+		}
+
+	}
+	buf_exit();
+
+	return 0;
+}
diff --git a/scripts/userApp.c b/scripts/userApp.c
new file mode 100644
index 000000000000..a2d44fbd17c6
--- /dev/null
+++ b/scripts/userApp.c
@@ -0,0 +1,314 @@
+#define _GNU_SOURCE
+#include <stdio.h>
+#include <sys/syscall.h>
+#include <unistd.h>
+#include <stdio.h>
+#include <unistd.h>
+#include <sys/mman.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <stdlib.h>
+#include <stdint.h>
+#include <inttypes.h>
+#include "event_structs.h"
+#include <pthread.h>
+
+
+#define ML 500000  // The size of profiler buffer (Unit: memory page)
+#define NB 16
+#define BUFF_MUTEX_LOCK { \
+		while(*buff_mutex)\
+		*buff_mutex = *buff_mutex + 1;\
+	}
+
+#define BUFF_MUTEX_UNLOCK {*buff_mutex = *buff_mutex - 1;}
+
+#define BUFF_FILL_RESET {*buff_fill = 0;}
+
+
+
+static int buff_fd_list[NB];
+static int buf_len;
+struct stat s ;
+char *buffList[NB];
+char *buf;
+char *buff_end;
+char *buff_start;
+char *buff_fill;
+char *buff_mutex;
+struct memorizer_kernel_event *mke_ptr;
+unsigned int *buff_free_size;
+char *stringBase;
+unsigned int idx;
+char outputFileName[30];
+FILE *fp;
+unsigned int curBuff = 0;
+
+
+/*
+ * switchBuffer - switches the the buffer being written to, when the buffer is full
+ */
+void switchBuffer()
+{
+		buf = (char *)buffList[curBuff];
+
+		buff_fill = buf;
+		buf = buf + 1;
+
+		buff_mutex = buf;
+		buf = buf + 1;
+
+
+		buff_free_size = (unsigned int *)buf;
+		buf = buf + sizeof(unsigned int);
+
+
+		buff_start = buf;
+
+
+}
+
+
+
+
+
+
+
+
+// This function opens a character device (which is pointed by a file named as fname) and performs the mmap() operation. If the operations are successful, the base address of memory mapped buffer is returned. Otherwise, a NULL pointer is returned.
+void *buf_init(char *fname,int *buf_fd)
+{
+	unsigned int *kadr;
+
+	buf_len = ML * getpagesize();
+	if ((*buf_fd=open(fname, O_RDWR|O_SYNC))<0){
+	          printf("File open error. %s\n", fname);
+	          return NULL;
+	}
+	kadr = mmap(0, buf_len, PROT_READ|PROT_WRITE, MAP_SHARED, *buf_fd, 0);
+	if (kadr == MAP_FAILED){
+		printf("Buf file open error.\n");
+		return NULL;
+		}
+	return kadr;
+}
+
+// This function closes the opened character device file
+void buf_exit(int buf_fd)
+{
+	if(buf_fd!=-1){
+		close(buf_fd);
+		buf_fd = -1;
+	}
+}
+
+void printAllocHex()
+{
+	struct memorizer_kernel_alloc *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_alloc *)buf;
+	fprintf(fp,"aa, ");
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->event_ip);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_pa_ptr);
+	fprintf(fp,"%x, ",mke_ptr->event_size);
+	fprintf(fp,"%lx, ",mke_ptr->event_jiffies);
+	fprintf(fp,"%x, ",mke_ptr->pid);
+	fprintf(fp,"%s, ",mke_ptr->comm);
+	fprintf(fp,"%s\n",mke_ptr->funcstr);
+	buf = buf + sizeof(struct memorizer_kernel_alloc);
+//	fprintf(stderr, "before 1 incrementing = %u\n", *buff_free_size);
+//	*buff_free_size = *buff_free_size + sizeof(struct memorizer_kernel_alloc);
+//	fprintf(stderr, "after 1 incrementing = %u\n", *buff_free_size);
+}
+
+void printAlloc()
+{
+	struct memorizer_kernel_alloc *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_alloc *)buf;
+	fprintf(fp,"Alloc: ");
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->event_ip);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_pa_ptr);
+	fprintf(fp,"%u, ",mke_ptr->event_size);
+	fprintf(fp,"%lu, ",mke_ptr->event_jiffies);
+	fprintf(fp,"%u, ",mke_ptr->pid);
+	fprintf(fp,"%s, ",mke_ptr->comm);
+	fprintf(fp,"%s\n",mke_ptr->funcstr);
+	buf = buf + sizeof(struct memorizer_kernel_alloc);
+//	fprintf(stderr, "before 2 incrementing = %u\n", *buff_free_size);
+//	*buff_free_size = *buff_free_size + sizeof(struct memorizer_kernel_alloc);
+//	fprintf(stderr, "after 2 incrementing = %u\n", *buff_free_size);
+}
+
+
+void printFreeHex()
+{
+	struct memorizer_kernel_free *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_free *)buf;
+	fprintf(fp,"0xbb, ");
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->event_ip);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	fprintf(fp,"%lx, ",mke_ptr->event_jiffies);
+	fprintf(fp,"%x\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_free);
+//	fprintf(stderr, "before 3 incrementing = %u\n", *buff_free_size);
+//	*buff_free_size = *buff_free_size + sizeof(struct memorizer_kernel_free);
+//	fprintf(stderr, "after 3 incrementing = %u\n", *buff_free_size);
+}
+
+void printFree()
+{
+	struct memorizer_kernel_free *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_free *)buf;
+	fprintf(fp,"Free: ");
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->event_ip);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	fprintf(fp,"%lu, ",mke_ptr->event_jiffies);
+	fprintf(fp,"%u\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_free);
+//	fprintf(stderr, "before 4 incrementing = %u\n", *buff_free_size);
+//	*buff_free_size = *buff_free_size + sizeof(struct memorizer_kernel_free);
+//	fprintf(stderr, "after 4 incrementing = %u\n", *buff_free_size);
+}
+
+void printAccessHex(char type)
+{
+	struct memorizer_kernel_access *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_access *)buf;
+	if(type=='r')
+		fprintf(fp,"0xcc, ");
+	else
+		fprintf(fp,"0xdd, ");
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->event_ip);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	fprintf(fp,"%x, ",mke_ptr->event_size);
+	fprintf(fp,"%lx, ",mke_ptr->event_jiffies);
+	fprintf(fp,"%x\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_access);
+//	fprintf(stderr, "before 5 incrementing = %u\n", *buff_free_size);
+//	*buff_free_size = *buff_free_size + sizeof(struct memorizer_kernel_access);
+//	fprintf(stderr, "after 5 incrementing = %u\n", *buff_free_size);
+}
+
+
+void printAccess(char type)
+{
+	struct memorizer_kernel_access *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_access *)buf;
+	if(type=='r')
+		fprintf(fp,"Read: ");
+	else
+		fprintf(fp,"Write: ");
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->event_ip);
+	fprintf(fp,"%llx, ",(unsigned long long)mke_ptr->src_va_ptr);
+	fprintf(fp,"%u, ",mke_ptr->event_size);
+	fprintf(fp,"%lu, ",mke_ptr->event_jiffies);
+	fprintf(fp,"%u\n",mke_ptr->pid);
+	buf = buf + sizeof(struct memorizer_kernel_access);
+//	fprintf(stderr, "before 6 incrementing = %u\n", *buff_free_size);
+//	*buff_free_size = *buff_free_size + sizeof(struct memorizer_kernel_access);
+//	fprintf(stderr, "after 6 incrementing = %u\n", *buff_free_size);
+}
+
+void printFork()
+{
+	struct memorizer_kernel_fork *mke_ptr;
+	mke_ptr = (struct memorizer_kernel_fork *)buf;
+	fprintf(fp,"Fork: ");
+	fprintf(fp,"%ld, ",mke_ptr->pid);
+	fprintf(fp,"%s\n",mke_ptr->comm);
+	buf = buf + sizeof(struct memorizer_kernel_fork);
+//	fprintf(stderr, "before 7 incrementing = %u\n", *buff_free_size);
+//	*buff_free_size = *buff_free_size + sizeof(struct memorizer_kernel_fork);
+//	fprintf(stderr, "after 7 incrementing = %u\n", *buff_free_size);
+}
+
+int main (int argc, char *argv[])
+{
+	unsigned int i;
+	char devName[12];
+	if(argc != 2)
+	{
+		printf("Incorrect number of Command Line Arguments!\n");
+		return 0;
+	}
+
+
+	for(i = 0;i<NB;i++)
+	{
+		sprintf(devName,"node%u",i);
+		buffList[i] = buf_init(devName,&buff_fd_list[i]);
+		if(!buff_fd_list[i])
+			return -1;
+	}
+	// Open the Character Device and MMap
+
+	switchBuffer();
+
+	printf("Choosing inital buffer\n");
+
+
+	if(*argv[1]=='c')
+	{
+		printf("Remaining Bytes: ");
+		printf("%u\n",*buff_free_size);
+	}
+	else if(*argv[1]=='p')
+	{
+
+		while(1)
+		{
+
+
+			//We Don't want the memorizer tracking us clearing out the buffer from userspace
+			if(!*buff_fill)
+			{
+				curBuff = (curBuff + 1)%NB;
+				continue;
+			}
+
+			printf("Userspace: Buffer Full! Now Clearing Buffer %d\n",curBuff);
+
+
+			sprintf(outputFileName,"ouput%d",idx);
+			fp = fopen(outputFileName,"w+");
+
+
+			//printf("Acquired the Lock\n");
+			while(*buf!=0)
+			{
+				if(*buf == 0xffffffaa)
+					printAlloc();
+				else if (*buf == 0xffffffbb)
+					printFree();
+				else if(*buf == 0xffffffcc)
+					printAccess('r');
+				else if(*buf == 0xffffffdd)
+					printAccess('w');
+				else if(*buf == 0xffffffee)
+					printFork();
+				idx++;
+			}
+			*buff_free_size = (4096 * ML) - 6;
+			*buff_fill = 0;
+			printf("Done Printing\n");
+
+			fclose(fp);
+			printf("Closed the File Pointer\n");
+
+			idx++;
+
+		}
+
+	}
+	for(i = 0;i<NB;i++)
+	{
+		buf_exit(buff_fd_list[i]);
+
+	}
+
+
+
+	return 0;
+}
diff --git a/security/Kconfig b/security/Kconfig
index e6db09a779b7..3b78d0b40863 100644
--- a/security/Kconfig
+++ b/security/Kconfig
@@ -159,6 +159,7 @@ config FORTIFY_SOURCE
 	depends on !CC_IS_CLANG || CLANG_VERSION >= 120001
 	# https://github.com/llvm/llvm-project/issues/53645
 	depends on !CC_IS_CLANG || !X86_32
+	depends on !MEMORIZER
 	help
 	  Detect overflows of buffers in common string and memory functions
 	  where the compiler can determine and validate the buffer sizes.
diff --git a/tools/include/linux/log2.h b/tools/include/linux/log2.h
index e20a67d538b8..6182febb5357 100644
--- a/tools/include/linux/log2.h
+++ b/tools/include/linux/log2.h
@@ -173,3 +173,42 @@ unsigned long __rounddown_pow_of_two(unsigned long n)
  )
 
 #endif /* _TOOLS_LINUX_LOG2_H */
+#if 0
+// TODO memorizer
+diff a/tools/include/linux/log2.h b/tools/include/linux/log2.h	(rejected hunks)
+@@ -12,12 +12,6 @@
+// #ifndef _TOOLS_LINUX_LOG2_H
+// #define _TOOLS_LINUX_LOG2_H
+ 
+-/*
+- * deal with unrepresentable constant logarithms
+- */
+-extern __attribute__((const, noreturn))
+-int ____ilog2_NaN(void);
+-
+ /*
+  * non-constant log of base 2 calculators
+  * - the arch may override these in asm/bitops.h if they can be implemented
+@@ -78,7 +72,7 @@ unsigned long __rounddown_pow_of_two(unsigned long n)
+ #define ilog2(n)				\
+ (						\
+ 	__builtin_constant_p(n) ? (		\
+-		(n) < 1 ? ____ilog2_NaN() :	\
++		(n) < 2 ? 0 :			\
+ 		(n) & (1ULL << 63) ? 63 :	\
+ 		(n) & (1ULL << 62) ? 62 :	\
+ 		(n) & (1ULL << 61) ? 61 :	\
+@@ -141,10 +135,7 @@ unsigned long __rounddown_pow_of_two(unsigned long n)
+ 		(n) & (1ULL <<  4) ?  4 :	\
+ 		(n) & (1ULL <<  3) ?  3 :	\
+ 		(n) & (1ULL <<  2) ?  2 :	\
+-		(n) & (1ULL <<  1) ?  1 :	\
+-		(n) & (1ULL <<  0) ?  0 :	\
+-		____ilog2_NaN()			\
+-				   ) :		\
++		1 ) :				\
+ 	(sizeof(n) <= 4) ?			\
+ 	__ilog2_u32(n) :			\
+ 	__ilog2_u64(n)				\
+TODO memorizer */
+#endif
